{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will work on this project with Adnan Fazlinovic. \n",
    "\n",
    "Our idea is to compare how different embeddings affect OCR accuracy, similar to the approach taken in this article: https://medium.com/states-title/using-nlp-bert-to-improve-ocr-accuracy-385c98ae174c (Links to an external site.)\n",
    "\n",
    "We would like to try some embeddings we have learned about in the course, such as CBoW, word2vec, FastText, ELMo, BERT. If we have more time and find other interesting representations online we will evaluate them as well. The idea is to find a baseline OCR accuracy and then exploring if, and how much, this accuracy can be improved by applying word embeddings to the incorrectly scanned words. As datasets we want to use images of machine written text, in English (suggestions on good datasets would be appreciated). If the data is of inconsistent quality, the focus will be too much on the image scanning part and not on the NLP application. \n",
    "\n",
    "If we don't find any good image resource dataset, we could use a dataset from the course and randomly corrupt a subset of the words in each text. \n",
    "\n",
    "A preliminary title is \"Asessing word embeddings for improving OCR accuracy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Probably use a synthetic dataset (read in europarl corpus and corrupt it a bit)\n",
    "- Richard suggests corrupting it in ways that often happen in OCR, like rn <-> m, i <-> l, cl <-> d \n",
    "    - (https://scribenet.com/articles/2016/03/04/how-to-get-the-most-out-of-ocr)\n",
    "- I think he means we should use a spellchecker to find misspelled words, and check with NER that they are not a name\n",
    "- Then maybe we don't throw out the misspelled word, it can be useful\n",
    "\n",
    "It feels like we're mostly making a spell/grammar checking model - that's fine\n",
    "\n",
    "- Richard thinks that character level embeddings can be useful here\n",
    "- Evaluation of performance: see what people usually use within OCR. Some ideas:\n",
    "\n",
    "    - https://www.aclweb.org/anthology/I17-1101.pdf\n",
    "    - https://loicbarrault.github.io/papers/afli_cicling2015.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I therefore agree with the European Parliament 's recommendations to the World Bank in this area .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = r\"C:\\Users\\saran\\OneDrive\\Dokument\\GitHub\\NLP\\Project5\\europarl.txt\"\n",
    "encoding = \"utf-8\"\n",
    "\n",
    "with open(file, encoding = encoding) as f:\n",
    "    for line in f:\n",
    "        print(line)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enchant.checker import SpellChecker\n",
    "\n",
    "spell = SpellChecker(\"en-UK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'s\n"
     ]
    }
   ],
   "source": [
    "words = line.split()\n",
    "\n",
    "for word in words:\n",
    "    if not spell.check(word):\n",
    "        print(word)\n",
    "        \n",
    "# what do we do with 's and similar words? they are correct. maybe just merge them with previous word, or remove them? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell.check(\"cat's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spiel',\n",
       " 'spelt',\n",
       " 'spell',\n",
       " 'seel',\n",
       " 'spec',\n",
       " 'sped',\n",
       " 'spew',\n",
       " 'Opel',\n",
       " 'sp el',\n",
       " 'sp-el']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell.suggest(\"spel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CAT',\n",
       " 'act',\n",
       " 'vat',\n",
       " 'car',\n",
       " 'cay',\n",
       " 'ca',\n",
       " 'ct',\n",
       " 'at',\n",
       " 'cate',\n",
       " 'cats',\n",
       " 'cast',\n",
       " 'scat',\n",
       " 'cant',\n",
       " 'cart',\n",
       " 'coat']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell.suggest(\"cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in and corrupt the Europarl dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character elision is when letter pairs and individual letters are confused by the software. The most frequent examples are cl and d or rn and m. These types of errors occur any time pairs of letters are shaped similarly to other letters (vv for w, ol for d, li for h, nn for m). \n",
    "\n",
    "rn <-> m, i <-> l, cl <-> d, vv <-> w, ol <-> d, li <-> h, nn <-> m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionMatrix = []\n",
    "confusionMatrix.append(['rn', 'm'])\n",
    "confusionMatrix.append(['ol', 'd'])\n",
    "confusionMatrix.append(['cl', 'd'])\n",
    "confusionMatrix.append(['vv', 'w'])\n",
    "confusionMatrix.append(['li', 'h'])\n",
    "confusionMatrix.append(['nn', 'm'])\n",
    "confusionMatrix = np.array(confusionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['rn', 'm'],\n",
       "       ['ol', 'd'],\n",
       "       ['cl', 'd'],\n",
       "       ['vv', 'w'],\n",
       "       ['li', 'h'],\n",
       "       ['nn', 'm']], dtype='<U2')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I therefore agree with the European Parliament 's recommendations to the World Bank in this area .\n",
      "\n",
      "The Commissioner responsible , Jacques Barrot , has promised to present an informative report by the end of July , and our group was keen to wait for this .\n",
      "\n",
      "Resumption of the session\n",
      "\n",
      "I am pleased that , in dialogue with the institutions , a solution has successfully been found that can satisfy everybody , or at least I hope it can , and I would thank you for your constructive work in this process .\n",
      "\n",
      "It has only done so when faced with intense pressure from dairy producers , the European Parliament and 21 Member States .\n",
      "\n",
      "These measures have also been relevant for the textile and clothing industry : for instance , the Globalisation Fund support has been used to reintegrate workers laid off in mostly small and medium-sized enterprises of the sector in Italy , Malta , Spain , Portugal , Lithuania and Belgium .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = r\"C:\\Users\\saran\\OneDrive\\Dokument\\GitHub\\NLP\\Project5\\europarl.txt\"\n",
    "encoding = \"utf-8\"\n",
    "\n",
    "with open(file, encoding = encoding) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        print(line)\n",
    "        \n",
    "        if i == 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These measures have also been relevant for the textile and clothing industry : for instance , the Globalisation Fund support has been used to reintegrate workers laid off in mostly small and medium-sized enterprises of the sector in Italy , Malta , Spain , Portugal , Lithuania and Belgium .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in confusionMatrix:\n",
    "    line = line.replace(pair[0], pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These measures have also been relevant for the textile and dothing industry : for instance , the Globahsation Fund support has been used to reintegrate workers laid off in mostly small and medium-sized enterprises of the sector in Italy , Malta , Spain , Portugal , Lithuania and Belgium .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
