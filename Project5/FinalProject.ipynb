{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Treating OCR character elision using BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Optical Character Recognition, or OCR for short, is the process of taking an image of text and making it machine readable. This technique is dependent on two areas of machine learning: computer vision and natural language processing. One needs a good vision model to identify words and paragraphs, and ideally also a language model to find and clean up errors outputted from the scanned text. Here we will focus on the post-correction stage of the OCR process. \n",
    "\n",
    "One issue that can arise within OCR is that a pair of characters are mistaken for another character, because their shapes are similar. This is called character elision. An elision that appears to be quite common is when the computer vision software mistkes `rn` for the letter `m`. This kind of issue seems relatively common within OCR, and could be corrected using NLP concepts and models. \n",
    "\n",
    "#### Detecting and correcting elisions\n",
    "\n",
    "We want our code to accurately correct elisions, but first it needs to detect them. When elision happens there are two possible outcomes: either we have a `legal word error`, or the elision results in nonsense. With a legal error, the eliased word is a valid English word, like when `modern` is interpreted as `modem`. Detecting legal errors would require quite advanced, and expensive, language modelling to score each word in the corpus and determine which ones don't seem to match the context. On the other hand, when the eliased word is not a part of the English language it can be found using a regular spell checker. \n",
    "\n",
    "The goal for this project is to explore how using BERT can improve the correction accuracy for eliased text. The idea was inspired by [this article](https://medium.com/states-title/using-nlp-bert-to-improve-ocr-accuracy-385c98ae174c). We will insert synthetic elisions in the Europarl corpus. We then apply a spell checker to detect misspelled words and give a suggested replacement. These spelling suggestions will make up a baseline accuracy. We then explore if, and how much, the correction accuracy can be improved by instead using NLP methods in the correction stage. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import spacy\n",
    "import torch\n",
    "import difflib\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from enchant.checker import SpellChecker \n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "spell = SpellChecker(\"en-UK\")\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Installation instructions for enchant package: pip install pyenchant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Read in corpus and apply elision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Based on [this article](https://medium.com/states-title/using-nlp-bert-to-improve-ocr-accuracy-385c98ae174c) and suggestions from Richard, we have decided on six elisions to use when corrupting our corpus:\n",
    "\n",
    "1. rn $\\to$ m\n",
    "2. ol $\\to$ d\n",
    "3. cl $\\to$ d\n",
    "4. vv $\\to$ w\n",
    "5. li $\\to$ h\n",
    "6. nn $\\to$ m\n",
    "\n",
    "When the text is read in, we search for occurences of the six letter pairs to the left. Each time a pair is found it is eliased to the corresponding letter on the right, with a certain elision probability. This is intended to reflect the fact that elision might not occur every time the letter pairs are observed. \n",
    "\n",
    "With some words, like `cliff`, it matters in what order we look for the letter pairs. If we find the `cl` and replace it with `d`, the resulting word is `diff`. However if we look for the `li` pair first, the resulting word will be `chff`. To avoid bias in the synthetic elisions, the search order will be randomised for each new document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# An array containing the elision transformations\n",
    "elisionArray = []\n",
    "elisionArray.append(['rn', 'm'])\n",
    "elisionArray.append(['ol', 'd'])\n",
    "elisionArray.append(['cl', 'd'])\n",
    "elisionArray.append(['vv', 'w'])\n",
    "elisionArray.append(['li', 'h'])\n",
    "elisionArray.append(['nn', 'm'])\n",
    "elisionArray = np.array(elisionArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Elision example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We now give an example of how the elisions will be performed, and how we store the results in an accessible way. The function `elision_example` will process a nonsense sentence containing a lot of `rn` and `li`.  Stepping through the sentence, some of the words are read incorrectly. To what extent the line is corrupted depends on the elision probability inputted to the function.\n",
    "\n",
    "There is a counter for the total number of elisions present in the text, and information about them is stored in a data structure. For each eliased word we save its position in the document, and the original spelling. This is combined to a vector and tupled with the document number in the corpus; this is set to 0 in this case since we only have one line of text. For a single document that has N synthetic elisions, the resulting data structure will look like this:  \n",
    "\n",
    "> [ ( doc_idx, [ ( word_pos_1, original_word_1 ), ..., ( word_pos_N, original_word_N ) ] ) ]\n",
    "\n",
    "Another way to access the same information would be to compare the ground truth dataset with the eliased dataset on a line by line basis. Since comparisons will need to be done multiple times, the data structure described above was introduced to save computational time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def elision_example(elisionArray, elision_prob):\n",
    "\n",
    "    \"\"\"\n",
    "    A function illustrating how the character elision will be handled in this project. \n",
    "    Later code follow the same principle, but with more help functions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # A test sentence that contains a lot of 'rn' and 'li'\n",
    "    line = \"I scorn this little barn with my lilac yarn and delicious fern\"\n",
    "    print(\"Original line:\")\n",
    "    print(line)\n",
    "    print('-'*80)\n",
    "\n",
    "    new_line = line\n",
    "\n",
    "    # Randomize search order, since some letter pairs overlap \n",
    "    elisionArray = np.random.permutation(elisionArray)\n",
    "    \n",
    "    # Loop over all pairs and apply elisions\n",
    "    for pair in elisionArray:\n",
    "        \n",
    "        n_errors = 0\n",
    "        \n",
    "        for m in re.finditer(pair[0], new_line):\n",
    "\n",
    "            rd = np.random.rand(1)\n",
    "            if rd < elision_prob:\n",
    "\n",
    "                # Note that the position can't be used later since the line changes length!\n",
    "                print(\"--> Replaced \", pair[0], \" at position \", m.start(), \"\")\n",
    "\n",
    "                # Do some line magic\n",
    "                tmp = list(new_line)\n",
    "                tmp[m.start()-n_errors:m.end()-n_errors] = \"%%\"\n",
    "                new_line = \"\".join(tmp)\n",
    "                new_line = new_line.replace(\"%%\", pair[1])\n",
    "                print(new_line)\n",
    "\n",
    "                # count number of replacements\n",
    "                n_errors += 1\n",
    "\n",
    "    line = list(line.split())\n",
    "    new_line = list(new_line.split())\n",
    "    total_errors = 0\n",
    "    ground_truth = []\n",
    "\n",
    "    # This will be the index of a document in the corpus\n",
    "    doc_num = 0     \n",
    "\n",
    "    # This contains the ground truth for all eliased words\n",
    "    tmp = []\n",
    "\n",
    "    # Save location and correct spelling for the corrupted words\n",
    "    for j in range(len(line)):\n",
    "        if line[j] != new_line[j]:\n",
    "\n",
    "            total_errors += 1\n",
    "            tmp.append((j, line[j]))\n",
    "\n",
    "    ground_truth.append((doc_num, tmp))\n",
    "    print('-'*80)\n",
    "    print(\"Total elisions: \", total_errors)\n",
    "    print(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original line:\n",
      "I scorn this little barn with my lilac yarn and delicious fern\n",
      "--------------------------------------------------------------------------------\n",
      "--> Replaced  rn  at position  22 \n",
      "I scorn this little bam with my lilac yarn and delicious fern\n",
      "--> Replaced  li  at position  13 \n",
      "I scorn this httle bam with my lilac yarn and delicious fern\n",
      "--------------------------------------------------------------------------------\n",
      "Total elisions:  2\n",
      "[(0, [(3, 'little'), (4, 'barn')])]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "elision_example(elisionArray, elision_prob = 0.5)\n",
    "np.random.seed(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Apply character elision to the Europarl corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here we work with the Europarl corpus, in the hope that the language will be consistent and that there won't be a lot of spelling errors. We read in part of the corpus and apply elisions. Like in the example above, they are inserted randomly and their positions and original spellings are saved. Then a spell checker is applied to detect errors in the eliased corpus. We provide some words and characters for the spellchecker to ignore, and make sure that any candidate misspelled words are not part of a name. \n",
    "\n",
    "While spell checking, some words might appear that have no spelling suggestions. Those words tend to be in French or German, since the EU parliament uses these languages occasionally. It's not fair to ask our English spell checker to handle these words, and the purpose here is not to handle multiple different languages. However the impact of this is minimal: in the first 10 000 documents of the corpus, only around 10 contain a word that doesn't have a spelling suggestion. We note how many lines are affected, but the issue is not adressed further. \n",
    "\n",
    "Last of all, we apply a train/test split that will be needed for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def check_line_errors(line, new_line, total_errors):\n",
    "    \n",
    "    \"\"\"\n",
    "    Check for errors between gold standard document and eliased document.\n",
    "    \n",
    "    Returns: \n",
    "        truth: For each word mismatch, note down the word position and the original word. \n",
    "        total_errors: A counter for the total number of synthetic elisions in the corpus. \n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    line = list(line.split())\n",
    "    new_line = list(new_line.split()) \n",
    "    truth = []\n",
    "\n",
    "    for j in range(len(line)):\n",
    "        if line[j] != new_line[j]:\n",
    "\n",
    "            total_errors += 1\n",
    "            truth.append((j, line[j]))\n",
    "    \n",
    "    return truth, total_errors\n",
    "\n",
    "\n",
    "def apply_elision(line, elision_array, elision_prob):\n",
    "    \n",
    "    \"\"\"\n",
    "    Apply elision to a line of text. Requires an array with elisions, where letter pairs are in the first column and the \n",
    "    resulting letter after elision is in the second column. Whenever one of these letter pairs is observed in the text, \n",
    "    it eliased with probability elision_prob.\n",
    "    \n",
    "    Returns: \n",
    "        new_line: The line of text after elision has (potentially) occured. \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Randomize search order, since some letter pairs overlap \n",
    "    elision_array = np.random.permutation(elision_array)\n",
    "    for elision_pair in elision_array:\n",
    "\n",
    "        # Count number of times each letter pair has been corrupted (since this changes the line length)\n",
    "        n_errors = 0 \n",
    "\n",
    "        for m in re.finditer(elision_pair[0], line):\n",
    "\n",
    "            rd = np.random.rand(1)\n",
    "            if rd < elision_prob:\n",
    "\n",
    "                # Replace the letter pair and convert to a new line\n",
    "                tmp = list(line)\n",
    "                tmp[m.start()-n_errors:m.end()-n_errors] = \"%%\"\n",
    "                line = \"\".join(tmp)\n",
    "                line = line.replace(\"%%\", elision_pair[1])\n",
    "\n",
    "                # count number of replacements\n",
    "                n_errors += 1\n",
    "\n",
    "    return line\n",
    "\n",
    "\n",
    "def read_data(corpus_file, corpus_encoding, max_lines, elision_array, elision_prob):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given a file path and encoding, as well as a maximum number of lines, this function reads in a corpus and \n",
    "    saves it as a list of documents. It also creates and saves an eliased version of the same corpus. \n",
    "        \n",
    "    Returns:\n",
    "        ground_truth: The original corpus.\n",
    "        corrupted_data: The corrupted corpus, where some character elisions have been performed.\n",
    "        elisions: Information about the elisions. Contains the document indices, line positions, and original spellings.\n",
    "        total_errors: A counter for the total number of synthetic elisions applied to the corpus.\n",
    "          \n",
    "    \"\"\"\n",
    "    \n",
    "    total_errors = 0\n",
    "    ground_truth = []\n",
    "    corrupted_data = []\n",
    "    line_indices = []\n",
    "    elision_errors = []\n",
    "    \n",
    "    with open(corpus_file, encoding = corpus_encoding) as f:\n",
    "        \n",
    "        for d, line in enumerate(f):\n",
    "        \n",
    "            if d == max_lines:\n",
    "                break\n",
    "        \n",
    "            # Apply elision and keep track of which words have been corrupted\n",
    "            new_line = apply_elision(line, elision_array, elision_prob)  \n",
    "            line_truth, total_errors = check_line_errors(line, new_line, total_errors)\n",
    "            \n",
    "            if len(line_truth) > 0:\n",
    "                line_indices.append(d)\n",
    "                elision_errors.append(line_truth)\n",
    "                \n",
    "            # Append original and (potentially) corrupted line\n",
    "            ground_truth.append(line) \n",
    "            corrupted_data.append(new_line)\n",
    "                \n",
    "    elisions = (line_indices, elision_errors)\n",
    "            \n",
    "    return ground_truth, corrupted_data, elisions, total_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def contains_numbers(string):\n",
    "    \n",
    "    \"\"\"\n",
    "    Check if the given string contains any digits.\n",
    "    \"\"\"\n",
    "    \n",
    "    return any(char.isdigit() for char in string)\n",
    "\n",
    "\n",
    "def is_part_of_name(line, word):\n",
    "    \n",
    "    \"\"\"\n",
    "    Using spacy for NER, we check if a specific word in a line of text appears to be part of a name.\n",
    "    Check if the word is a subset of the name of a person, nationality, company, or country. \n",
    "    \n",
    "    Returns:\n",
    "        is_name: a boolean which is True if the word appears to be part of a name.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Apply nlp pipeline, check if this \"misspelled word\" is a name\n",
    "    result = nlp(line, disable = ['tagger', 'parser'])\n",
    "    is_name = False\n",
    "\n",
    "    for entity in result.ents:\n",
    "        # If the \"misspelled\" word is part of the name of a person, country etc - we ignore it\n",
    "        if entity.label_ in  [\"PERSON\", \"NORP\", \"GPE\", \"ORG\"] and entity.text.find(word) > -1:\n",
    "            is_name = True\n",
    "\n",
    "    return is_name\n",
    "\n",
    "\n",
    "def identify_spelling_errors(data, ignore):\n",
    "\n",
    "    \"\"\"\n",
    "    Given a dataset and a list of characters to ignore, find misspelled words. The word positions are saved, and\n",
    "    we also store the spellchecker's suggested spelling. Only words that are not part of a name are considered.\n",
    "    \n",
    "    Returns:\n",
    "        spelling_errors: A data structure containing document indices and line positions for misspelled words.\n",
    "        spelling_corrected: A data structure containing the same indices as above, but with a spell check applied.\n",
    "        n_misspelled: A counter for the total number of misspelled words in the text. \n",
    "        not_in_english: A list of lines where spelling suggestion was not possible; usually because word is not in English.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    n_misspelled = 0\n",
    "    line_indices = []\n",
    "    spelling_errors = []\n",
    "    spelling_corrected = []\n",
    "    not_in_english = []\n",
    "\n",
    "    for d, line in enumerate(data):\n",
    "\n",
    "        words = line.split()\n",
    "        tmp_1 = []\n",
    "        tmp_2 = []\n",
    "        \n",
    "        for i, word in enumerate(words):\n",
    "\n",
    "            # Some dates and similar are marked as misspelled - ignore words containing numbers!\n",
    "            if not word in ignore and not contains_numbers(word):\n",
    "            \n",
    "                # Apply a spell checker\n",
    "                if not spell.check(word):\n",
    "        \n",
    "                    # Check if the word is part of a name\n",
    "                    if not is_part_of_name(line, word):\n",
    "\n",
    "                        try:\n",
    "                            # Apply a spell correction to the word\n",
    "                            corrected_word = spell.suggest(word)[0]\n",
    "                            \n",
    "                            # Note down word and position\n",
    "                            tmp_1.append((i, word))\n",
    "                            tmp_2.append((i, corrected_word))\n",
    "                            n_misspelled += 1\n",
    "\n",
    "                        except IndexError:\n",
    "\n",
    "                            # If no spelling suggestions exist, note the document index. \n",
    "                            not_in_english.append(d)\n",
    "        \n",
    "        # If spelling errors were found, save the line index and words.\n",
    "        if len(tmp_1) > 0:\n",
    "            line_indices.append(d)\n",
    "            spelling_errors.append(tmp_1)\n",
    "            spelling_corrected.append(tmp_2)\n",
    "            \n",
    "    spelling_errors = (line_indices, spelling_errors)\n",
    "    spelling_corrected = (line_indices, spelling_corrected)\n",
    "\n",
    "    return spelling_errors, spelling_corrected, n_misspelled, not_in_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def match_and_split(indices, datastruct):\n",
    "    \n",
    "    \"\"\"\n",
    "    Match data structure index component to an array, and keep the overlap.\n",
    "    \"\"\"\n",
    "    \n",
    "    tmp_idx = []\n",
    "    tmp_info = []\n",
    "    \n",
    "    for i in indices:\n",
    "        if i in datastruct[0]:\n",
    "            tmp_idx.append(i)\n",
    "            j = datastruct[0].index(i)\n",
    "            tmp_info.append(datastruct[1][j])\n",
    "            \n",
    "    return (tmp_idx, tmp_info)\n",
    "\n",
    "def train_test_split(data, elisions, spell_errors, spell_corrected, train_frac = 0.7):\n",
    "    \n",
    "    \"\"\"\n",
    "    Split dataset and all related data structures into a training and test part. \n",
    "    \n",
    "    Returns (train, test) where:\n",
    "        train = (train_data, train_lines, train_elisions, train_errors, train_corrected)\n",
    "        test = (test_data, test_lines, test_elisions, test_errors, test_corrected)\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    train_len = int(len(data)*train_frac)\n",
    "    \n",
    "    line_vec = np.random.permutation(len(data))\n",
    "    train_lines = np.sort(line_vec[:train_len]).tolist()\n",
    "    test_lines = np.sort(line_vec[train_len:]).tolist()\n",
    "    \n",
    "    train_data = []\n",
    "    for i in train_lines:\n",
    "        train_data.append(data[i])\n",
    "\n",
    "    train_elisions = match_and_split(train_lines, elisions)\n",
    "    train_errors = match_and_split(train_lines, spell_errors)\n",
    "    train_corrected = match_and_split(train_lines, spell_corrected)\n",
    "    \n",
    "    test_data = []\n",
    "    for i in test_lines:\n",
    "        test_data.append(data[i])\n",
    "        \n",
    "    test_elisions = match_and_split(test_lines, elisions)\n",
    "    test_errors = match_and_split(test_lines, spell_errors)\n",
    "    test_corrected = match_and_split(test_lines, spell_corrected)\n",
    "            \n",
    "    return ((train_data, train_lines, train_elisions, train_errors, train_corrected), \n",
    "            (test_data, test_lines, test_elisions, test_errors, test_corrected))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of synthetic errors:  5821\n",
      "Number of identified misspelled words:  5806\n",
      "Number of lines where no spelling suggestion could be given: 6\n"
     ]
    }
   ],
   "source": [
    "file = \"europarl.txt\"\n",
    "encoding = \"utf-8\"\n",
    "\n",
    "elision_prob = 0.5\n",
    "max_lines = 10000\n",
    "np.random.seed(None)\n",
    "\n",
    "# Words and characters for the spellchecker to ignore\n",
    "ignore = [\",\", \".\", '\"', \"(\", \")\", \"-\", \"'\", \"!\", \"?\", \":\", \";\", \"/\", \n",
    "          \"n't\", \"'s\", \"'m\", \"%\", \"--\", \"``\", \"___LANGCODE___\", \"''\"]\n",
    "\n",
    "ground_truth, data, elisions, n_elisions = read_data(file, encoding, max_lines, elisionArray, elision_prob)\n",
    "spell_errors, spell_corrected, n_misspelled, excl_lines = identify_spelling_errors(data, ignore)\n",
    "\n",
    "# Bundle relevant information\n",
    "lines = list(range(len(data)))\n",
    "bundle = (data, lines, elisions, spell_errors, spell_corrected)\n",
    "\n",
    "print(\"Number of synthetic errors: \", n_elisions)\n",
    "print(\"Number of identified misspelled words: \", n_misspelled)\n",
    "print(\"Number of lines where no spelling suggestion could be given:\", len(excl_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Apply a train test split\n",
    "(train, test) = train_test_split(data, elisions, spell_errors, spell_corrected, train_frac = 0.7)\n",
    "train_data, train_lines, train_elisions, train_errors, train_corrected = train\n",
    "test_data, test_lines, test_elisions, test_errors, test_corrected = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute baseline accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, a baseline accuracy is computed based on the spelling suggestions. With inspiration from [this article](https://www.aclweb.org/anthology/I17-1101.pdf), we introduce two word based accuracy metrics: the `detection accuracy` and `correction accuracy`. These are both evaluated on a document level. \n",
    "\n",
    "- __Detection accuracy__ shows the proportion of correctly detected errors and nonerrors in the evaluated documents.\n",
    "- __Correction accuracy__ reflects the ability of the model to accurately correct corrupted strings, without editing non-corrupted strings.\n",
    "\n",
    "These metrics are calculated as follows:\n",
    "\n",
    "$$\\text{detectionAccuracy} = \\frac{(TP + TN + \\text{incorrectEdits})}{(TP + TN + FP + FN + \\text{incorrectEdits})}$$\n",
    "\n",
    "$$\\text{correctionAccuracy} = \\frac{(TP + TN)}{(TP + TN + FP + FN + \\text{incorrectEdits})}$$\n",
    "\n",
    "- __TP__: There is an error on the line, which is corrected successfully.\n",
    "- __TN__: There is no error on the line, and no correction is made.\n",
    "- __FP__: There is no error on the line, but the model makes a change anyway.\n",
    "- __FN__: There is an error on the line, but it's not corrected.\n",
    "- __incorrectEdit__: There is an error on the line, but the attempted correction is wrong.\n",
    "\n",
    "It is worth noting that not all documents are the same length. This could possibly be adjusted for in the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy_scores(bundle, verbose = True, corrections = None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute detection and correction accuracy according to definitions above.\n",
    "    \n",
    "    Returns:\n",
    "        det_accuracy: detection accuracy\n",
    "        corr_accuracy: correction accuracy\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Either use provided corrections, or the one in the bundle\n",
    "    if corrections == None:\n",
    "        data, _ , elisions, _, corrections = bundle \n",
    "    else: \n",
    "        data, _ , elisions, _, _ = bundle \n",
    "    \n",
    "    elision_idx = elisions[0]\n",
    "    elision_words = elisions[1]\n",
    "    corrected_idx = corrections[0]\n",
    "    corrected_words = corrections[1]\n",
    "    \n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    incorrect_edits = 0\n",
    "    \n",
    "    # True positives = number of correct changes\n",
    "    for pos1, idx in enumerate(elision_idx):\n",
    "        if idx in corrected_idx:\n",
    "            \n",
    "            pos2 = corrected_idx.index(idx)\n",
    "            \n",
    "            if elision_words[pos1] == corrected_words[pos2]:\n",
    "                TP += 1\n",
    "            else:\n",
    "                incorrect_edits += 1\n",
    "\n",
    "    # True negatives = number of lines that do not have elision, and that have not been touched by spellchecker\n",
    "    corpus_size = len(data)\n",
    "    all_touched_lines = set(corrected_idx) | set(elision_idx)\n",
    "    TN = corpus_size - len(all_touched_lines)\n",
    "    \n",
    "    # False negatives = lines that should have been changed, but were not\n",
    "    false_neg_lines = set(elision_idx) - set(corrected_idx)\n",
    "    FN = len(false_neg_lines)\n",
    "    \n",
    "    # False positives = there was no elision, but a correction was made anyway\n",
    "    false_pos_lines = set(corrected_idx) - set(elision_idx)\n",
    "    FP = len(false_pos_lines)\n",
    "        \n",
    "    # Double check that it all adds up\n",
    "    if FP + FN + TN + TP + incorrect_edits != corpus_size:\n",
    "        print(\"ERROR: These scores don't add up to the number of lines!\")\n",
    "        \n",
    "    # Compute accuracy scores    \n",
    "    det_accuracy = (TP + TN + incorrect_edits)/corpus_size\n",
    "    corr_accuracy = (TP + TN)/corpus_size\n",
    "    \n",
    "    if verbose == True:\n",
    "        \n",
    "        print(\"True positives: \", TP)  \n",
    "        print(\"True negatives: \", TN)\n",
    "        print(\"False negatives: \", FN)\n",
    "        print(\"False positives: \", FP)\n",
    "        print(\"Incorrect edits: \", incorrect_edits)\n",
    "        print(\"-\"*30)\n",
    "        print(\"Detection accuracy: \", np.round(det_accuracy, 4))\n",
    "        print(\"Correction accuracy: \", np.round(corr_accuracy, 4))\n",
    "        print(\"=\"*40)\n",
    "    \n",
    "    return det_accuracy, corr_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives:  738\n",
      "True negatives:  5580\n",
      "False negatives:  335\n",
      "False positives:  294\n",
      "Incorrect edits:  3053\n",
      "------------------------------\n",
      "Detection accuracy:  0.9371\n",
      "Correction accuracy:  0.6318\n",
      "========================================\n",
      "True positives:  531\n",
      "True negatives:  3911\n",
      "False negatives:  227\n",
      "False positives:  216\n",
      "Incorrect edits:  2115\n",
      "------------------------------\n",
      "Detection accuracy:  0.9367\n",
      "Correction accuracy:  0.6346\n",
      "========================================\n",
      "True positives:  207\n",
      "True negatives:  1669\n",
      "False negatives:  108\n",
      "False positives:  78\n",
      "Incorrect edits:  938\n",
      "------------------------------\n",
      "Detection accuracy:  0.938\n",
      "Correction accuracy:  0.6253\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "det_acc_full, corr_acc_full = compute_accuracy_scores(bundle)\n",
    "det_acc_train, corr_acc_train = compute_accuracy_scores(train)\n",
    "det_acc_test, corr_acc_test = compute_accuracy_scores(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill in missing words using BERT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have created synthetic elisions, and attempted to find them using a spellchecker. We now want to correct the identified misspelled words using BERT for masked LM. This is a BERT model with a language modelling head on top, from the Transformers library. \n",
    "\n",
    "The following is performed only for the lines with spelling errors: each misspelled word is replaced with the `[MASK]` token, and the BERT tokenizer is applied to the line. We also generate segment tensors, that tell us which parts of the document are in different sentences (this is done by looking for a period in the text). Feeding the tokenized line and the segment information to the BertForMaskedLM model, we can ask it to suggest words that could be hidden behind the `[MASK]` token. When considering these suggestions, the code provides two options:\n",
    "\n",
    "1. Use the top suggestion, ignoring any information about the misspelled word.\n",
    "2. Generate a certain number of suggestions, then choose the one that best matches the misspelled word. This is intended to give a suggestion that matches the context, but also uses information about the misspelled word.\n",
    "\n",
    "Option number 2 has the potential of yielding a higher correction accuracy, depending on the number of suggestions given. To give a more specific example of the two approaches, we consider a sentence in the corpus that contains the word `families`. If elision happens, this word instead becomes `famihes`. The spell checker suggests correcting this to `famished`, which is does not fit into the same context. Bert's top suggestion based on the context is `parents`. However if we ask for more suggestions, and match them against the misspelled word, we correctly end up with `families`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def find_best_suggestion(word, suggestions):\n",
    "    \n",
    "    \"\"\"\n",
    "    Find the spelling suggestion that best matches a given word.\n",
    "    \"\"\"\n",
    "    \n",
    "    dict, max = {}, 0\n",
    "    a = set(suggestions)\n",
    "\n",
    "    for b in a:\n",
    "\n",
    "        tmp = difflib.SequenceMatcher(None, word, b).ratio();\n",
    "        dict[tmp] = b\n",
    "\n",
    "        if tmp > max:\n",
    "            max = tmp\n",
    "\n",
    "    return(dict[max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_segments_tensors(tokenized_text):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given a line of tokenized text, create segments tensors needed for maskedLM. \n",
    "    \"\"\"\n",
    "    \n",
    "    segs = [i for i, e in enumerate(tokenized_text) if e == \".\"]\n",
    "    segments_ids=[]\n",
    "    prev=-1\n",
    "    for k, s in enumerate(segs):\n",
    "        segments_ids = segments_ids + [k] * (s-prev)\n",
    "        prev=s\n",
    "    segments_ids = segments_ids + [len(segs)] * (len(tokenized_text) - len(segments_ids))\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "    return segments_ids, segments_tensors\n",
    "\n",
    "\n",
    "def masked_LM_oneline(tokenizer, model, line, line_info, use_word=False, n_suggestions=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Perform masked LM on one line of text and return the top predictions, given a tokenizer and model.\n",
    "    \n",
    "    Returns:\n",
    "        mask_predictions:\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Replace misspelled words with MASK token\n",
    "    split_line = line.split()\n",
    "    for item in line_info:\n",
    "        split_line[item[0]] = '[MASK]'\n",
    "    line = \" \".join(split_line)\n",
    "    \n",
    "    # Load, train and predict using pre-trained model\n",
    "    tokenized_text = tokenizer.tokenize(line)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    MASKIDS = [i for i, e in enumerate(tokenized_text) if e == '[MASK]']\n",
    "\n",
    "    # Create the segments tensors\n",
    "    segments_ids, segments_tensors = create_segments_tensors(tokenized_text)\n",
    "\n",
    "    # Predict all tokens\n",
    "    with torch.no_grad():\n",
    "        predictions = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "    # Process and display top prediction at each position\n",
    "    mask_predictions = []\n",
    "\n",
    "    if use_word == False:\n",
    "        \n",
    "        for mask_idx in MASKIDS:\n",
    "            word_int = predictions[0][0, mask_idx, :].topk(1).indices.tolist()\n",
    "            word_text = tokenizer.convert_ids_to_tokens(word_int)        \n",
    "            mask_predictions.append(word_text[0])\n",
    "    else:\n",
    "        \n",
    "        for k, mask_idx in enumerate(MASKIDS):\n",
    "\n",
    "            word_int = predictions[0][0, mask_idx, :].topk(n_suggestions).indices.tolist()\n",
    "            word_text = tokenizer.convert_ids_to_tokens(word_int)\n",
    "            original_word = line_info[k][1]\n",
    "            best = find_best_suggestion(original_word, word_text)\n",
    "            mask_predictions.append(best)\n",
    "        \n",
    "    return mask_predictions\n",
    "\n",
    "\n",
    "def masked_prediction(bundle, tokenizer, model, use_word=False, n_suggestions=5):   \n",
    "    \n",
    "    \"\"\"\n",
    "    Given a dataset and locations of misspelled words, perform masked prediction to replace the words. \n",
    "    Can either use information about the misspelled words, or disregard them entirely.\n",
    "    \"\"\"\n",
    "    \n",
    "    data, lines, _, spell_errors, _ = bundle \n",
    "    corrected = []\n",
    "\n",
    "    for i in range(len(spell_errors[0])):\n",
    "\n",
    "        line_idx = spell_errors[0][i]\n",
    "        line_info = spell_errors[1][i]\n",
    "        j = lines.index(line_idx)\n",
    "        line = data[j]           \n",
    "        \n",
    "        # Retrieve prediction using masked LM\n",
    "        preds = masked_LM_oneline(tokenizer, model, line, line_info, use_word, n_suggestions)\n",
    "        \n",
    "        # Save the results\n",
    "        tmp = []\n",
    "        for j, item in enumerate(line_info):\n",
    "            idx = item[0]\n",
    "            tmp.append((idx, preds[j]))\n",
    "\n",
    "        corrected.append(tmp)\n",
    "        \n",
    "    corrected = (spell_errors[0], corrected)\n",
    "    return corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrections using BERT for masked LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives:  331\n",
      "True negatives:  5580\n",
      "False negatives:  335\n",
      "False positives:  294\n",
      "Incorrect edits:  3460\n",
      "------------------------------\n",
      "Detection accuracy:  0.9371\n",
      "Correction accuracy:  0.5911\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# Completely ignore information about the misspelled word\n",
    "bert_corrected_1 = masked_prediction(bundle, tokenizer, model, use_word = False)\n",
    "det_acc_bert, corr_acc_bert = compute_accuracy_scores(bundle, corrections = bert_corrected_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives:  721\n",
      "True negatives:  5580\n",
      "False negatives:  335\n",
      "False positives:  294\n",
      "Incorrect edits:  3070\n",
      "------------------------------\n",
      "Detection accuracy:  0.9371\n",
      "Correction accuracy:  0.6301\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# Ask for 5 suggested words, and match them against the misspelled word\n",
    "bert_corrected_2 = masked_prediction(bundle, tokenizer, model, use_word = True, n_suggestions = 5)\n",
    "det_acc_bert, corr_acc_bert = compute_accuracy_scores(bundle, corrections = bert_corrected_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives:  909\n",
      "True negatives:  5580\n",
      "False negatives:  335\n",
      "False positives:  294\n",
      "Incorrect edits:  2882\n",
      "------------------------------\n",
      "Detection accuracy:  0.9371\n",
      "Correction accuracy:  0.6489\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# Ask for 5 suggested words, and match them against the misspelled word\n",
    "bert_corrected_3 = masked_prediction(bundle, tokenizer, model, use_word = True, n_suggestions = 10)\n",
    "det_acc_bert, corr_acc_bert = compute_accuracy_scores(bundle, corrections = bert_corrected_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have run the BERT for masked LM on the entire corpus. We note that the detection accuracy is exactly the same for all three runs, and this is also the same as the baseline detection accuracy. This is not surprising since the error detection is performed in the same way for all models. The detection accuracy is quite high (around 93%), which supports our choice of using the spell checker to find elisions. Using more sophisticated methods to improve the detection accuracy would likely demand a lot more computational power, and even then it might be a case of diminishing returns. With the detection accuracy constant, we will henceforth only compare progress using the correction accuracy. There are also more improvements to be made in this area. \n",
    "\n",
    "***\n",
    "\n",
    "From the three results above, we see that accepting more word suggestions from BERT can increase the correction accuracy. Logically, this only affects the number of true positives and incorrect edits. If n_suggestions is chosen optimally, we can convert a maximum amount of incorrect edits into true positives. We will tune n_suggestions on the training part of the corpus, and then see what correction accuracy this gives for the test part of the corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune n_suggestions on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code was run on a corpus with max_lines = 10000, train_frac = 0.7, and elision_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with  1  suggestions. Correction accuracy:  0.589 . Elapsed time:  340.81361269950867\n",
      "Done with  11  suggestions. Correction accuracy:  0.6318 . Elapsed time:  352.20549154281616\n",
      "Done with  21  suggestions. Correction accuracy:  0.6457 . Elapsed time:  337.28776836395264\n",
      "Done with  31  suggestions. Correction accuracy:  0.6515 . Elapsed time:  337.46693754196167\n",
      "Done with  41  suggestions. Correction accuracy:  0.6559 . Elapsed time:  332.2921733856201\n",
      "Done with  51  suggestions. Correction accuracy:  0.6578 . Elapsed time:  344.25616478919983\n",
      "Done with  61  suggestions. Correction accuracy:  0.6588 . Elapsed time:  326.5397262573242\n",
      "Done with  71  suggestions. Correction accuracy:  0.6612 . Elapsed time:  332.3647768497467\n",
      "Done with  81  suggestions. Correction accuracy:  0.6629 . Elapsed time:  339.96380162239075\n",
      "Done with  91  suggestions. Correction accuracy:  0.6643 . Elapsed time:  336.81837725639343\n",
      "Done with  101  suggestions. Correction accuracy:  0.6653 . Elapsed time:  346.5211651325226\n",
      "Done with  111  suggestions. Correction accuracy:  0.6665 . Elapsed time:  362.2215485572815\n",
      "Done with  121  suggestions. Correction accuracy:  0.6674 . Elapsed time:  347.5793001651764\n",
      "Done with  131  suggestions. Correction accuracy:  0.6685 . Elapsed time:  365.50592017173767\n",
      "Done with  141  suggestions. Correction accuracy:  0.669 . Elapsed time:  328.8239426612854\n",
      "Elapsed time:  5130.664708137512\n"
     ]
    }
   ],
   "source": [
    "start_1 = time.time()\n",
    "corr_acc_vec = []\n",
    "n_sugg_vec = range(1, 150, 10)\n",
    "\n",
    "for n_suggestions in n_sugg_vec:\n",
    "    start_2 = time.time()\n",
    "    bert_corr = masked_prediction(train, tokenizer, model, use_word = True, n_suggestions = n_suggestions)\n",
    "    _, acc = compute_accuracy_scores(bundle,  verbose = False, corrections = bert_corr)\n",
    "    corr_acc_vec.append(acc)\n",
    "    print(\"Done with \", n_suggestions, \" suggestions. Correction accuracy: \", acc, \". Elapsed time: \", time.time() - start_2)\n",
    "    \n",
    "print(\"Elapsed time: \", time.time() - start_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, corr_acc_train = compute_accuracy_scores(train, verbose = False)\n",
    "\n",
    "plt.plot(n_sugg_vec, corr_acc_vec, label = \"BERT\")\n",
    "plt.title(\"Training: corr_acc as a function of n_suggestions\")\n",
    "plt.hlines(corr_acc_train, 0, 150, 'r', label = \"Spell checker\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives:  443\n",
      "True negatives:  1669\n",
      "False negatives:  108\n",
      "False positives:  78\n",
      "Incorrect edits:  702\n",
      "------------------------------\n",
      "Detection accuracy:  0.938\n",
      "Correction accuracy:  0.704\n",
      "========================================\n",
      "True positives:  207\n",
      "True negatives:  1669\n",
      "False negatives:  108\n",
      "False positives:  78\n",
      "Incorrect edits:  938\n",
      "------------------------------\n",
      "Detection accuracy:  0.938\n",
      "Correction accuracy:  0.6253\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set!\n",
    "\n",
    "best_idx = corr_acc_vec.index(max(corr_acc_vec))\n",
    "best_n_suggestions = n_sugg_vec[best_idx]\n",
    "\n",
    "bert_corr = masked_prediction(test, tokenizer, model, use_word = True, n_suggestions = best_n_suggestions)\n",
    "_, acc = compute_accuracy_scores(test, corrections = bert_corr, verbose = True)\n",
    "_, corr_acc_test = compute_accuracy_scores(test, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It keeps increasing! Europarl does use some pretty uncommon words, so it makes sense that we'd need many suggestions to find the one that's right for the context.\n",
    "\n",
    "Eventually it flattens out. I guess at that point either Bert stops giving suggestions, or we cover the right level o uncommon words. Does it flatten out faster for distilBert?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy as a function of elision_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_pipeline(file, encoding, elisionArray, elision_probs, max_lines, ignore_list, n_suggestions, verbose = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Pipeline for loading in data with different levels of elision probability, and applying the Bert model. \n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    corr_acc_vec = []\n",
    "    n_elisions_vec = []\n",
    "    lines = list(range(max_lines))\n",
    "\n",
    "    for elision_prob in elision_probs:\n",
    "    \n",
    "        ground_truth, data, elisions, n_elisions = read_data(file, encoding, max_lines, elisionArray, elision_prob)\n",
    "        spell_errors, spell_corrected, n_misspelled, excl_lines = identify_spelling_errors(data, ignore_list)\n",
    "        n_elisions_vec.append(n_elisions)\n",
    "        bundle = (data, lines, elisions, spell_errors, spell_corrected)\n",
    "        \n",
    "        if verbose == True:\n",
    "            \n",
    "            print(\"Number of identified misspelled words: \", n_misspelled)\n",
    "            print(\"Number of synthetic errors: \", n_elisions)\n",
    "            print(\"=\"*75)\n",
    "\n",
    "        bert_corrected = masked_prediction(bundle, tokenizer, model, use_word=True, n_suggestions=n_suggestions)\n",
    "        _, acc = compute_accuracy_scores(bundle, verbose = False, corrections = bert_corrected)    \n",
    "        corr_acc_vec.append(acc)\n",
    "        \n",
    "    print(\"Elapsed time: \", time.time() - start)\n",
    "        \n",
    "    return corr_acc_vec, n_elisions_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elision_probs = np.linspace(0, 1, 10)\n",
    "n_lines = 10000\n",
    "np.random.seed(None)\n",
    "\n",
    "corr_vec_1, n_elisions_1 = evaluation_pipeline(file, encoding, elisionArray, elision_probs, \n",
    "                                 max_lines = n_lines, ignore_list = ignore, n_suggestions = 1, verbose = False)\n",
    "corr_vec_5, n_elisions_5 = evaluation_pipeline(file, encoding, elisionArray, elision_probs, \n",
    "                                 max_lines = n_lines, ignore_list = ignore, n_suggestions = 50, verbose = False)\n",
    "corr_vec_15, n_elisions_15 = evaluation_pipeline(file, encoding, elisionArray, elision_probs, \n",
    "                                 max_lines = n_lines, ignore_list = ignore, n_suggestions = 100, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(elision_probs, corr_vec_1, label = \"1\")\n",
    "plt.plot(elision_probs, corr_vec_5, label = \"50\")\n",
    "plt.plot(elision_probs, corr_vec_15, label = \"100\")\n",
    "plt.title(\"Correction accuracy as a function of elision probability\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(elision_probs, n_elisions_1, label = \"1\")\n",
    "plt.plot(elision_probs, n_elisions_5, label = \"50\")\n",
    "plt.plot(elision_probs, n_elisions_15, label = \"100\")\n",
    "plt.title(\"Number of elisions as a function of elision probability\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# some variability in number of elisions but that's fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(elision_probs, corr_vec_1, label = \"n_sugg = 1\")\n",
    "plt.plot(elision_probs, corr_vec_5, label = \"n_sugg = 50\")\n",
    "plt.plot(elision_probs, corr_vec_15, label = \"n_sugg = 100\")\n",
    "plt.title(\"Correction accuracy as a function of elision probability\")\n",
    "plt.xlabel(\"Elision probability\")\n",
    "plt.ylabel(\"Correction accuracy\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "#plt.savefig('elision_prob.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Distilbert - does this give suggestions in the same way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name_2 = 'albert-base-v2'\n",
    "tokenizer_2 = AutoTokenizer.from_pretrained(model_name_2)\n",
    "model_2 = AutoModelForMaskedLM.from_pretrained(model_name_2).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name_3 = 'google/electra-small-discriminator'\n",
    "tokenizer_3 = AutoTokenizer.from_pretrained(model_name_3)\n",
    "model_3 = AutoModelForMaskedLM.from_pretrained(model_name_3).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "distilbert_corrected = masked_prediction(bundle, tokenizer_2, model_2, False)\n",
    "det_acc_distilbert, corr_acc_distilbert = compute_accuracy_scores(elisions, distilbert_corrected, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "distilbert_corrected = masked_prediction(bundle, tokenizer_2, model_2, True, n_suggestions = 50)\n",
    "det_acc_distilbert, corr_acc_distilbert = compute_accuracy_scores(elisions, distilbert_corrected, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Electra - does this give suggestions in the same way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "electra_corrected = masked_prediction(bundle, tokenizer_3, model_3, True, n_suggestions = 15)\n",
    "det_acc_electra, corr_acc_electra = compute_accuracy_scores(bundle, verbose = True, corrections = electra_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Try using bigrams \n",
    "This could catch when an incorrectly scanned word becomes another, correctly spelled word: for example, God -> Cod. The spell checker will not react to this, but it should create a strange bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Try some other word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "two approaches: either throw away the misspelled word and try to fill it in from context, or use character level embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawbacks of using correction accuracy, consider using cosine distance. But using the old letters we are pretty certain that we preserve the word shape and the context - most likely we find the right word, I hope. \n",
    "\n",
    "our detection accuracy is high enough, spell checker is fine for finding elisions\n",
    "\n",
    "Try on another dataset and see what happens!\n",
    "Implement other masked LM models and compare\n",
    "\n",
    "Ideas for improvements: finding errors, and correcting them. \n",
    "- Finding errors can of course be done better than with the spellchecker. Assign a score to each word based on the context? But this seems expensive.\n",
    "- Correcting them: character based model, more suggestions from BERT somehow? Another model (albert, electra)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What fraction of elisions are never discovered? Check if false negative rate remains roughly constant - these might be words that end up being a correctly spelled word after elisions. How much better are we than the enchant spellchecker?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are entries in excl_lines, make adjustments as described above. (a bit buggy, have to fix this)\n",
    "\n",
    "remove_excl_lines(data, excl_lines)\n",
    "remove_excl_lines(ground_truth, excl_lines)\n",
    "\n",
    "elisions, n_elisions = remove_excl_entries(elisions, n_elisions, excl_lines)\n",
    "spell_errors, n_misspelled = remove_excl_entries(spell_errors, n_misspelled, excl_lines)\n",
    "spell_corrected, _ = remove_excl_entries(spell_corrected, n_misspelled, excl_lines)\n",
    "\n",
    "def remove_excl_lines(data, excl_lines):\n",
    "    \n",
    "    \"\"\"\n",
    "    Remove given lines, by index, from a dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    for i, line in enumerate(excl_lines):\n",
    "        del data[line-i]\n",
    "\n",
    "        \n",
    "def remove_excl_entries(entries, count, excl_lines):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given lines to exclude, by index, remove them from the data structure at the right position. Also adjust count.\n",
    "    \"\"\"\n",
    "    \n",
    "    a = set(excl_lines)\n",
    "    b = set(entries[0])\n",
    "    \n",
    "    for line_idx in a & b:\n",
    "            \n",
    "        # Find position of the line to be removed\n",
    "        list_pos = entries[0].index(line_idx)\n",
    "        \n",
    "        # Adjust the word count\n",
    "        count -= len(entries[1][list_pos])\n",
    "\n",
    "        # Delete the identified entries\n",
    "        del entries[0][list_pos]\n",
    "        del entries[1][list_pos]\n",
    "          \n",
    "    return entries, count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
