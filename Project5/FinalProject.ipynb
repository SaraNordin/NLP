{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing word embeddings for improving OCR accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will work on this project with Adnan Fazlinovic. \n",
    "\n",
    "Our idea is to compare how different embeddings affect OCR accuracy, similar to the approach taken in this article: https://medium.com/states-title/using-nlp-bert-to-improve-ocr-accuracy-385c98ae174c (Links to an external site.)\n",
    "\n",
    "We would like to try some embeddings we have learned about in the course, such as CBoW, word2vec, FastText, ELMo, BERT. If we have more time and find other interesting representations online we will evaluate them as well. The idea is to find a baseline OCR accuracy and then exploring if, and how much, this accuracy can be improved by applying word embeddings to the incorrectly scanned words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Probably use a synthetic dataset (read in europarl corpus and corrupt it a bit)\n",
    "- Richard suggests corrupting it in ways that often happen in OCR, like rn <-> m, i <-> l, cl <-> d \n",
    "    - (https://scribenet.com/articles/2016/03/04/how-to-get-the-most-out-of-ocr)\n",
    "- I think he means we should use a spellchecker to find misspelled words, and check with NER that they are not a name\n",
    "- Then maybe we don't throw out the misspelled word, it can be useful\n",
    "\n",
    "It feels like we're mostly making a spell/grammar checking model - that's fine\n",
    "\n",
    "- Richard thinks that character level embeddings can be useful here\n",
    "- Evaluation of performance: see what people usually use within OCR. Some ideas:\n",
    "\n",
    "    - https://www.aclweb.org/anthology/I17-1101.pdf\n",
    "    - https://loicbarrault.github.io/papers/afli_cicling2015.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "from enchant.checker import SpellChecker\n",
    "\n",
    "spell = SpellChecker(\"en-UK\")\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out spellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell.check(\"cat's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spiel',\n",
       " 'spelt',\n",
       " 'spell',\n",
       " 'seel',\n",
       " 'spec',\n",
       " 'sped',\n",
       " 'spew',\n",
       " 'Opel',\n",
       " 'sp el',\n",
       " 'sp-el']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell.suggest(\"spel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem\n",
      "problem\n"
     ]
    }
   ],
   "source": [
    "import enchant, difflib\n",
    "word=\"prblm\"\n",
    "\n",
    "# example - is this a good way to get most relevant word from enchant.spellcheck()?\n",
    "\n",
    "dict,max = {},0\n",
    "a = set(spell.suggest(word))\n",
    "\n",
    "for b in a:\n",
    "\n",
    "    tmp = difflib.SequenceMatcher(None, word, b).ratio();\n",
    "    dict[tmp] = b\n",
    "    \n",
    "    if tmp > max:\n",
    "        max = tmp\n",
    "\n",
    "print(dict[max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'princedom'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in and corrupt the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character elision is when letter pairs and individual letters are confused by the software. These types of errors occur any time pairs of letters are shaped similarly to other letters. Six common pairs are:\n",
    "\n",
    "rn <-> m, cl <-> d, vv <-> w, ol <-> d, li <-> h, nn <-> m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "elisionArray = []\n",
    "elisionArray.append(['rn', 'm'])\n",
    "elisionArray.append(['ol', 'd'])\n",
    "elisionArray.append(['cl', 'd'])\n",
    "elisionArray.append(['vv', 'w'])\n",
    "elisionArray.append(['li', 'h'])\n",
    "elisionArray.append(['nn', 'm'])\n",
    "elisionArray = np.array(elisionArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## An example of how the character elision will be handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I smilingly scorn this little barn with my lilac yarn and delicious fern\n",
      "--> Replaced  rn  at position  15 \n",
      "I smilingly scom this little barn with my lilac yarn and delicious fern\n",
      "--> Replaced  rn  at position  32 \n",
      "I smilingly scom this little bam with my lilac yarn and delicious fern\n",
      "--> Replaced  rn  at position  51 \n",
      "I smilingly scom this little bam with my lilac yam and delicious fern\n",
      "--> Replaced  rn  at position  70 \n",
      "I smilingly scom this little bam with my lilac yam and delicious fem\n",
      "--> Replaced  li  at position  5 \n",
      "I smihngly scom this little bam with my lilac yam and delicious fem\n",
      "--------------------------------------------------------------------------------\n",
      "Total errors:  5\n",
      "[(0, [(1, 'smilingly'), (2, 'scorn'), (5, 'barn'), (9, 'yarn'), (12, 'fern')])]\n"
     ]
    }
   ],
   "source": [
    "# A test sentence that contains a lot of 'rn' and 'li'\n",
    "line = \"I smilingly scorn this little barn with my lilac yarn and delicious fern\"\n",
    "print(line)\n",
    "\n",
    "new_line = line\n",
    "elision_prob = 0.5\n",
    "\n",
    "# Randomize search order, since some letter pairs overlap \n",
    "elisionArray = np.random.permutation(elisionArray)\n",
    "for pair in elisionArray:\n",
    "    \n",
    "    n_errors = 0\n",
    "    \n",
    "    for m in re.finditer(pair[0], new_line):\n",
    "        \n",
    "        rd = np.random.rand(1)\n",
    "        if rd < elision_prob:\n",
    "            \n",
    "            # Note that the position can't be used later since the line changes length!\n",
    "            print(\"--> Replaced \", pair[0], \" at position \", m.start(), \"\")\n",
    "            \n",
    "            tmp = list(new_line)\n",
    "            tmp[m.start()-n_errors:m.end()-n_errors] = \"%%\"\n",
    "            new_line = \"\".join(tmp)\n",
    "            new_line = new_line.replace(\"%%\", pair[1])\n",
    "            \n",
    "            print(new_line)\n",
    "\n",
    "            # count number of replacements\n",
    "            n_errors += 1\n",
    "            \n",
    "print('-'*80)\n",
    "\n",
    "# Save location and correct spelling for the corrupted words\n",
    "\n",
    "line = list(line.split())\n",
    "new_line = list(new_line.split())\n",
    "\n",
    "total_errors = 0\n",
    "ground_truth = []\n",
    "\n",
    "# This will be the index of a document in the corpus\n",
    "doc_num = 0     \n",
    "\n",
    "# This contains the ground truth for each document \n",
    "tmp = []\n",
    "\n",
    "for j in range(len(line)):\n",
    "    if line[j] != new_line[j]:\n",
    "        \n",
    "        total_errors += 1\n",
    "        tmp.append((j, line[j]))\n",
    "        \n",
    "ground_truth.append((doc_num, tmp))\n",
    "print(\"Total errors: \", total_errors)\n",
    "print(ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply character elision to the Europarl data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_line_errors(line, new_line, total_errors):\n",
    "    \n",
    "    \"\"\"\n",
    "    Check for errors between gold standard document and \"OCR\" document.\n",
    "    \"\"\"\n",
    "\n",
    "    line = list(line.split())\n",
    "    new_line = list(new_line.split()) \n",
    "    truth = []\n",
    "\n",
    "    for j in range(len(line)):\n",
    "        if line[j] != new_line[j]:\n",
    "\n",
    "            total_errors += 1\n",
    "            truth.append((j, line[j]))\n",
    "    \n",
    "    return truth, total_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(corpus_file, corpus_encoding, max_lines, elisionArray, elision_prob):\n",
    "    \n",
    "    \"\"\"\n",
    "    Read in ground truth version of the text, and a (synthetic) corrupted OCR dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    total_errors = 0\n",
    "    ground_truth = []\n",
    "    elision_errors = []\n",
    "    corrupted_data = []\n",
    "    \n",
    "    with open(corpus_file, encoding = corpus_encoding) as f:\n",
    "        \n",
    "        for d, line in enumerate(f):\n",
    "        \n",
    "            if d == max_lines:\n",
    "                break\n",
    "        \n",
    "            ground_truth.append(line) \n",
    "            new_line = line\n",
    "            \n",
    "            # Randomize search order, since some letter pairs overlap \n",
    "            elisionArray = np.random.permutation(elisionArray)\n",
    "            for pair in elisionArray:\n",
    "                \n",
    "                # Count number of times each letter pair has been corrupted (since this changes the line length)\n",
    "                n_errors = 0 \n",
    "                \n",
    "                for m in re.finditer(pair[0], new_line):\n",
    "                    \n",
    "                    rd = np.random.rand(1)\n",
    "                    if rd < elision_prob:\n",
    "                        \n",
    "                        # Replace the letter pair and convert to a new line\n",
    "                        tmp = list(new_line)\n",
    "                        tmp[m.start()-n_errors:m.end()-n_errors] = \"%%\"\n",
    "                        new_line = \"\".join(tmp)\n",
    "                        new_line = new_line.replace(\"%%\", pair[1])\n",
    "\n",
    "                        # count number of replacements\n",
    "                        n_errors += 1\n",
    "                     \n",
    "            corrupted_data.append(new_line)\n",
    "            line_truth, total_errors = check_line_errors(line, new_line, total_errors)\n",
    "            if len(line_truth) > 0:\n",
    "                elision_errors.append((d, line_truth))\n",
    "            \n",
    "    return ground_truth, corrupted_data, elision_errors, total_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_spelling_errors(data, ignore):\n",
    "\n",
    "    \"\"\"\n",
    "    Given a dataset and a list of characters to ignore, find misspelled words that are _not_ names.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_misspelled = 0\n",
    "    spelling_errors = []\n",
    "    spelling_corrected = []\n",
    "\n",
    "    for d, line in enumerate(data):\n",
    "\n",
    "        words = line.split()\n",
    "        tmp_1 = []\n",
    "        tmp_2 = []\n",
    "        \n",
    "        for i, word in enumerate(words):\n",
    "\n",
    "            if not word in ignore and not spell.check(word):\n",
    "\n",
    "                # Apply nlp pipeline, check if this \"misspelled word\" is a name\n",
    "                result = nlp(line, disable = ['tagger', 'parser'])\n",
    "                is_name = False\n",
    "\n",
    "                for entity in result.ents:\n",
    "                    # Ignore \"misspelled\" names of people, places, organisations - and numbers\n",
    "                    if entity.label_ in  [\"PERSON\", \"NORP\", \"GPE\", \"ORG\", \"CARDINAL\"] and entity.text.find(word) > -1:\n",
    "                        is_name = True\n",
    "\n",
    "                if not is_name:\n",
    "\n",
    "                    n_misspelled += 1\n",
    "\n",
    "                    # Note down word and position\n",
    "                    tmp_1.append((i, word))\n",
    "    \n",
    "        \n",
    "        if len(tmp_1) > 0:\n",
    "            \n",
    "            spelling_errors.append((d, tmp_1))\n",
    "            spelling_corrected.append((d, tmp_2))\n",
    "\n",
    "    return spelling_errors, n_misspelled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r\"C:\\Users\\saran\\OneDrive\\Dokument\\GitHub\\NLP\\Project5\\europarl.txt\"\n",
    "encoding = \"utf-8\"\n",
    "\n",
    "# Probability that a letter pair from our list will be confused if it is seen in the text\n",
    "elision_prob = 0.1\n",
    "max_lines = 100\n",
    "\n",
    "np.random.seed(0)\n",
    "ground_truth, data, elision_errors, n_errors = read_data(file, encoding, max_lines, elisionArray, elision_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "famihes\n",
      "selfemployed\n",
      "apphcation\n",
      "renationalise\n",
      "ultra-hberal\n",
      "pohcy\n",
      "black-marketeering\n",
      "concems\n",
      "hke\n",
      "pditics\n",
      "Solbes\n",
      "trialogue\n"
     ]
    }
   ],
   "source": [
    "# Words and characters for the spellchecker to ignore\n",
    "ignore = [\",\", \".\", '\"', \"(\", \")\", \"-\", \"'\", \"!\", \"?\", \":\", \";\", \"/\", \"n't\", \"'s\", \"'m\", \"%\", \"--\", \"___LANGCODE___\"]\n",
    "spelling_errors, n_misspelled = identify_spelling_errors(data, ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15, [(8, 'clear')]),\n",
       " (18, [(50, 'families')]),\n",
       " (21, [(23, 'application')]),\n",
       " (43, [(13, 'ultra-liberal')]),\n",
       " (48, [(6, 'policy')]),\n",
       " (72, [(1, 'concerns')]),\n",
       " (80, [(3, 'like')]),\n",
       " (81, [(12, 'politics')])]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elision_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(18, [(50, 'famihes')]),\n",
       " (19, [(3, 'selfemployed')]),\n",
       " (21, [(23, 'apphcation')]),\n",
       " (30, [(13, 'renationalise')]),\n",
       " (43, [(13, 'ultra-hberal')]),\n",
       " (48, [(6, 'pohcy')]),\n",
       " (59, [(17, 'black-marketeering')]),\n",
       " (72, [(1, 'concems')]),\n",
       " (80, [(3, 'hke')]),\n",
       " (81, [(12, 'pditics'), (28, 'Solbes')]),\n",
       " (83, [(30, 'trialogue')])]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spelling_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of identified misspelled words:  12\n",
      "Number of synthetic errors:  8\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of identified misspelled words: \", n_misspelled)\n",
    "print(\"Number of synthetic errors: \", n_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute baseline accuracy using spellcheck suggestions\n",
    "\n",
    "baseline: use top suggested word from enchant spellchecker\n",
    "\n",
    "so I guess we create a list of misspelled words just like doc_errors, apply the suggestion for each, and see if we get closer to the ground truth or not?\n",
    "\n",
    "- https://www.aclweb.org/anthology/I17-1101.pdf\n",
    "\n",
    "[From the article] For evaluation, we use the CER (Character Error Rate) metric as defined in OCR post-correction evaluations:\n",
    "\n",
    "$$ CER = \\frac{S + D + I}{S + D + C} $$\n",
    "\n",
    "Where S refers to the number of substituted characters in the OCR text (w.r.t. the reference texts), D to the number of deleted characters, I to the number of inserted characters and C to the number of ‘correct’ characters. We use the CER metric when comparing to the baseline system. __Is this useful in the character elision setting? Doesn't look like it__\n",
    "\n",
    "We want to measure the systems performance per input, rather than per character. We therefore introduce two complementary accuracy-based evaluation metrics, which evaluate on the level of the character window:\n",
    "- detection accuracy (detAcc) shows the proportion of correctly detected errors and nonerrors in the evaluated set of 20-character strings\n",
    "- correction accuracy (corrAcc) reflects the ability of the language model to accurately correct corrupted strings without overgenerating and editing non-corrupted strings\n",
    "\n",
    "These metrics are calculated as follows:\n",
    "\n",
    "$detAcc = \\frac{(TP + TN + incorrectEdit)}{(TP + TN + FP + FN + incorrectEdit)}$\n",
    "\n",
    "$corrAcc = \\frac{(TP + TN)}{(TP + TN + FP + FN + incorrectEdit)}$\n",
    "\n",
    "- TP: There is an error on the line, which is corrected.\n",
    "- TN: No error on the line, no correction\n",
    "- FP: No error on the line, makes a correction anyway\n",
    "- FN: There is an error on the line, but it's not corrected\n",
    "- incorrectEdit: Error is identified and corrected, but other words are also corrected which should not be (within the same document). Basically, anything that doesn't fall into the other categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://loicbarrault.github.io/papers/afli_cicling2015.pdf\n",
    "\n",
    "They use BLEU. The Bilingual Evaluation Understudy Score, or BLEU for short, is a metric for evaluating a generated sentence to a reference sentence. A perfect match results in a score of 1.0, whereas a perfect mismatch results in a score of 0.0\n",
    "\n",
    "Also, they use WER (word error rate). To get the WER, start by adding up the substitutions, insertions, and deletions that occur in a sequence of recognized words. Divide that number by the total number of words originally spoken. The result is the WER. To put it in a simple formula, Word Error Rate = (Substitutions + Insertions + Deletions) / Number of Words Spoken\n",
    "\n",
    "But how do you add up those factors? Let’s look at each one:\n",
    "\n",
    "- A substitution occurs when a word gets replaced (for example, “noose” is transcribed as “moose”)\n",
    "- An insertion is when a word is added that wasn’t said (for example, “SAT” becomes “essay tea”)\n",
    "- A deletion happens when a word is left out of the transcript completely (for example, “turn it around” becomes “turn around”)\n",
    "\n",
    "Since we don't consider insertions and deletions, WER doesn't seem very relevant either. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From one of the articles\n",
    "true_line = \"n oven for 15 minute\"\n",
    "\n",
    "# True positive case\n",
    "line_in  = \"n o5en for 15 minute\"\n",
    "line_out = \"n oven for 15 minute\"\n",
    "\n",
    "# True negative case\n",
    "line_in  = \"n oven for 15 minute\"\n",
    "line_out = \"n oven for 15 minute\"\n",
    "\n",
    "# False positive case\n",
    "line_in  = \"n oven for 15 minute\"\n",
    "line_out = \"n oven for 15 m1nute\"\n",
    "\n",
    "# False negative case\n",
    "line_in  = \"n o5en for 15 minute\"\n",
    "line_out = \"n o5en for 15 minute\"\n",
    "\n",
    "# Incorrect edit case\n",
    "line_in  = \"n o5en for 15 minute\"\n",
    "line_out = \"n oven for 15 m1nute\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to fill in missing words using BERT \n",
    "(like in the article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try using bigrams \n",
    "This could catch when an incorrectly scanned word becomes another, correctly spelled word: for example, God -> Cod. The spell checker will not react to this, but it should create a strange bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try some other word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "two approaches: either throw away the misspelled word and try to fill it in from context, or use character level embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try some character level embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
