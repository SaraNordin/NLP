{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing word embeddings for improving OCR accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our idea is to compare how different embeddings affect OCR accuracy, similar to the approach taken in this article: https://medium.com/states-title/using-nlp-bert-to-improve-ocr-accuracy-385c98ae174c (Links to an external site.)\n",
    "\n",
    "We would like to try some embeddings we have learned about in the course, such as CBoW, word2vec, FastText, ELMo, BERT. If we have more time and find other interesting representations online we will evaluate them as well. The idea is to find a baseline OCR accuracy and then exploring if, and how much, this accuracy can be improved by applying word embeddings to the incorrectly scanned words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use a synthetic dataset (read in europarl corpus and corrupt it a bit)\n",
    "- Richard suggests corrupting it in ways that often happen in OCR, like rn <-> m, i <-> l, cl <-> d \n",
    "    - (https://scribenet.com/articles/2016/03/04/how-to-get-the-most-out-of-ocr)\n",
    "- I think he means we should use a spellchecker to find misspelled words, and check with NER that they are not a name\n",
    "- Then maybe we don't throw out the misspelled word, it can be useful\n",
    "- Richard thinks that character level embeddings can be useful here\n",
    "- Evaluation of performance: see what people usually use within OCR. Some ideas:\n",
    "\n",
    "    - https://www.aclweb.org/anthology/I17-1101.pdf\n",
    "    - https://loicbarrault.github.io/papers/afli_cicling2015.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "from enchant.checker import SpellChecker\n",
    "\n",
    "spell = SpellChecker(\"en-UK\")\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Try out spellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell.check(\"cat's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spiel',\n",
       " 'spelt',\n",
       " 'spell',\n",
       " 'seel',\n",
       " 'spec',\n",
       " 'sped',\n",
       " 'spew',\n",
       " 'Opel',\n",
       " 'sp el',\n",
       " 'sp-el']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell.suggest(\"spel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "princedom\n"
     ]
    }
   ],
   "source": [
    "# example - is this a good way to get most relevant word from enchant.spellcheck()?\n",
    "\n",
    "import difflib\n",
    "word=\"prfomnc\"\n",
    "\n",
    "dict,max = {},0\n",
    "a = set(spell.suggest(word))\n",
    "\n",
    "for b in a:\n",
    "    \n",
    "    tmp = difflib.SequenceMatcher(None, word, b).ratio();\n",
    "    dict[tmp] = b\n",
    "    \n",
    "    if tmp > max:\n",
    "        max = tmp\n",
    "\n",
    "print(dict[max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in and corrupt the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character elision is when letter pairs and individual letters are confused by the software. These types of errors occur any time pairs of letters are shaped similarly to other letters. Six common pairs are:\n",
    "\n",
    "rn <-> m, cl <-> d, vv <-> w, ol <-> d, li <-> h, nn <-> m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "elisionArray = []\n",
    "elisionArray.append(['rn', 'm'])\n",
    "elisionArray.append(['ol', 'd'])\n",
    "elisionArray.append(['cl', 'd'])\n",
    "elisionArray.append(['vv', 'w'])\n",
    "elisionArray.append(['li', 'h'])\n",
    "elisionArray.append(['nn', 'm'])\n",
    "elisionArray = np.array(elisionArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## An example of how the character elision will be handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I smilingly scorn this little barn with my lilac yarn and delicious fern\n",
      "--> Replaced  rn  at position  15 \n",
      "I smilingly scom this little barn with my lilac yarn and delicious fern\n",
      "--> Replaced  rn  at position  32 \n",
      "I smilingly scom this little bam with my lilac yarn and delicious fern\n",
      "--> Replaced  rn  at position  51 \n",
      "I smilingly scom this little bam with my lilac yam and delicious fern\n",
      "--> Replaced  rn  at position  70 \n",
      "I smilingly scom this little bam with my lilac yam and delicious fem\n",
      "--> Replaced  li  at position  5 \n",
      "I smihngly scom this little bam with my lilac yam and delicious fem\n",
      "--------------------------------------------------------------------------------\n",
      "Total errors:  5\n",
      "[(0, [(1, 'smilingly'), (2, 'scorn'), (5, 'barn'), (9, 'yarn'), (12, 'fern')])]\n"
     ]
    }
   ],
   "source": [
    "# A test sentence that contains a lot of 'rn' and 'li'\n",
    "line = \"I smilingly scorn this little barn with my lilac yarn and delicious fern\"\n",
    "print(line)\n",
    "\n",
    "new_line = line\n",
    "elision_prob = 0.5\n",
    "\n",
    "# Randomize search order, since some letter pairs overlap \n",
    "elisionArray = np.random.permutation(elisionArray)\n",
    "for pair in elisionArray:\n",
    "    \n",
    "    n_errors = 0\n",
    "    \n",
    "    for m in re.finditer(pair[0], new_line):\n",
    "        \n",
    "        rd = np.random.rand(1)\n",
    "        if rd < elision_prob:\n",
    "            \n",
    "            # Note that the position can't be used later since the line changes length!\n",
    "            print(\"--> Replaced \", pair[0], \" at position \", m.start(), \"\")\n",
    "            \n",
    "            tmp = list(new_line)\n",
    "            tmp[m.start()-n_errors:m.end()-n_errors] = \"%%\"\n",
    "            new_line = \"\".join(tmp)\n",
    "            new_line = new_line.replace(\"%%\", pair[1])\n",
    "            \n",
    "            print(new_line)\n",
    "\n",
    "            # count number of replacements\n",
    "            n_errors += 1\n",
    "            \n",
    "print('-'*80)\n",
    "\n",
    "# Save location and correct spelling for the corrupted words\n",
    "\n",
    "line = list(line.split())\n",
    "new_line = list(new_line.split())\n",
    "\n",
    "total_errors = 0\n",
    "ground_truth = []\n",
    "\n",
    "# This will be the index of a document in the corpus\n",
    "doc_num = 0     \n",
    "\n",
    "# This contains the ground truth for each document \n",
    "tmp = []\n",
    "\n",
    "for j in range(len(line)):\n",
    "    if line[j] != new_line[j]:\n",
    "        \n",
    "        total_errors += 1\n",
    "        tmp.append((j, line[j]))\n",
    "        \n",
    "ground_truth.append((doc_num, tmp))\n",
    "print(\"Total errors: \", total_errors)\n",
    "print(ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply character elision to the Europarl data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_line_errors(line, new_line, total_errors):\n",
    "    \n",
    "    \"\"\"\n",
    "    Check for errors between gold standard document and \"OCR\" document.\n",
    "    \"\"\"\n",
    "\n",
    "    line = list(line.split())\n",
    "    new_line = list(new_line.split()) \n",
    "    truth = []\n",
    "\n",
    "    for j in range(len(line)):\n",
    "        if line[j] != new_line[j]:\n",
    "\n",
    "            total_errors += 1\n",
    "            truth.append((j, line[j]))\n",
    "    \n",
    "    return truth, total_errors\n",
    "\n",
    "\n",
    "def apply_elision(line, elision_array, elision_prob):\n",
    "    \n",
    "    \"\"\"\n",
    "    Apply elision to a line of text. An observed pair will be exchanged with probability elision_prob.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Randomize search order, since some letter pairs overlap \n",
    "    elision_array = np.random.permutation(elision_array)\n",
    "    for elision_pair in elision_array:\n",
    "\n",
    "        # Count number of times each letter pair has been corrupted (since this changes the line length)\n",
    "        n_errors = 0 \n",
    "\n",
    "        for m in re.finditer(elision_pair[0], line):\n",
    "\n",
    "            rd = np.random.rand(1)\n",
    "            if rd < elision_prob:\n",
    "\n",
    "                # Replace the letter pair and convert to a new line\n",
    "                tmp = list(line)\n",
    "                tmp[m.start()-n_errors:m.end()-n_errors] = \"%%\"\n",
    "                line = \"\".join(tmp)\n",
    "                line = line.replace(\"%%\", elision_pair[1])\n",
    "\n",
    "                # count number of replacements\n",
    "                n_errors += 1\n",
    "\n",
    "    return line\n",
    "\n",
    "\n",
    "def read_data(corpus_file, corpus_encoding, max_lines, elision_array, elision_prob):\n",
    "    \n",
    "    \"\"\"\n",
    "    Read in ground truth version of the text, and a (synthetic) corrupted OCR dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    total_errors = 0\n",
    "    ground_truth = []\n",
    "    corrupted_data = []\n",
    "    line_indices = []\n",
    "    elision_errors = []\n",
    "    \n",
    "    with open(corpus_file, encoding = corpus_encoding) as f:\n",
    "        \n",
    "        for d, line in enumerate(f):\n",
    "        \n",
    "            if d == max_lines:\n",
    "                break\n",
    "        \n",
    "            # Apply elision and keep track of which words have been corrupted\n",
    "            new_line = apply_elision(line, elision_array, elision_prob)  \n",
    "            line_truth, total_errors = check_line_errors(line, new_line, total_errors)\n",
    "            \n",
    "            if len(line_truth) > 0:\n",
    "                line_indices.append(d)\n",
    "                elision_errors.append(line_truth)\n",
    "                \n",
    "            # Append original and (potentially) corrupted line\n",
    "            ground_truth.append(line) \n",
    "            corrupted_data.append(new_line)\n",
    "                \n",
    "    elisions = (line_indices, elision_errors)\n",
    "            \n",
    "    return ground_truth, corrupted_data, elisions, total_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_numbers(string):\n",
    "    \n",
    "    \"\"\"\n",
    "    Check if the given string contains any digits.\n",
    "    \"\"\"\n",
    "    \n",
    "    return any(char.isdigit() for char in string)\n",
    "\n",
    "\n",
    "def is_part_of_name(line, word):\n",
    "    \n",
    "    \"\"\"\n",
    "    Using spacy for NER, we check if a specific word is part of a name.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Apply nlp pipeline, check if this \"misspelled word\" is a name\n",
    "    result = nlp(line, disable = ['tagger', 'parser'])\n",
    "    is_name = False\n",
    "\n",
    "    for entity in result.ents:\n",
    "        # If the \"misspelled\" word is part of the name of a person, country etc - we ignore it\n",
    "        if entity.label_ in  [\"PERSON\", \"NORP\", \"GPE\", \"ORG\"] and entity.text.find(word) > -1:\n",
    "            is_name = True\n",
    "\n",
    "    return is_name\n",
    "\n",
    "\n",
    "def identify_spelling_errors(data, ignore):\n",
    "\n",
    "    \"\"\"\n",
    "    Given a dataset and a list of characters to ignore, find misspelled words that are _not_ names.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_misspelled = 0\n",
    "    line_indices = []\n",
    "    spelling_errors = []\n",
    "    spelling_corrected = []\n",
    "    not_in_english = []\n",
    "\n",
    "    for d, line in enumerate(data):\n",
    "\n",
    "        words = line.split()\n",
    "        tmp_1 = []\n",
    "        tmp_2 = []\n",
    "        \n",
    "        for i, word in enumerate(words):\n",
    "\n",
    "            # Some dates and similar are marked as misspelled - ignore words containing numbers!\n",
    "            if not word in ignore and not contains_numbers(word):\n",
    "            \n",
    "                # Apply a spell checker\n",
    "                if not spell.check(word):\n",
    "        \n",
    "                    # Check if the word is part of a name\n",
    "                    if not is_part_of_name(line, word):\n",
    "\n",
    "                        try:\n",
    "                            # Apply a spell correction to the word\n",
    "                            corrected_word = spell.suggest(word)[0]\n",
    "                            \n",
    "                            n_misspelled += 1\n",
    "\n",
    "                            # Note down word and position\n",
    "                            tmp_1.append((i, word))\n",
    "                            tmp_2.append((i, corrected_word))\n",
    "                        \n",
    "                        except IndexError:\n",
    "\n",
    "                            # If no spelling suggestions exist, the line is usually not in English. \n",
    "                            # The EU parlaiment uses many languages,exclude the line just in case.\n",
    "                            not_in_english.append(d)\n",
    "        \n",
    "        # If spelling errors were found, save the line and words.\n",
    "        if len(tmp_1) > 0:\n",
    "            line_indices.append(d)\n",
    "            spelling_errors.append(tmp_1)\n",
    "            spelling_corrected.append(tmp_2)\n",
    "            \n",
    "    spelling_errors = (line_indices, spelling_errors)\n",
    "    spelling_corrected = (line_indices, spelling_corrected)\n",
    "\n",
    "    return spelling_errors, spelling_corrected, n_misspelled, not_in_english"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r\"C:\\Users\\saran\\OneDrive\\Dokument\\GitHub\\NLP\\Project5\\europarl.txt\"\n",
    "encoding = \"utf-8\"\n",
    "\n",
    "# Probability that a letter pair from our list will be confused if it is seen in the text\n",
    "elision_prob = 0.1\n",
    "max_lines = 10000\n",
    "\n",
    "np.random.seed(0)\n",
    "ground_truth, data, elisions, n_errors = read_data(file, encoding, max_lines, elisionArray, elision_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Words and characters for the spellchecker to ignore\n",
    "ignore = [\",\", \".\", '\"', \"(\", \")\", \"-\", \"'\", \"!\", \"?\", \":\", \";\", \"/\", \"n't\", \"'s\", \"'m\", \"%\", \"--\", \"``\", \"___LANGCODE___\"]\n",
    "spell_errors, spell_corrected, n_misspelled, excl_lines = identify_spelling_errors(data, ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "excl_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasNumbers(\"G20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN\n"
     ]
    }
   ],
   "source": [
    "result = nlp(\"G20\")\n",
    "\n",
    "for token in result:\n",
    "    print(token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elisions[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([15, 18, 21, 43, 48, 72, 80, 81],\n",
       " [[(8, 'clear')],\n",
       "  [(50, 'families')],\n",
       "  [(23, 'application')],\n",
       "  [(13, 'ultra-liberal')],\n",
       "  [(6, 'policy')],\n",
       "  [(1, 'concerns')],\n",
       "  [(3, 'like')],\n",
       "  [(28, 'Solbes')]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([18, 19, 21, 30, 43, 48, 59, 72, 80, 81, 83],\n",
       " [[(50, 'famishes')],\n",
       "  [(3, 'self employed')],\n",
       "  [(23, 'application')],\n",
       "  [(13, 'renationalised')],\n",
       "  [(13, 'ultra-herbal')],\n",
       "  [(6, 'poncy')],\n",
       "  [(17, 'black-racketeering')],\n",
       "  [(1, 'conc ems')],\n",
       "  [(3, 'he')],\n",
       "  [(28, 'Besides')],\n",
       "  [(30, 'travelogue')]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of identified misspelled words:  12\n",
      "Number of synthetic errors:  8\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of identified misspelled words: \", n_misspelled)\n",
    "print(\"Number of synthetic errors: \", n_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute baseline accuracy using spellcheck suggestions\n",
    "\n",
    "baseline: use top suggested word from enchant spellchecker\n",
    "\n",
    "so I guess we create a list of misspelled words just like doc_errors, apply the suggestion for each, and see if we get closer to the ground truth or not?\n",
    "\n",
    "- https://www.aclweb.org/anthology/I17-1101.pdf\n",
    "\n",
    "[From the article] For evaluation, we use the CER (Character Error Rate) metric as defined in OCR post-correction evaluations:\n",
    "\n",
    "$$ CER = \\frac{S + D + I}{S + D + C} $$\n",
    "\n",
    "Where S refers to the number of substituted characters in the OCR text (w.r.t. the reference texts), D to the number of deleted characters, I to the number of inserted characters and C to the number of ‘correct’ characters. We use the CER metric when comparing to the baseline system. __Is this useful in the character elision setting? Doesn't look like it__\n",
    "\n",
    "We want to measure the systems performance per input, rather than per character. We therefore introduce two complementary accuracy-based evaluation metrics, which evaluate on the level of the character window:\n",
    "- detection accuracy (detAcc) shows the proportion of correctly detected errors and nonerrors in the evaluated set of 20-character strings\n",
    "- correction accuracy (corrAcc) reflects the ability of the language model to accurately correct corrupted strings without overgenerating and editing non-corrupted strings\n",
    "\n",
    "These metrics are calculated as follows:\n",
    "\n",
    "$detAcc = \\frac{(TP + TN + incorrectEdit)}{(TP + TN + FP + FN + incorrectEdit)}$\n",
    "\n",
    "$corrAcc = \\frac{(TP + TN)}{(TP + TN + FP + FN + incorrectEdit)}$\n",
    "\n",
    "- TP: There is an error on the line, which is corrected.\n",
    "- TN: No error on the line, no correction\n",
    "- FP: No error on the line, makes a correction anyway\n",
    "- FN: There is an error on the line, but it's not corrected\n",
    "- incorrectEdit: Error is identified and corrected, but other words are also corrected which should not be (within the same document). Basically, anything that doesn't fall into the other categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://loicbarrault.github.io/papers/afli_cicling2015.pdf\n",
    "\n",
    "They use BLEU. The Bilingual Evaluation Understudy Score, or BLEU for short, is a metric for evaluating a generated sentence to a reference sentence. A perfect match results in a score of 1.0, whereas a perfect mismatch results in a score of 0.0\n",
    "\n",
    "Also, they use WER (word error rate). To get the WER, start by adding up the substitutions, insertions, and deletions that occur in a sequence of recognized words. Divide that number by the total number of words originally spoken. The result is the WER. To put it in a simple formula, Word Error Rate = (Substitutions + Insertions + Deletions) / Number of Words Spoken\n",
    "\n",
    "But how do you add up those factors? Let’s look at each one:\n",
    "\n",
    "- A substitution occurs when a word gets replaced (for example, “noose” is transcribed as “moose”)\n",
    "- An insertion is when a word is added that wasn’t said (for example, “SAT” becomes “essay tea”)\n",
    "- A deletion happens when a word is left out of the transcript completely (for example, “turn it around” becomes “turn around”)\n",
    "\n",
    "Since we don't consider insertions and deletions, WER doesn't seem very relevant either. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From one of the articles\n",
    "true_line = \"In oven for 15 minutes\"\n",
    "\n",
    "# True positive case\n",
    "line_in  = \"In o5en for 15 minutes\"\n",
    "line_out = \"In oven for 15 minutes\"\n",
    "\n",
    "# True negative case\n",
    "line_in  = \"In oven for 15 minutes\"\n",
    "line_out = \"In oven for 15 minutes\"\n",
    "\n",
    "# False positive case\n",
    "line_in  = \"In oven for 15 minutes\"\n",
    "line_out = \"In oven for 15 m1nutes\"\n",
    "\n",
    "# False negative case\n",
    "line_in  = \"In o5en for 15 minutes\"\n",
    "line_out = \"In o5en for 15 minutes\"\n",
    "\n",
    "# Incorrect edit case\n",
    "line_in  = \"In o5en for 15 minutes\"\n",
    "line_out = \"In oven for 15 m1nutes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_scores(elisions, corrections, data):\n",
    "    \n",
    "    elision_idx = elisions[0]\n",
    "    elision_words = elisions[1]\n",
    "    corrected_idx = corrections[0]\n",
    "    corrected_words = corrections[1]\n",
    "    \n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    incorrect_edits = 0\n",
    "    \n",
    "    # True positives = number of correct changes\n",
    "    for pos1, idx in enumerate(elision_idx):\n",
    "        if idx in corrected_idx:\n",
    "            \n",
    "            pos2 = corrected_idx.index(idx)\n",
    "            \n",
    "            if elision_words[pos1] == corrected_words[pos2]:\n",
    "                #print(\"TP: \", idx, elision_words[pos1], corrected_words[pos2])\n",
    "                TP += 1\n",
    "            else:\n",
    "                #print(\"Incorrect edit: \", idx, elision_words[pos1], corrected_words[pos2])\n",
    "                incorrect_edits += 1\n",
    "\n",
    "    print(\"True positives: \", TP)  \n",
    "    \n",
    "    # True negatives = number of lines that do not have elision, and that have not been touched by spellchecker\n",
    "    corpus_size = len(data)\n",
    "    all_touched_lines = set(corrected_idx) | set(elision_idx)\n",
    "    TN = corpus_size - len(all_touched_lines)\n",
    "    print(\"True negatives: \", TN)\n",
    "    \n",
    "    # False negatives = lines that should have been changed, but were not\n",
    "    false_neg_lines = set(elision_idx) - set(corrected_idx)\n",
    "    FN = len(false_neg_lines)\n",
    "    print(\"False negatives: \", FN)\n",
    "    \n",
    "    # False positives = there was no elision, but a correction was made anyway\n",
    "    false_pos_lines = set(corrected_idx) - set(elision_idx)\n",
    "    FP = len(false_pos_lines)\n",
    "    print(\"False positives: \", FP)\n",
    "    \n",
    "    # Instances when the right line has been edited, but incorrectly\n",
    "    print(\"Incorrect edits: \", incorrect_edits)\n",
    "    \n",
    "    if FP + FN + TN + TP + incorrect_edits != corpus_size:\n",
    "        print(\"ERROR: These scores don't add up to the number of lines!\")\n",
    "        \n",
    "    det_accuracy = (TP + TN + incorrect_edits)/corpus_size\n",
    "    corr_accuracy = (TP + TN)/corpus_size\n",
    "    \n",
    "    return det_accuracy, corr_accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([15, 18, 21, 43, 48, 72, 80, 81],\n",
       " [[(8, 'clear')],\n",
       "  [(50, 'families')],\n",
       "  [(23, 'application')],\n",
       "  [(13, 'ultra-liberal')],\n",
       "  [(6, 'policy')],\n",
       "  [(1, 'concerns')],\n",
       "  [(3, 'like')],\n",
       "  [(12, 'politics')]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([18, 19, 21, 30, 43, 48, 59, 72, 80, 81, 83],\n",
       " [[(50, 'famishes')],\n",
       "  [(3, 'self employed')],\n",
       "  [(23, 'application')],\n",
       "  [(13, 'renationalised')],\n",
       "  [(13, 'ultra-herbal')],\n",
       "  [(6, 'poncy')],\n",
       "  [(17, 'black-racketeering')],\n",
       "  [(1, 'conc ems')],\n",
       "  [(3, 'he')],\n",
       "  [(12, 'politics'), (28, 'Solves')],\n",
       "  [(30, 'travelogue')]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TN + FN = len(spell_corrected[0])\n",
    "spell_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives:  1\n",
      "True negatives:  88\n",
      "False negatives:  1\n",
      "False positives:  4\n",
      "Incorrect edits:  6\n"
     ]
    }
   ],
   "source": [
    "det_acc, corr_acc = compute_accuracy_scores(elisions, spell_corrected, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to fill in missing words using BERT \n",
    "(like in the article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try using bigrams \n",
    "This could catch when an incorrectly scanned word becomes another, correctly spelled word: for example, God -> Cod. The spell checker will not react to this, but it should create a strange bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try some other word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "two approaches: either throw away the misspelled word and try to fill it in from context, or use character level embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try some character level embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
