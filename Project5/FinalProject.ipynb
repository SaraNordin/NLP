{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treating OCR character elision using BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optical Character Recognition, or OCR for short, is the process of taking an image of text and making it machine readable. This technique is dependent on two areas of machine learning: computer vision and natural language processing. One needs a good vision model to identify words and paragraphs, and ideally also a language model to find and clean up errors outputted from the scanned text. Here we will focus on the post-correction stage of the OCR process. \n",
    "\n",
    "One issue that can arise within OCR is that a pair of characters are mistaken for another character, because their shapes are similar. This is called character elision. An elision that appears to be quite common is when the computer vision software mistkes `rn` for the letter `m`. This kind of issue seems relatively common within OCR, and could be corrected using NLP concepts and models. \n",
    "\n",
    "#### Detecting and correcting elisions\n",
    "\n",
    "We want our code to accurately correct elisions, but first it needs to detect them. When elision happens there are two possible outcomes: either we have a `legal word error`, or the elision results in nonsense. With a legal error, the eliased word is still part of the English language, like when `modern` is interpreted as `modem`. These errors would require quite advanced, and expensive, language modelling to score each word in the corpus and determine which ones don't see, to match \n",
    "\n",
    "\n",
    "\n",
    "The goal for this project is to explore how using BERT can improve the correction accry\n",
    "\n",
    "\n",
    "for a given vision output: different levels of sophistication to detect and correct for errors in the text. \n",
    "\n",
    "\n",
    "Two outcomes when elision happens: either the resulting word is a ''legal error'' (the resulting word is still a part of the English language, modern becomes modem), or it outputs nonsense. The first case would require quite advanced, and expensive, language modelling to score each word in the corpus and attempt to correct the ones that don't seem to match the context. In the cases where the eliased word is not a valid English word, it's of course easier to find. \n",
    "\n",
    "Our idea is to compare how using BERT can improve the correction accuracy of eliased text, similar to the approach taken in this article: https://medium.com/states-title/using-nlp-bert-to-improve-ocr-accuracy-385c98ae174c \n",
    "\n",
    "Synthetic elisions in a dataset consisting of Europarl transcripts. Use a spell checker to detect misspelled words, and give suggested replacements. This is the baseline, to be compared against the original dataset. then exploring if, and how much, this accuracy can be improved by applying word embeddings to the incorrectly scanned words. \n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "Drawbacks of using correction accuracy, consider using cosine distance. But using the old letters we are pretty certain that we preserve the word shape and the context - most likely we find the right word, I hope. \n",
    "\n",
    "our detection accuracy is high enough, spell checker is fine for finding elisions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Richard thinks that character level embeddings can be useful here\n",
    "- Evaluation of performance: see what people usually use within OCR. Some ideas:\n",
    "\n",
    "    - https://www.aclweb.org/anthology/I17-1101.pdf\n",
    "    - https://loicbarrault.github.io/papers/afli_cicling2015.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try on another dataset and see what happens!\n",
    "Implement other masked LM models and compare\n",
    "\n",
    "Ideas for improvements: finding errors, and correcting them. \n",
    "- Finding errors can of course be done better than with the spellchecker. Assign a score to each word based on the context? But this seems expensive.\n",
    "- Correcting them: character based model, more suggestions from BERT somehow? Another model (albert, electra)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import spacy\n",
    "import torch\n",
    "import difflib\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from enchant.checker import SpellChecker \n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "spell = SpellChecker(\"en-UK\")\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Installation instructions for enchant package: pip install pyenchant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in corpus and apply elision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on [this article](https://medium.com/states-title/using-nlp-bert-to-improve-ocr-accuracy-385c98ae174c) and suggestions from Richard, we have decided on six elisions to apply to the text:\n",
    "\n",
    "1. rn $\\to$ m\n",
    "2. ol $\\to$ d\n",
    "3. cl $\\to$ d\n",
    "4. vv $\\to$ w\n",
    "5. li $\\to$ h\n",
    "6. nn $\\to$ m\n",
    "\n",
    "When the text is read in, we search for occurences of the six letter pairs to the left. Each time a pair is found it is eliased to the corresponding letter on the right, with a certain elision probability. This is intended to reflect the fact that elision might not occur every time the letter pairs are observed. \n",
    "\n",
    "With some words, like `cliff`, it matters in what order we look for the letter pairs. If we find the `cl` and replace it with `d`, the resulting word is `diff`. However if we look for the `li` pair first, the resulting word will be `chff`. To avoid bias in the synthetic elisions, the search order will be randomised for each new document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An array containing the elision transformations\n",
    "elisionArray = []\n",
    "elisionArray.append(['rn', 'm'])\n",
    "elisionArray.append(['ol', 'd'])\n",
    "elisionArray.append(['cl', 'd'])\n",
    "elisionArray.append(['vv', 'w'])\n",
    "elisionArray.append(['li', 'h'])\n",
    "elisionArray.append(['nn', 'm'])\n",
    "elisionArray = np.array(elisionArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Elision example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We now give an example of how the elisions will be performed, and how we store the results in an accessible way. The function `elision_example` will process a nonsense sentence containing a lot of `rn` and `li`.  Stepping through the sentence, some of the words are read incorrectly. To what extent the line is corrupted depends on the elision probability inputted to the function.\n",
    "\n",
    "There is a counter for the total number of elisions present in the text, and information about them is stored in a data structure. For each eliased word we save its position in the document, and the original spelling. This is combined to a vector and tupled with the document number in the corpus; this is set to 0 in this case since we only have one line of text. For a single document that has N synthetic elisions, the resulting data structure will look like this:  \n",
    "\n",
    "> [ ( doc_idx, [ ( word_pos_1, original_word_1 ), ..., ( word_pos_N, original_word_N ) ] ) ]\n",
    "\n",
    "Another way to access the same information would be to compare the ground truth dataset with the eliased dataset on a line by line basis. Since comparisons will need to be done multiple times, the data structure described above was introduced to save computational time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def elision_example(elisionArray, elision_prob):\n",
    "\n",
    "    \"\"\"\n",
    "    A function illustrating how the character elision will be handled in this project. \n",
    "    Later code follow the same principle, but with more help functions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # A test sentence that contains a lot of 'rn' and 'li'\n",
    "    line = \"I scorn this little barn with my lilac yarn and delicious fern\"\n",
    "    print(\"Original line:\")\n",
    "    print(line)\n",
    "    print('-'*80)\n",
    "\n",
    "    new_line = line\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Randomize search order, since some letter pairs overlap \n",
    "    elisionArray = np.random.permutation(elisionArray)\n",
    "    \n",
    "    # Loop over all pairs and apply elisions\n",
    "    for pair in elisionArray:\n",
    "        \n",
    "        n_errors = 0\n",
    "        \n",
    "        for m in re.finditer(pair[0], new_line):\n",
    "\n",
    "            rd = np.random.rand(1)\n",
    "            if rd < elision_prob:\n",
    "\n",
    "                # Note that the position can't be used later since the line changes length!\n",
    "                print(\"--> Replaced \", pair[0], \" at position \", m.start(), \"\")\n",
    "\n",
    "                # Do some line magic\n",
    "                tmp = list(new_line)\n",
    "                tmp[m.start()-n_errors:m.end()-n_errors] = \"%%\"\n",
    "                new_line = \"\".join(tmp)\n",
    "                new_line = new_line.replace(\"%%\", pair[1])\n",
    "                print(new_line)\n",
    "\n",
    "                # count number of replacements\n",
    "                n_errors += 1\n",
    "\n",
    "    line = list(line.split())\n",
    "    new_line = list(new_line.split())\n",
    "    total_errors = 0\n",
    "    ground_truth = []\n",
    "\n",
    "    # This will be the index of a document in the corpus\n",
    "    doc_num = 0     \n",
    "\n",
    "    # This contains the ground truth for all eliased words\n",
    "    tmp = []\n",
    "\n",
    "    # Save location and correct spelling for the corrupted words\n",
    "    for j in range(len(line)):\n",
    "        if line[j] != new_line[j]:\n",
    "\n",
    "            total_errors += 1\n",
    "            tmp.append((j, line[j]))\n",
    "\n",
    "    ground_truth.append((doc_num, tmp))\n",
    "    print('-'*80)\n",
    "    print(\"Total elisions: \", total_errors)\n",
    "    print(ground_truth)\n",
    "    np.random.seed(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original line:\n",
      "I scorn this little barn with my lilac yarn and delicious fern\n",
      "--------------------------------------------------------------------------------\n",
      "--> Replaced  rn  at position  22 \n",
      "I scorn this little bam with my lilac yarn and delicious fern\n",
      "--> Replaced  li  at position  13 \n",
      "I scorn this httle bam with my lilac yarn and delicious fern\n",
      "--------------------------------------------------------------------------------\n",
      "Total elisions:  2\n",
      "[(0, [(3, 'little'), (4, 'barn')])]\n"
     ]
    }
   ],
   "source": [
    "elision_example(elisionArray, elision_prob = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Apply character elision to the Europarl data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Working with europarl, in the hope that the language will be consistent and that there won't be a lot of spelling errors.\n",
    "\n",
    "We read in part of the corpus and apply elisions. Like in the example above, they are inserted randomly and their positions and original spellings are saved. \n",
    "\n",
    "We then apply a spell checker to detect errors in the eliased corpus. We provide some words and characters for the spellchecker to ignore, and make sure that any detected misspelled words are not part of a name. \n",
    "\n",
    "While spell checking, some words might appear that have no spelling suggestions. These words tend to be in French or German, since the EU parliament uses these languages occasionally in quotes. It's not fair to ask the spell checker to handle these words, and the purpose here is not to handle multiple different languages. However the impact of this is minimal: in the first 10 000 documents of the corpus, only around 10 are affected. Thus we can safely ignore this language issue in this corpus.\n",
    "\n",
    "also a train/ test split that can be used for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def check_line_errors(line, new_line, total_errors):\n",
    "    \n",
    "    \"\"\"\n",
    "    Check for errors between gold standard document and eliased document.\n",
    "    \n",
    "    Returns: \n",
    "        truth: For each word mismatch, note down the word position and the original word. \n",
    "        total_errors: A counter for the total number of synthetic elisions in the corpus. \n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    line = list(line.split())\n",
    "    new_line = list(new_line.split()) \n",
    "    truth = []\n",
    "\n",
    "    for j in range(len(line)):\n",
    "        if line[j] != new_line[j]:\n",
    "\n",
    "            total_errors += 1\n",
    "            truth.append((j, line[j]))\n",
    "    \n",
    "    return truth, total_errors\n",
    "\n",
    "\n",
    "def apply_elision(line, elision_array, elision_prob):\n",
    "    \n",
    "    \"\"\"\n",
    "    Apply elision to a line of text. Requires an array with elisions, where letter pairs are in the first column and the \n",
    "    resulting letter after elision is in the second column. Whenever one of these letter pairs is observed in the text, \n",
    "    it eliased with probability elision_prob.\n",
    "    \n",
    "    Returns: \n",
    "        new_line: The line of text after elision has (potentially) occured. \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Randomize search order, since some letter pairs overlap \n",
    "    elision_array = np.random.permutation(elision_array)\n",
    "    for elision_pair in elision_array:\n",
    "\n",
    "        # Count number of times each letter pair has been corrupted (since this changes the line length)\n",
    "        n_errors = 0 \n",
    "\n",
    "        for m in re.finditer(elision_pair[0], line):\n",
    "\n",
    "            rd = np.random.rand(1)\n",
    "            if rd < elision_prob:\n",
    "\n",
    "                # Replace the letter pair and convert to a new line\n",
    "                tmp = list(line)\n",
    "                tmp[m.start()-n_errors:m.end()-n_errors] = \"%%\"\n",
    "                line = \"\".join(tmp)\n",
    "                line = line.replace(\"%%\", elision_pair[1])\n",
    "\n",
    "                # count number of replacements\n",
    "                n_errors += 1\n",
    "\n",
    "    return line\n",
    "\n",
    "\n",
    "def read_data(corpus_file, corpus_encoding, max_lines, elision_array, elision_prob):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given a file path and encoding, as well as a maximum number of lines, this function reads in a corpus and \n",
    "    saves it as a list of documents. It also creates and saves an eliased version of the same corpus. \n",
    "        \n",
    "    Returns:\n",
    "        ground_truth: The original corpus.\n",
    "        corrupted_data: The corrupted corpus, where some character elisions have been performed.\n",
    "        elisions: Information about the elisions. Contains the document indices, line positions, and original spellings.\n",
    "        total_errors: A counter for the total number of synthetic elisions applied to the corpus.\n",
    "          \n",
    "    \"\"\"\n",
    "    \n",
    "    total_errors = 0\n",
    "    ground_truth = []\n",
    "    corrupted_data = []\n",
    "    line_indices = []\n",
    "    elision_errors = []\n",
    "    \n",
    "    with open(corpus_file, encoding = corpus_encoding) as f:\n",
    "        \n",
    "        for d, line in enumerate(f):\n",
    "        \n",
    "            if d == max_lines:\n",
    "                break\n",
    "        \n",
    "            # Apply elision and keep track of which words have been corrupted\n",
    "            new_line = apply_elision(line, elision_array, elision_prob)  \n",
    "            line_truth, total_errors = check_line_errors(line, new_line, total_errors)\n",
    "            \n",
    "            if len(line_truth) > 0:\n",
    "                line_indices.append(d)\n",
    "                elision_errors.append(line_truth)\n",
    "                \n",
    "            # Append original and (potentially) corrupted line\n",
    "            ground_truth.append(line) \n",
    "            corrupted_data.append(new_line)\n",
    "                \n",
    "    elisions = (line_indices, elision_errors)\n",
    "            \n",
    "    return ground_truth, corrupted_data, elisions, total_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def contains_numbers(string):\n",
    "    \n",
    "    \"\"\"\n",
    "    Check if the given string contains any digits.\n",
    "    \"\"\"\n",
    "    \n",
    "    return any(char.isdigit() for char in string)\n",
    "\n",
    "\n",
    "def is_part_of_name(line, word):\n",
    "    \n",
    "    \"\"\"\n",
    "    Using spacy for NER, we check if a specific word in a line of text appears to be part of a name.\n",
    "    Check if the word is a subset of the name of a person, nationality, company, or country. \n",
    "    \n",
    "    Returns:\n",
    "        is_name: a boolean which is True if the word appears to be part of a name.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Apply nlp pipeline, check if this \"misspelled word\" is a name\n",
    "    result = nlp(line, disable = ['tagger', 'parser'])\n",
    "    is_name = False\n",
    "\n",
    "    for entity in result.ents:\n",
    "        # If the \"misspelled\" word is part of the name of a person, country etc - we ignore it\n",
    "        if entity.label_ in  [\"PERSON\", \"NORP\", \"GPE\", \"ORG\"] and entity.text.find(word) > -1:\n",
    "            is_name = True\n",
    "\n",
    "    return is_name\n",
    "\n",
    "\n",
    "def identify_spelling_errors(data, ignore):\n",
    "\n",
    "    \"\"\"\n",
    "    Given a dataset and a list of characters to ignore, find misspelled words. The word positions are saved, and\n",
    "    we also store the spellchecker's suggested spelling. Only words that are not part of a name are considered.\n",
    "    \n",
    "    Returns:\n",
    "        spelling_errors: A data structure containing document indices and line positions for misspelled words.\n",
    "        spelling_corrected: A data structure containing the same indices as above, but with a spell check applied.\n",
    "        n_misspelled: A counter for the total number of misspelled words in the text. \n",
    "        not_in_english: A list of lines where spelling suggestion was not possible; usually because word is not in English.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    n_misspelled = 0\n",
    "    line_indices = []\n",
    "    spelling_errors = []\n",
    "    spelling_corrected = []\n",
    "    not_in_english = []\n",
    "\n",
    "    for d, line in enumerate(data):\n",
    "\n",
    "        words = line.split()\n",
    "        tmp_1 = []\n",
    "        tmp_2 = []\n",
    "        \n",
    "        for i, word in enumerate(words):\n",
    "\n",
    "            # Some dates and similar are marked as misspelled - ignore words containing numbers!\n",
    "            if not word in ignore and not contains_numbers(word):\n",
    "            \n",
    "                # Apply a spell checker\n",
    "                if not spell.check(word):\n",
    "        \n",
    "                    # Check if the word is part of a name\n",
    "                    if not is_part_of_name(line, word):\n",
    "\n",
    "                        try:\n",
    "                            # Apply a spell correction to the word\n",
    "                            corrected_word = spell.suggest(word)[0]\n",
    "                            \n",
    "                            # Note down word and position\n",
    "                            tmp_1.append((i, word))\n",
    "                            tmp_2.append((i, corrected_word))\n",
    "                            n_misspelled += 1\n",
    "\n",
    "                        except IndexError:\n",
    "\n",
    "                            # If no spelling suggestions exist, note the document index. \n",
    "                            not_in_english.append(d)\n",
    "        \n",
    "        # If spelling errors were found, save the line index and words.\n",
    "        if len(tmp_1) > 0:\n",
    "            line_indices.append(d)\n",
    "            spelling_errors.append(tmp_1)\n",
    "            spelling_corrected.append(tmp_2)\n",
    "            \n",
    "    spelling_errors = (line_indices, spelling_errors)\n",
    "    spelling_corrected = (line_indices, spelling_corrected)\n",
    "\n",
    "    return spelling_errors, spelling_corrected, n_misspelled, not_in_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def match_and_split(indices, datastruct):\n",
    "    \n",
    "    \"\"\"\n",
    "    Match data structure index component to an array, and keep the overlap.\n",
    "    \"\"\"\n",
    "    \n",
    "    tmp_idx = []\n",
    "    tmp_info = []\n",
    "    \n",
    "    for i in indices:\n",
    "        if i in datastruct[0]:\n",
    "            tmp_idx.append(i)\n",
    "            j = datastruct[0].index(i)\n",
    "            tmp_info.append(datastruct[1][j])\n",
    "            \n",
    "    return (tmp_idx, tmp_info)\n",
    "\n",
    "def train_test_split(data, elisions, spell_errors, spell_corrected, train_frac = 0.7):\n",
    "    \n",
    "    \"\"\"\n",
    "    Split dataset and all related data structures into a training and test part. \n",
    "    \n",
    "    Returns (train, test) where:\n",
    "        train = (train_data, train_lines, train_elisions, train_errors, train_corrected)\n",
    "        test = (test_data, test_lines, test_elisions, test_errors, test_corrected)\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    train_len = int(len(data)*train_frac)\n",
    "    \n",
    "    line_vec = np.random.permutation(len(data))\n",
    "    train_lines = np.sort(line_vec[:train_len]).tolist()\n",
    "    test_lines = np.sort(line_vec[train_len:]).tolist()\n",
    "    \n",
    "    train_data = []\n",
    "    for i in train_lines:\n",
    "        train_data.append(data[i])\n",
    "\n",
    "    train_elisions = match_and_split(train_lines, elisions)\n",
    "    train_errors = match_and_split(train_lines, spell_errors)\n",
    "    train_corrected = match_and_split(train_lines, spell_corrected)\n",
    "    \n",
    "    test_data = []\n",
    "    for i in test_lines:\n",
    "        test_data.append(data[i])\n",
    "        \n",
    "    test_elisions = match_and_split(test_lines, elisions)\n",
    "    test_errors = match_and_split(test_lines, spell_errors)\n",
    "    test_corrected = match_and_split(test_lines, spell_corrected)\n",
    "            \n",
    "    return ((train_data, train_lines, train_elisions, train_errors, train_corrected), \n",
    "            (test_data, test_lines, test_elisions, test_errors, test_corrected))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of synthetic errors:  5821\n",
      "Number of identified misspelled words:  5806\n",
      "Number of lines where no spelling suggestion could be given: 6\n"
     ]
    }
   ],
   "source": [
    "file = \"europarl.txt\"\n",
    "encoding = \"utf-8\"\n",
    "\n",
    "elision_prob = 0.5\n",
    "max_lines = 10000\n",
    "np.random.seed(None)\n",
    "\n",
    "# Words and characters for the spellchecker to ignore\n",
    "ignore = [\",\", \".\", '\"', \"(\", \")\", \"-\", \"'\", \"!\", \"?\", \":\", \";\", \"/\", \n",
    "          \"n't\", \"'s\", \"'m\", \"%\", \"--\", \"``\", \"___LANGCODE___\", \"''\"]\n",
    "\n",
    "ground_truth, data, elisions, n_elisions = read_data(file, encoding, max_lines, elisionArray, elision_prob)\n",
    "spell_errors, spell_corrected, n_misspelled, excl_lines = identify_spelling_errors(data, ignore)\n",
    "\n",
    "# Bundle relevant information\n",
    "lines = list(range(len(data)))\n",
    "bundle = (data, lines, elisions, spell_errors, spell_corrected)\n",
    "\n",
    "print(\"Number of synthetic errors: \", n_elisions)\n",
    "print(\"Number of identified misspelled words: \", n_misspelled)\n",
    "print(\"Number of lines where no spelling suggestion could be given:\", len(excl_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Apply a train test split\n",
    "(train, test) = train_test_split(data, elisions, spell_errors, spell_corrected, train_frac = 0.7)\n",
    "train_data, train_lines, train_elisions, train_errors, train_corrected = train\n",
    "test_data, test_lines, test_elisions, test_errors, test_corrected = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute baseline accuracy using spellcheck suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[From this article](https://www.aclweb.org/anthology/I17-1101.pdf) \n",
    "For evaluation, they use the CER (Character Error Rate) metric, which does not seem to be useful in this setting. We want to measure the systems performance per input, rather than per character. We therefore introduce two accuracy-based evaluation metrics (also mentioned in the article), which evaluate on the level of the character window:\n",
    "\n",
    "- detection accuracy (detAcc) shows the proportion of correctly detected errors and nonerrors in the evaluated set of strings\n",
    "- correction accuracy (corrAcc) reflects the ability of the language model to accurately correct corrupted strings without overgenerating and editing non-corrupted strings\n",
    "\n",
    "These metrics are calculated as follows:\n",
    "\n",
    "$detAcc = \\frac{(TP + TN + incorrectEdit)}{(TP + TN + FP + FN + incorrectEdit)}$\n",
    "\n",
    "$corrAcc = \\frac{(TP + TN)}{(TP + TN + FP + FN + incorrectEdit)}$\n",
    "\n",
    "- TP: There is an error on the line, which is corrected.\n",
    "- TN: No error on the line, no correction\n",
    "- FP: No error on the line, makes a correction anyway\n",
    "- FN: There is an error on the line, but it's not corrected\n",
    "- incorrectEdit: Error is identified, but the correction is wrong.\n",
    "\n",
    "\n",
    "Not all documents are the same length, should this be adjusted for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy_scores(bundle, verbose = True, corrections = None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute detection and correction accuracy according to definitions above.\n",
    "    \n",
    "    Returns:\n",
    "        det_accuracy: detection accuracy\n",
    "        corr_accuracy: correction accuracy\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Either use provided corrections, or the one in the bundle\n",
    "    if corrections == None:\n",
    "        data, _ , elisions, _, corrections = bundle \n",
    "    else: \n",
    "        data, _ , elisions, _, _ = bundle \n",
    "    \n",
    "    elision_idx = elisions[0]\n",
    "    elision_words = elisions[1]\n",
    "    corrected_idx = corrections[0]\n",
    "    corrected_words = corrections[1]\n",
    "    \n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    incorrect_edits = 0\n",
    "    \n",
    "    # True positives = number of correct changes\n",
    "    for pos1, idx in enumerate(elision_idx):\n",
    "        if idx in corrected_idx:\n",
    "            \n",
    "            pos2 = corrected_idx.index(idx)\n",
    "            \n",
    "            if elision_words[pos1] == corrected_words[pos2]:\n",
    "                TP += 1\n",
    "            else:\n",
    "                incorrect_edits += 1\n",
    "\n",
    "    # True negatives = number of lines that do not have elision, and that have not been touched by spellchecker\n",
    "    corpus_size = len(data)\n",
    "    all_touched_lines = set(corrected_idx) | set(elision_idx)\n",
    "    TN = corpus_size - len(all_touched_lines)\n",
    "    \n",
    "    # False negatives = lines that should have been changed, but were not\n",
    "    false_neg_lines = set(elision_idx) - set(corrected_idx)\n",
    "    FN = len(false_neg_lines)\n",
    "    \n",
    "    # False positives = there was no elision, but a correction was made anyway\n",
    "    false_pos_lines = set(corrected_idx) - set(elision_idx)\n",
    "    FP = len(false_pos_lines)\n",
    "        \n",
    "    # Double check that it all adds up\n",
    "    if FP + FN + TN + TP + incorrect_edits != corpus_size:\n",
    "        print(\"ERROR: These scores don't add up to the number of lines!\")\n",
    "        \n",
    "    # Compute accuracy scores    \n",
    "    det_accuracy = (TP + TN + incorrect_edits)/corpus_size\n",
    "    corr_accuracy = (TP + TN)/corpus_size\n",
    "    \n",
    "    if verbose == True:\n",
    "        \n",
    "        print(\"True positives: \", TP)  \n",
    "        print(\"True negatives: \", TN)\n",
    "        print(\"False negatives: \", FN)\n",
    "        print(\"False positives: \", FP)\n",
    "        print(\"Incorrect edits: \", incorrect_edits)\n",
    "        print(\"-\"*30)\n",
    "        print(\"Detection accuracy: \", np.round(det_accuracy, 4))\n",
    "        print(\"Correction accuracy: \", np.round(corr_accuracy, 4))\n",
    "        print(\"=\"*40)\n",
    "    \n",
    "    return det_accuracy, corr_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives:  738\n",
      "True negatives:  5580\n",
      "False negatives:  335\n",
      "False positives:  294\n",
      "Incorrect edits:  3053\n",
      "------------------------------\n",
      "Detection accuracy:  0.9371\n",
      "Correction accuracy:  0.6318\n",
      "========================================\n",
      "True positives:  531\n",
      "True negatives:  3911\n",
      "False negatives:  227\n",
      "False positives:  216\n",
      "Incorrect edits:  2115\n",
      "------------------------------\n",
      "Detection accuracy:  0.9367\n",
      "Correction accuracy:  0.6346\n",
      "========================================\n",
      "True positives:  207\n",
      "True negatives:  1669\n",
      "False negatives:  108\n",
      "False positives:  78\n",
      "Incorrect edits:  938\n",
      "------------------------------\n",
      "Detection accuracy:  0.938\n",
      "Correction accuracy:  0.6253\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "det_acc_full, corr_acc_full = compute_accuracy_scores(bundle)\n",
    "det_acc_train, corr_acc_train = compute_accuracy_scores(train)\n",
    "det_acc_test, corr_acc_test = compute_accuracy_scores(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill in missing words using BERT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so far we have created synthetic elisions, and attemmpted to find them using a spellchecker. Next up: correct the misspelled words using BERT for masked language modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def find_best_suggestion(word, suggestions):\n",
    "    \n",
    "    \"\"\"\n",
    "    Find the spelling suggestion that best matches a given word.\n",
    "    \"\"\"\n",
    "    \n",
    "    dict, max = {}, 0\n",
    "    a = set(suggestions)\n",
    "\n",
    "    for b in a:\n",
    "\n",
    "        tmp = difflib.SequenceMatcher(None, word, b).ratio();\n",
    "        dict[tmp] = b\n",
    "\n",
    "        if tmp > max:\n",
    "            max = tmp\n",
    "\n",
    "    return(dict[max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_segments_tensors(tokenized_text):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given a line of tokenized text, create segments tensors needed for maskedLM. \n",
    "    \"\"\"\n",
    "    \n",
    "    segs = [i for i, e in enumerate(tokenized_text) if e == \".\"]\n",
    "    segments_ids=[]\n",
    "    prev=-1\n",
    "    for k, s in enumerate(segs):\n",
    "        segments_ids = segments_ids + [k] * (s-prev)\n",
    "        prev=s\n",
    "    segments_ids = segments_ids + [len(segs)] * (len(tokenized_text) - len(segments_ids))\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "    return segments_ids, segments_tensors\n",
    "\n",
    "\n",
    "def masked_LM_oneline(tokenizer, model, line, line_info, use_word=False, n_suggestions=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Perform masked LM on one line of text and return the top predictions, given a tokenizer and model.\n",
    "    \n",
    "    Returns:\n",
    "        mask_predictions:\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Replace misspelled words with MASK token\n",
    "    split_line = line.split()\n",
    "    for item in line_info:\n",
    "        split_line[item[0]] = '[MASK]'\n",
    "    line = \" \".join(split_line)\n",
    "    \n",
    "    # Load, train and predict using pre-trained model\n",
    "    tokenized_text = tokenizer.tokenize(line)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    MASKIDS = [i for i, e in enumerate(tokenized_text) if e == '[MASK]']\n",
    "\n",
    "    # Create the segments tensors\n",
    "    segments_ids, segments_tensors = create_segments_tensors(tokenized_text)\n",
    "\n",
    "    # Predict all tokens\n",
    "    with torch.no_grad():\n",
    "        predictions = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "    # Process and display top prediction at each position\n",
    "    mask_predictions = []\n",
    "\n",
    "    if use_word == False:\n",
    "        \n",
    "        for mask_idx in MASKIDS:\n",
    "            word_int = predictions[0][0, mask_idx, :].topk(1).indices.tolist()\n",
    "            word_text = tokenizer.convert_ids_to_tokens(word_int)        \n",
    "            mask_predictions.append(word_text[0])\n",
    "    else:\n",
    "        \n",
    "        for k, mask_idx in enumerate(MASKIDS):\n",
    "\n",
    "            word_int = predictions[0][0, mask_idx, :].topk(n_suggestions).indices.tolist()\n",
    "            word_text = tokenizer.convert_ids_to_tokens(word_int)\n",
    "            original_word = line_info[k][1]\n",
    "            best = find_best_suggestion(original_word, word_text)\n",
    "            mask_predictions.append(best)\n",
    "        \n",
    "    return mask_predictions\n",
    "\n",
    "\n",
    "def masked_prediction(bundle, tokenizer, model, use_word=False, n_suggestions=5):   \n",
    "    \n",
    "    \"\"\"\n",
    "    Given a dataset and locations of misspelled words, perform masked prediction to replace the words. \n",
    "    Can either use information about the misspelled words, or disregard them entirely.\n",
    "    \"\"\"\n",
    "    \n",
    "    data, lines, _, spell_errors, _ = bundle \n",
    "    corrected = []\n",
    "\n",
    "    for i in range(len(spell_errors[0])):\n",
    "\n",
    "        line_idx = spell_errors[0][i]\n",
    "        line_info = spell_errors[1][i]\n",
    "        j = lines.index(line_idx)\n",
    "        line = data[j]           \n",
    "        \n",
    "        # Retrieve prediction using masked LM\n",
    "        preds = masked_LM_oneline(tokenizer, model, line, line_info, use_word, n_suggestions)\n",
    "        \n",
    "        # Save the results\n",
    "        tmp = []\n",
    "        for j, item in enumerate(line_info):\n",
    "            idx = item[0]\n",
    "            tmp.append((idx, preds[j]))\n",
    "\n",
    "        corrected.append(tmp)\n",
    "        \n",
    "    corrected = (spell_errors[0], corrected)\n",
    "    return corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### An example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 [(3, 'selfemployed')]\n",
      "Only one-third of selfemployed workers are women .\n",
      "\n",
      "['all']\n"
     ]
    }
   ],
   "source": [
    "# Find a good example document to use BERT on\n",
    "\n",
    "example_idx = 6\n",
    "\n",
    "print(spell_errors[0][example_idx], spell_errors[1][example_idx])\n",
    "\n",
    "line_idx = spell_errors[0][example_idx]\n",
    "line_info = spell_errors[1][example_idx]\n",
    "line = data[line_idx]\n",
    "preds = masked_LM_oneline(tokenizer, model, line, line_info, use_word=True, n_suggestions=5 )\n",
    "\n",
    "print(line)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the code with Bert - different number of suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives:  331\n",
      "True negatives:  5580\n",
      "False negatives:  335\n",
      "False positives:  294\n",
      "Incorrect edits:  3460\n",
      "------------------------------\n",
      "Detection accuracy:  0.9371\n",
      "Correction accuracy:  0.5911\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "bert_corrected_1 = masked_prediction(bundle, tokenizer, model, use_word = False)\n",
    "det_acc_bert, corr_acc_bert = compute_accuracy_scores(bundle, corrections = bert_corrected_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives:  721\n",
      "True negatives:  5580\n",
      "False negatives:  335\n",
      "False positives:  294\n",
      "Incorrect edits:  3070\n",
      "------------------------------\n",
      "Detection accuracy:  0.9371\n",
      "Correction accuracy:  0.6301\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "bert_corrected_2 = masked_prediction(bundle, tokenizer, model, use_word = True, n_suggestions = 5)\n",
    "det_acc_bert, corr_acc_bert = compute_accuracy_scores(bundle, corrections = bert_corrected_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives:  909\n",
      "True negatives:  5580\n",
      "False negatives:  335\n",
      "False positives:  294\n",
      "Incorrect edits:  2882\n",
      "------------------------------\n",
      "Detection accuracy:  0.9371\n",
      "Correction accuracy:  0.6489\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "bert_corrected_3 = masked_prediction(bundle, tokenizer, model, use_word = True, n_suggestions = 10)\n",
    "det_acc_bert, corr_acc_bert = compute_accuracy_scores(bundle, corrections = bert_corrected_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that accepting more word suggestions from Bert can increase the accuracy to an extent. This only affects the number of true positives and incorrect edits - if n_suggestions is chosen well, we can convert a maximum amount of incorrect edits into true positives. Accepting more word suggestions after this point only increases the incorrect edits again.\n",
    "\n",
    "Tune this parameter on a training part of the corpus, then validate it! Other ways to improve incorrect edits?\n",
    "\n",
    "What fraction of elisions are never discovered? Check if false negative rate remains roughly constant - these might be words that end up being a correctly spelled word after elisions. How much better are we than the enchant spellchecker?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find optimal number of suggested words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_lines = 10000, elision_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "corr_acc_vec = []\n",
    "n_sugg_vec = range(1, 150, 5)\n",
    "\n",
    "for n_suggestions in n_sugg_vec:\n",
    "    bert_corr = masked_prediction(train, tokenizer, model, use_word = True, n_suggestions = n_suggestions)\n",
    "    _, acc = compute_accuracy_scores(bundle,  verbose = False, corrections = bert_corr)\n",
    "    corr_acc_vec.append(acc)\n",
    "    print(\"Done with \", n_suggestions, \" suggestions.\")\n",
    "    \n",
    "print(\"Elapsed time: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_, corr_acc_train = compute_accuracy_scores(train, verbose = False)\n",
    "\n",
    "plt.plot(n_sugg_vec, corr_acc_vec, label = \"BERT\")\n",
    "plt.title(\"Training: corr_acc as a function of n_suggestions\")\n",
    "plt.hlines(corr_acc_train, 0, 150, 'r', label = \"Spell checker\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set!\n",
    "\n",
    "best_idx = corr_acc_vec.index(max(corr_acc_vec))\n",
    "best_n_suggestions = n_sugg_vec[best_idx]\n",
    "\n",
    "bert_corr = masked_prediction(test, tokenizer, model, use_word = True, n_suggestions = best_n_suggestions)\n",
    "_, acc = compute_accuracy_scores(test, corrections = bert_corr, verbose = True)\n",
    "_, corr_acc_test = compute_accuracy_scores(test, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It keeps increasing! Europarl does use some pretty uncommon words, so it makes sense that we'd need many suggestions to find the one that's right for the context.\n",
    "\n",
    "Eventually it flattens out. I guess at that point either Bert stops giving suggestions, or we cover the right level o uncommon words. Does it flatten out faster for distilBert?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy as a function of elision_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_pipeline(file, encoding, elisionArray, elision_probs, max_lines, ignore_list, n_suggestions, verbose = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Pipeline for loading in data with different levels of elision probability, and applying the Bert model. \n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    corr_acc_vec = []\n",
    "    n_elisions_vec = []\n",
    "    lines = list(range(max_lines))\n",
    "\n",
    "    for elision_prob in elision_probs:\n",
    "    \n",
    "        ground_truth, data, elisions, n_elisions = read_data(file, encoding, max_lines, elisionArray, elision_prob)\n",
    "        spell_errors, spell_corrected, n_misspelled, excl_lines = identify_spelling_errors(data, ignore_list)\n",
    "        n_elisions_vec.append(n_elisions)\n",
    "        bundle = (data, lines, elisions, spell_errors, spell_corrected)\n",
    "        \n",
    "        if verbose == True:\n",
    "            \n",
    "            print(\"Number of identified misspelled words: \", n_misspelled)\n",
    "            print(\"Number of synthetic errors: \", n_elisions)\n",
    "            print(\"=\"*75)\n",
    "\n",
    "        bert_corrected = masked_prediction(bundle, tokenizer, model, use_word=True, n_suggestions=n_suggestions)\n",
    "        _, acc = compute_accuracy_scores(bundle, verbose = False, corrections = bert_corrected)    \n",
    "        corr_acc_vec.append(acc)\n",
    "        \n",
    "    print(\"Elapsed time: \", time.time() - start)\n",
    "        \n",
    "    return corr_acc_vec, n_elisions_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elision_probs = np.linspace(0, 1, 10)\n",
    "n_lines = 10000\n",
    "np.random.seed(None)\n",
    "\n",
    "corr_vec_1, n_elisions_1 = evaluation_pipeline(file, encoding, elisionArray, elision_probs, \n",
    "                                 max_lines = n_lines, ignore_list = ignore, n_suggestions = 1, verbose = False)\n",
    "corr_vec_5, n_elisions_5 = evaluation_pipeline(file, encoding, elisionArray, elision_probs, \n",
    "                                 max_lines = n_lines, ignore_list = ignore, n_suggestions = 50, verbose = False)\n",
    "corr_vec_15, n_elisions_15 = evaluation_pipeline(file, encoding, elisionArray, elision_probs, \n",
    "                                 max_lines = n_lines, ignore_list = ignore, n_suggestions = 100, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(elision_probs, corr_vec_1, label = \"1\")\n",
    "plt.plot(elision_probs, corr_vec_5, label = \"50\")\n",
    "plt.plot(elision_probs, corr_vec_15, label = \"100\")\n",
    "plt.title(\"Correction accuracy as a function of elision probability\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(elision_probs, n_elisions_1, label = \"1\")\n",
    "plt.plot(elision_probs, n_elisions_5, label = \"50\")\n",
    "plt.plot(elision_probs, n_elisions_15, label = \"100\")\n",
    "plt.title(\"Number of elisions as a function of elision probability\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# some variability in number of elisions but that's fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(elision_probs, corr_vec_1, label = \"n_sugg = 1\")\n",
    "plt.plot(elision_probs, corr_vec_5, label = \"n_sugg = 50\")\n",
    "plt.plot(elision_probs, corr_vec_15, label = \"n_sugg = 100\")\n",
    "plt.title(\"Correction accuracy as a function of elision probability\")\n",
    "plt.xlabel(\"Elision probability\")\n",
    "plt.ylabel(\"Correction accuracy\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "#plt.savefig('elision_prob.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Distilbert - does this give suggestions in the same way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name_2 = 'albert-base-v2'\n",
    "tokenizer_2 = AutoTokenizer.from_pretrained(model_name_2)\n",
    "model_2 = AutoModelForMaskedLM.from_pretrained(model_name_2).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name_3 = 'google/electra-small-discriminator'\n",
    "tokenizer_3 = AutoTokenizer.from_pretrained(model_name_3)\n",
    "model_3 = AutoModelForMaskedLM.from_pretrained(model_name_3).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "distilbert_corrected = masked_prediction(bundle, tokenizer_2, model_2, False)\n",
    "det_acc_distilbert, corr_acc_distilbert = compute_accuracy_scores(elisions, distilbert_corrected, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "distilbert_corrected = masked_prediction(bundle, tokenizer_2, model_2, True, n_suggestions = 50)\n",
    "det_acc_distilbert, corr_acc_distilbert = compute_accuracy_scores(elisions, distilbert_corrected, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Electra - does this give suggestions in the same way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "electra_corrected = masked_prediction(bundle, tokenizer_3, model_3, True, n_suggestions = 15)\n",
    "det_acc_electra, corr_acc_electra = compute_accuracy_scores(bundle, verbose = True, corrections = electra_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Try using bigrams \n",
    "This could catch when an incorrectly scanned word becomes another, correctly spelled word: for example, God -> Cod. The spell checker will not react to this, but it should create a strange bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Try some other word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "two approaches: either throw away the misspelled word and try to fill it in from context, or use character level embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are entries in excl_lines, make adjustments as described above. (a bit buggy, have to fix this)\n",
    "\n",
    "remove_excl_lines(data, excl_lines)\n",
    "remove_excl_lines(ground_truth, excl_lines)\n",
    "\n",
    "elisions, n_elisions = remove_excl_entries(elisions, n_elisions, excl_lines)\n",
    "spell_errors, n_misspelled = remove_excl_entries(spell_errors, n_misspelled, excl_lines)\n",
    "spell_corrected, _ = remove_excl_entries(spell_corrected, n_misspelled, excl_lines)\n",
    "\n",
    "def remove_excl_lines(data, excl_lines):\n",
    "    \n",
    "    \"\"\"\n",
    "    Remove given lines, by index, from a dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    for i, line in enumerate(excl_lines):\n",
    "        del data[line-i]\n",
    "\n",
    "        \n",
    "def remove_excl_entries(entries, count, excl_lines):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given lines to exclude, by index, remove them from the data structure at the right position. Also adjust count.\n",
    "    \"\"\"\n",
    "    \n",
    "    a = set(excl_lines)\n",
    "    b = set(entries[0])\n",
    "    \n",
    "    for line_idx in a & b:\n",
    "            \n",
    "        # Find position of the line to be removed\n",
    "        list_pos = entries[0].index(line_idx)\n",
    "        \n",
    "        # Adjust the word count\n",
    "        count -= len(entries[1][list_pos])\n",
    "\n",
    "        # Delete the identified entries\n",
    "        del entries[0][list_pos]\n",
    "        del entries[1][list_pos]\n",
    "          \n",
    "    return entries, count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
