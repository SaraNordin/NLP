{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing word embeddings for improving OCR accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our idea is to compare how different embeddings affect OCR accuracy, similar to the approach taken in this article: https://medium.com/states-title/using-nlp-bert-to-improve-ocr-accuracy-385c98ae174c (Links to an external site.)\n",
    "\n",
    "We would like to try some embeddings we have learned about in the course, such as CBoW, word2vec, FastText, ELMo, BERT. If we have more time and find other interesting representations online we will evaluate them as well. The idea is to find a baseline OCR accuracy and then exploring if, and how much, this accuracy can be improved by applying word embeddings to the incorrectly scanned words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use a synthetic dataset (read in europarl corpus and corrupt it a bit)\n",
    "- Richard suggests corrupting it in ways that often happen in OCR, like rn <-> m, i <-> l, cl <-> d \n",
    "    - (https://scribenet.com/articles/2016/03/04/how-to-get-the-most-out-of-ocr)\n",
    "- I think he means we should use a spellchecker to find misspelled words, and check with NER that they are not a name\n",
    "- Then maybe we don't throw out the misspelled word, it can be useful\n",
    "- Richard thinks that character level embeddings can be useful here\n",
    "- Evaluation of performance: see what people usually use within OCR. Some ideas:\n",
    "\n",
    "    - https://www.aclweb.org/anthology/I17-1101.pdf\n",
    "    - https://loicbarrault.github.io/papers/afli_cicling2015.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "from enchant.checker import SpellChecker\n",
    "\n",
    "spell = SpellChecker(\"en-UK\")\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "#import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Try out spellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell.check(\"cat's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spiel',\n",
       " 'spelt',\n",
       " 'spell',\n",
       " 'seel',\n",
       " 'spec',\n",
       " 'sped',\n",
       " 'spew',\n",
       " 'Opel',\n",
       " 'sp el',\n",
       " 'sp-el']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell.suggest(\"spel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "princedom\n"
     ]
    }
   ],
   "source": [
    "# example - is this a good way to get most relevant word from enchant.spellcheck()?\n",
    "\n",
    "import difflib\n",
    "word=\"prfomnc\"\n",
    "\n",
    "dict,max = {},0\n",
    "a = set(spell.suggest(word))\n",
    "\n",
    "for b in a:\n",
    "    \n",
    "    tmp = difflib.SequenceMatcher(None, word, b).ratio();\n",
    "    dict[tmp] = b\n",
    "    \n",
    "    if tmp > max:\n",
    "        max = tmp\n",
    "\n",
    "print(dict[max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in and corrupt the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character elision is when letter pairs and individual letters are confused by the software. These types of errors occur any time pairs of letters are shaped similarly to other letters. Six common pairs are:\n",
    "\n",
    "rn <-> m, cl <-> d, vv <-> w, ol <-> d, li <-> h, nn <-> m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "elisionArray = []\n",
    "elisionArray.append(['rn', 'm'])\n",
    "elisionArray.append(['ol', 'd'])\n",
    "elisionArray.append(['cl', 'd'])\n",
    "elisionArray.append(['vv', 'w'])\n",
    "elisionArray.append(['li', 'h'])\n",
    "elisionArray.append(['nn', 'm'])\n",
    "elisionArray = np.array(elisionArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## An example of how the character elision will be handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I scorn this little barn with my lilac yarn and delicious fern\n",
      "--> Replaced  rn  at position  22 \n",
      "I scorn this little bam with my lilac yarn and delicious fern\n",
      "--> Replaced  li  at position  13 \n",
      "I scorn this httle bam with my lilac yarn and delicious fern\n",
      "--> Replaced  li  at position  32 \n",
      "I scorn this httle bam with my hlac yarn and delicious fern\n",
      "--------------------------------------------------------------------------------\n",
      "Total errors:  3\n",
      "[(0, [(3, 'little'), (4, 'barn'), (7, 'lilac')])]\n"
     ]
    }
   ],
   "source": [
    "# A test sentence that contains a lot of 'rn' and 'li'\n",
    "line = \"I scorn this little barn with my lilac yarn and delicious fern\"\n",
    "print(line)\n",
    "\n",
    "new_line = line\n",
    "elision_prob = 0.5\n",
    "\n",
    "# Randomize search order, since some letter pairs overlap \n",
    "elisionArray = np.random.permutation(elisionArray)\n",
    "for pair in elisionArray:\n",
    "    \n",
    "    n_errors = 0\n",
    "    \n",
    "    for m in re.finditer(pair[0], new_line):\n",
    "        \n",
    "        rd = np.random.rand(1)\n",
    "        if rd < elision_prob:\n",
    "            \n",
    "            # Note that the position can't be used later since the line changes length!\n",
    "            print(\"--> Replaced \", pair[0], \" at position \", m.start(), \"\")\n",
    "            \n",
    "            tmp = list(new_line)\n",
    "            tmp[m.start()-n_errors:m.end()-n_errors] = \"%%\"\n",
    "            new_line = \"\".join(tmp)\n",
    "            new_line = new_line.replace(\"%%\", pair[1])\n",
    "            \n",
    "            print(new_line)\n",
    "\n",
    "            # count number of replacements\n",
    "            n_errors += 1\n",
    "            \n",
    "print('-'*80)\n",
    "\n",
    "# Save location and correct spelling for the corrupted words\n",
    "\n",
    "line = list(line.split())\n",
    "new_line = list(new_line.split())\n",
    "\n",
    "total_errors = 0\n",
    "ground_truth = []\n",
    "\n",
    "# This will be the index of a document in the corpus\n",
    "doc_num = 0     \n",
    "\n",
    "# This contains the ground truth for each document \n",
    "tmp = []\n",
    "\n",
    "for j in range(len(line)):\n",
    "    if line[j] != new_line[j]:\n",
    "        \n",
    "        total_errors += 1\n",
    "        tmp.append((j, line[j]))\n",
    "        \n",
    "ground_truth.append((doc_num, tmp))\n",
    "print(\"Total errors: \", total_errors)\n",
    "print(ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply character elision to the Europarl data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def check_line_errors(line, new_line, total_errors):\n",
    "    \n",
    "    \"\"\"\n",
    "    Check for errors between gold standard document and \"OCR\" document.\n",
    "    \"\"\"\n",
    "\n",
    "    line = list(line.split())\n",
    "    new_line = list(new_line.split()) \n",
    "    truth = []\n",
    "\n",
    "    for j in range(len(line)):\n",
    "        if line[j] != new_line[j]:\n",
    "\n",
    "            total_errors += 1\n",
    "            truth.append((j, line[j]))\n",
    "    \n",
    "    return truth, total_errors\n",
    "\n",
    "\n",
    "def apply_elision(line, elision_array, elision_prob):\n",
    "    \n",
    "    \"\"\"\n",
    "    Apply elision to a line of text. An observed pair will be exchanged with probability elision_prob.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Randomize search order, since some letter pairs overlap \n",
    "    elision_array = np.random.permutation(elision_array)\n",
    "    for elision_pair in elision_array:\n",
    "\n",
    "        # Count number of times each letter pair has been corrupted (since this changes the line length)\n",
    "        n_errors = 0 \n",
    "\n",
    "        for m in re.finditer(elision_pair[0], line):\n",
    "\n",
    "            rd = np.random.rand(1)\n",
    "            if rd < elision_prob:\n",
    "\n",
    "                # Replace the letter pair and convert to a new line\n",
    "                tmp = list(line)\n",
    "                tmp[m.start()-n_errors:m.end()-n_errors] = \"%%\"\n",
    "                line = \"\".join(tmp)\n",
    "                line = line.replace(\"%%\", elision_pair[1])\n",
    "\n",
    "                # count number of replacements\n",
    "                n_errors += 1\n",
    "\n",
    "    return line\n",
    "\n",
    "\n",
    "def read_data(corpus_file, corpus_encoding, max_lines, elision_array, elision_prob):\n",
    "    \n",
    "    \"\"\"\n",
    "    Read in ground truth version of the text, and a (synthetic) corrupted OCR dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    total_errors = 0\n",
    "    ground_truth = []\n",
    "    corrupted_data = []\n",
    "    line_indices = []\n",
    "    elision_errors = []\n",
    "    \n",
    "    with open(corpus_file, encoding = corpus_encoding) as f:\n",
    "        \n",
    "        for d, line in enumerate(f):\n",
    "        \n",
    "            if d == max_lines:\n",
    "                break\n",
    "        \n",
    "            # Apply elision and keep track of which words have been corrupted\n",
    "            new_line = apply_elision(line, elision_array, elision_prob)  \n",
    "            line_truth, total_errors = check_line_errors(line, new_line, total_errors)\n",
    "            \n",
    "            if len(line_truth) > 0:\n",
    "                line_indices.append(d)\n",
    "                elision_errors.append(line_truth)\n",
    "                \n",
    "            # Append original and (potentially) corrupted line\n",
    "            ground_truth.append(line) \n",
    "            corrupted_data.append(new_line)\n",
    "                \n",
    "    elisions = (line_indices, elision_errors)\n",
    "            \n",
    "    return ground_truth, corrupted_data, elisions, total_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def contains_numbers(string):\n",
    "    \n",
    "    \"\"\"\n",
    "    Check if the given string contains any digits.\n",
    "    \"\"\"\n",
    "    \n",
    "    return any(char.isdigit() for char in string)\n",
    "\n",
    "\n",
    "def is_part_of_name(line, word):\n",
    "    \n",
    "    \"\"\"\n",
    "    Using spacy for NER, we check if a specific word is part of a name.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Apply nlp pipeline, check if this \"misspelled word\" is a name\n",
    "    result = nlp(line, disable = ['tagger', 'parser'])\n",
    "    is_name = False\n",
    "\n",
    "    for entity in result.ents:\n",
    "        # If the \"misspelled\" word is part of the name of a person, country etc - we ignore it\n",
    "        if entity.label_ in  [\"PERSON\", \"NORP\", \"GPE\", \"ORG\"] and entity.text.find(word) > -1:\n",
    "            is_name = True\n",
    "\n",
    "    return is_name\n",
    "\n",
    "\n",
    "def identify_spelling_errors(data, ignore):\n",
    "\n",
    "    \"\"\"\n",
    "    Given a dataset and a list of characters to ignore, find misspelled words that are _not_ names.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_misspelled = 0\n",
    "    line_indices = []\n",
    "    spelling_errors = []\n",
    "    spelling_corrected = []\n",
    "    not_in_english = []\n",
    "\n",
    "    for d, line in enumerate(data):\n",
    "\n",
    "        words = line.split()\n",
    "        tmp_1 = []\n",
    "        tmp_2 = []\n",
    "        \n",
    "        for i, word in enumerate(words):\n",
    "\n",
    "            # Some dates and similar are marked as misspelled - ignore words containing numbers!\n",
    "            if not word in ignore and not contains_numbers(word):\n",
    "            \n",
    "                # Apply a spell checker\n",
    "                if not spell.check(word):\n",
    "        \n",
    "                    # Check if the word is part of a name\n",
    "                    if not is_part_of_name(line, word):\n",
    "\n",
    "                        try:\n",
    "                            # Apply a spell correction to the word\n",
    "                            corrected_word = spell.suggest(word)[0]\n",
    "                            \n",
    "                            # Note down word and position\n",
    "                            tmp_1.append((i, word))\n",
    "                            tmp_2.append((i, corrected_word))\n",
    "                            n_misspelled += 1\n",
    "\n",
    "                        except IndexError:\n",
    "\n",
    "                            # If no spelling suggestions exist, the line is usually not in English. \n",
    "                            # The EU parlaiment uses many languages, exclude the line just in case.\n",
    "                            not_in_english.append(d)\n",
    "        \n",
    "        # If spelling errors were found, save the line and words.\n",
    "        if len(tmp_1) > 0:\n",
    "            line_indices.append(d)\n",
    "            spelling_errors.append(tmp_1)\n",
    "            spelling_corrected.append(tmp_2)\n",
    "            \n",
    "    spelling_errors = (line_indices, spelling_errors)\n",
    "    spelling_corrected = (line_indices, spelling_corrected)\n",
    "\n",
    "    return spelling_errors, spelling_corrected, n_misspelled, not_in_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_excl_lines(data, excl_lines):\n",
    "    \n",
    "    \"\"\"\n",
    "    Remove given lines, by index, from a dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    for i, line in enumerate(excl_lines):\n",
    "        del data[line-i]\n",
    "\n",
    "        \n",
    "def remove_excl_entries(entries, count, excl_lines):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given lines to exclude, by index, remove them from the data structure at the right position. Also adjust count.\n",
    "    \"\"\"\n",
    "    \n",
    "    a = set(excl_lines)\n",
    "    b = set(entries[0])\n",
    "    \n",
    "    for line_idx in a & b:\n",
    "            \n",
    "        # Find position of the line to be removed\n",
    "        list_pos = entries[0].index(line_idx)\n",
    "        \n",
    "        # Adjust the word count\n",
    "        count -= len(entries[1][list_pos])\n",
    "\n",
    "        # Delete the identified entries\n",
    "        del entries[0][list_pos]\n",
    "        del entries[1][list_pos]\n",
    "          \n",
    "    return entries, count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"europarl.txt\"\n",
    "encoding = \"utf-8\"\n",
    "\n",
    "# Probability that a letter pair from our list will be confused if it is seen in the text\n",
    "elision_prob = 0.1\n",
    "max_lines = 10000\n",
    "\n",
    "np.random.seed(0)\n",
    "ground_truth, data, elisions, n_elisions = read_data(file, encoding, max_lines, elisionArray, elision_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words and characters for the spellchecker to ignore\n",
    "ignore = [\",\", \".\", '\"', \"(\", \")\", \"-\", \"'\", \"!\", \"?\", \":\", \";\", \"/\", \"n't\", \"'s\", \"'m\", \"%\", \"--\", \"``\", \"___LANGCODE___\"]\n",
    "spell_errors, spell_corrected, n_misspelled, excl_lines = identify_spelling_errors(data, ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of identified misspelled words:  1995\n",
      "Number of synthetic errors:  1190\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of identified misspelled words: \", n_misspelled)\n",
    "print(\"Number of synthetic errors: \", n_elisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While spell checking, some words might appear that have no spelling suggestions. These words tend to be in French or German, since the EU parliament uses these languages occasionally in quotes. It's not fair to ask the spell checker to handle these words, and the purpose here is not to handle multiple different languages. Thus we will remove any lines that are not only in English (in the first 10 000 documents of the corpus, only 6 are affected). \n",
    "\n",
    "If there are entries in excl_lines:\n",
    "- Remove the excl_lines from ground_truth and data\n",
    "- If any of the excl_lines appear in elisions, elisions and n_elisions need to be adjusted\n",
    "- If any of the excl_lines appear in spell_errors or spell_corrected, adjust them and n_misspelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are entries in excl_lines, make adjustments as described above.\n",
    "\n",
    "remove_excl_lines(data, excl_lines)\n",
    "remove_excl_lines(ground_truth, excl_lines)\n",
    "\n",
    "elisions, n_elisions = remove_excl_entries(elisions, n_elisions, excl_lines)\n",
    "spell_errors, n_misspelled = remove_excl_entries(spell_errors, n_misspelled, excl_lines)\n",
    "spell_corrected, _ = remove_excl_entries(spell_corrected, n_misspelled, excl_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute baseline accuracy using spellcheck suggestions\n",
    "\n",
    "baseline: use top suggested word from enchant spellchecker\n",
    "\n",
    "so I guess we create a list of misspelled words just like doc_errors, apply the suggestion for each, and see if we get closer to the ground truth or not?\n",
    "\n",
    "- https://www.aclweb.org/anthology/I17-1101.pdf\n",
    "\n",
    "[From the article] For evaluation, we use the CER (Character Error Rate) metric as defined in OCR post-correction evaluations:\n",
    "\n",
    "$$ CER = \\frac{S + D + I}{S + D + C} $$\n",
    "\n",
    "Where S refers to the number of substituted characters in the OCR text (w.r.t. the reference texts), D to the number of deleted characters, I to the number of inserted characters and C to the number of ‘correct’ characters. We use the CER metric when comparing to the baseline system. __Is this useful in the character elision setting? Doesn't look like it__\n",
    "\n",
    "We want to measure the systems performance per input, rather than per character. We therefore introduce two complementary accuracy-based evaluation metrics, which evaluate on the level of the character window:\n",
    "- detection accuracy (detAcc) shows the proportion of correctly detected errors and nonerrors in the evaluated set of 20-character strings\n",
    "- correction accuracy (corrAcc) reflects the ability of the language model to accurately correct corrupted strings without overgenerating and editing non-corrupted strings\n",
    "\n",
    "These metrics are calculated as follows:\n",
    "\n",
    "$detAcc = \\frac{(TP + TN + incorrectEdit)}{(TP + TN + FP + FN + incorrectEdit)}$\n",
    "\n",
    "$corrAcc = \\frac{(TP + TN)}{(TP + TN + FP + FN + incorrectEdit)}$\n",
    "\n",
    "- TP: There is an error on the line, which is corrected.\n",
    "- TN: No error on the line, no correction\n",
    "- FP: No error on the line, makes a correction anyway\n",
    "- FN: There is an error on the line, but it's not corrected\n",
    "- incorrectEdit: Error is identified and corrected, but other words are also corrected which should not be (within the same document). Basically, anything that doesn't fall into the other categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://loicbarrault.github.io/papers/afli_cicling2015.pdf\n",
    "\n",
    "They use BLEU. The Bilingual Evaluation Understudy Score, or BLEU for short, is a metric for evaluating a generated sentence to a reference sentence. A perfect match results in a score of 1.0, whereas a perfect mismatch results in a score of 0.0\n",
    "\n",
    "Also, they use WER (word error rate). To get the WER, start by adding up the substitutions, insertions, and deletions that occur in a sequence of recognized words. Divide that number by the total number of words originally spoken. The result is the WER. To put it in a simple formula, Word Error Rate = (Substitutions + Insertions + Deletions) / Number of Words Spoken\n",
    "\n",
    "But how do you add up those factors? Let’s look at each one:\n",
    "\n",
    "- A substitution occurs when a word gets replaced (for example, “noose” is transcribed as “moose”)\n",
    "- An insertion is when a word is added that wasn’t said (for example, “SAT” becomes “essay tea”)\n",
    "- A deletion happens when a word is left out of the transcript completely (for example, “turn it around” becomes “turn around”)\n",
    "\n",
    "Since we don't consider insertions and deletions, WER doesn't seem very relevant either. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From one of the articles\n",
    "true_line = \"In oven for 15 minutes\"\n",
    "\n",
    "# True positive case\n",
    "line_in  = \"In o5en for 15 minutes\"\n",
    "line_out = \"In oven for 15 minutes\"\n",
    "\n",
    "# True negative case\n",
    "line_in  = \"In oven for 15 minutes\"\n",
    "line_out = \"In oven for 15 minutes\"\n",
    "\n",
    "# False positive case\n",
    "line_in  = \"In oven for 15 minutes\"\n",
    "line_out = \"In oven for 15 m1nutes\"\n",
    "\n",
    "# False negative case\n",
    "line_in  = \"In o5en for 15 minutes\"\n",
    "line_out = \"In o5en for 15 minutes\"\n",
    "\n",
    "# Incorrect edit case\n",
    "line_in  = \"In o5en for 15 minutes\"\n",
    "line_out = \"In oven for 15 m1nutes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_scores(elisions, corrections, data):\n",
    "    \n",
    "    elision_idx = elisions[0]\n",
    "    elision_words = elisions[1]\n",
    "    corrected_idx = corrections[0]\n",
    "    corrected_words = corrections[1]\n",
    "    \n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    incorrect_edits = 0\n",
    "    \n",
    "    # True positives = number of correct changes\n",
    "    for pos1, idx in enumerate(elision_idx):\n",
    "        if idx in corrected_idx:\n",
    "            \n",
    "            pos2 = corrected_idx.index(idx)\n",
    "            \n",
    "            if elision_words[pos1] == corrected_words[pos2]:\n",
    "                TP += 1\n",
    "            else:\n",
    "                incorrect_edits += 1\n",
    "\n",
    "    print(\"True positives: \", TP)  \n",
    "    \n",
    "    # True negatives = number of lines that do not have elision, and that have not been touched by spellchecker\n",
    "    corpus_size = len(data)\n",
    "    all_touched_lines = set(corrected_idx) | set(elision_idx)\n",
    "    TN = corpus_size - len(all_touched_lines)\n",
    "    print(\"True negatives: \", TN)\n",
    "    \n",
    "    # False negatives = lines that should have been changed, but were not\n",
    "    false_neg_lines = set(elision_idx) - set(corrected_idx)\n",
    "    FN = len(false_neg_lines)\n",
    "    print(\"False negatives: \", FN)\n",
    "    \n",
    "    # False positives = there was no elision, but a correction was made anyway\n",
    "    false_pos_lines = set(corrected_idx) - set(elision_idx)\n",
    "    FP = len(false_pos_lines)\n",
    "    print(\"False positives: \", FP)\n",
    "    \n",
    "    # Instances when the right line has been edited, but incorrectly\n",
    "    print(\"Incorrect edits: \", incorrect_edits)\n",
    "    \n",
    "    # Double check that it all adds up\n",
    "    if FP + FN + TN + TP + incorrect_edits != corpus_size:\n",
    "        print(\"ERROR: These scores don't add up to the number of lines!\")\n",
    "        \n",
    "    # Compute and display accuracy scores    \n",
    "    det_accuracy = (TP + TN + incorrect_edits)/corpus_size\n",
    "    corr_accuracy = (TP + TN)/corpus_size\n",
    "    \n",
    "    print(\"-\"*75)\n",
    "    print(\"Detection accuracy: \", np.round(det_accuracy, 4))\n",
    "    print(\"Correction accuracy: \", np.round(corr_accuracy, 4))\n",
    "    \n",
    "    return det_accuracy, corr_accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives:  28\n",
      "True negatives:  8192\n",
      "False negatives:  108\n",
      "False positives:  692\n",
      "Incorrect edits:  974\n",
      "---------------------------------------------------------------------------\n",
      "Detection accuracy:  0.92\n",
      "Correction accuracy:  0.8225\n"
     ]
    }
   ],
   "source": [
    "det_acc, corr_acc = compute_accuracy_scores(elisions, spell_corrected, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to fill in missing words using BERT \n",
    "(like in the article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1599 [(1, 'concems')]\n",
      "However , when health becomes an object of trade and exploitation , when large private clinics reap huge profits from groups of patients who need a transplant , there can be no terms or conditions to the safeguarding of transparency in organ donation .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find a good example document to use BERT on\n",
    "\n",
    "example_idx = 260\n",
    "\n",
    "print(spell_errors[0][example_idx], spell_errors[1][example_idx])\n",
    "\n",
    "line_idx = spell_errors[0][example_idx]\n",
    "line_info = spell_errors[1][example_idx]\n",
    "\n",
    "line = data[line_idx]\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the other hand , I believe that perhaps one of the amendments presented by the Committee on Foreign Affairs is rather over the top insofar as it could constitute a first step towards the creation of a European Union foreign service , but it is the case that these [MASK] officers , with the support of the offices of the Commission in relation to information and coordination , are without doubt a first step towards the creation of this Community immigration [MASK] which we all want to see .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace misspelled words with [MASK]\n",
    "split_line = line.split()\n",
    "\n",
    "for item in line_info:\n",
    "    word = split_line[item[0]]\n",
    "    line = line.replace(word, '[MASK]')\n",
    "\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load, train and predict using pre-trained model\n",
    "tokenized_text = tokenizer.tokenize(line)\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "MASKIDS = [i for i, e in enumerate(tokenized_text) if e == '[MASK]']\n",
    "\n",
    "# Create the segments tensors\n",
    "segs = [i for i, e in enumerate(tokenized_text) if e == \".\"]\n",
    "segments_ids=[]\n",
    "prev=-1\n",
    "for k, s in enumerate(segs):\n",
    "    segments_ids = segments_ids + [k] * (s-prev)\n",
    "    prev=s\n",
    "segments_ids = segments_ids + [len(segs)] * (len(tokenized_text) - len(segments_ids))\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "# prepare Torch inputs \n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "# Load pre-trained model\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased').eval()\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    predictions = model(tokens_tensor, segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['two', 'three', 'new', 'these', 'four']\n",
      "[',', 'service', 'system', '-', 'office']\n"
     ]
    }
   ],
   "source": [
    "mask_predictions = []\n",
    "n_guesses = 5\n",
    "\n",
    "for mask_idx in MASKIDS:\n",
    "    \n",
    "    word_int = predictions[0][0, mask_idx, :].topk(5).indices.tolist()\n",
    "    print(tokenizer.convert_ids_to_tokens(word_int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In its report , the Court of Auditors puts forward the view that there was a cumulation of subsidies and irregularities in the [MASK] of the programmes because the measures overlapped .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example from https://github.com/renatoviolin/next_word_prediction/blob/master/main.py\n",
    "\n",
    "def decode(tokenizer, pred_idx, top_clean):\n",
    "    ignore_tokens = string.punctuation + '[PAD]'\n",
    "    tokens = []\n",
    "    for w in pred_idx:\n",
    "        token = ''.join(tokenizer.decode(w).split())\n",
    "        if token not in ignore_tokens:\n",
    "            tokens.append(token.replace('##', ''))\n",
    "    return '\\n'.join(tokens[:top_clean])\n",
    "\n",
    "\n",
    "def encode(tokenizer, text_sentence, add_special_tokens=True):\n",
    "    text_sentence = text_sentence.replace('[MASK]', tokenizer.mask_token)\n",
    "    # if <mask> is the last token, append a \".\" so that models dont predict punctuation.\n",
    "    if tokenizer.mask_token == text_sentence.split()[-1]:\n",
    "        text_sentence += ' .'\n",
    "\n",
    "    input_ids = torch.tensor([tokenizer.encode(text_sentence, add_special_tokens=add_special_tokens)])\n",
    "    mask_idx = torch.where(input_ids == tokenizer.mask_token_id)[1].tolist()[0]\n",
    "    return input_ids, mask_idx\n",
    "\n",
    "# ========================= BERT =================================\n",
    "\n",
    "import torch\n",
    "import string\n",
    "\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertForMaskedLM.from_pretrained('bert-base-uncased').eval()\n",
    "\n",
    "text_sentence = line\n",
    "top_k = 10\n",
    "top_clean=5\n",
    "\n",
    "print(text_sentence)\n",
    "input_ids, mask_idx = encode(bert_tokenizer, text_sentence)\n",
    "with torch.no_grad():\n",
    "    predict = bert_model(input_ids)[0]\n",
    "bert = decode(bert_tokenizer, predict[0, mask_idx, :].topk(top_k).indices.tolist(), top_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try using bigrams \n",
    "This could catch when an incorrectly scanned word becomes another, correctly spelled word: for example, God -> Cod. The spell checker will not react to this, but it should create a strange bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try some other word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "two approaches: either throw away the misspelled word and try to fill it in from context, or use character level embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try some character level embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
