{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Assessing word embeddings for improving OCR accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I will work on this project with Adnan Fazlinovic. \n",
    "\n",
    "Our idea is to compare how different embeddings affect OCR accuracy, similar to the approach taken in this article: https://medium.com/states-title/using-nlp-bert-to-improve-ocr-accuracy-385c98ae174c (Links to an external site.)\n",
    "\n",
    "We would like to try some embeddings we have learned about in the course, such as CBoW, word2vec, FastText, ELMo, BERT. If we have more time and find other interesting representations online we will evaluate them as well. The idea is to find a baseline OCR accuracy and then exploring if, and how much, this accuracy can be improved by applying word embeddings to the incorrectly scanned words. As datasets we want to use images of machine written text, in English (suggestions on good datasets would be appreciated). If the data is of inconsistent quality, the focus will be too much on the image scanning part and not on the NLP application. \n",
    "\n",
    "If we don't find any good image resource dataset, we could use a dataset from the course and randomly corrupt a subset of the words in each text. \n",
    "\n",
    "A preliminary title is \"Asessing word embeddings for improving OCR accuracy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Probably use a synthetic dataset (read in europarl corpus and corrupt it a bit)\n",
    "- Richard suggests corrupting it in ways that often happen in OCR, like rn <-> m, i <-> l, cl <-> d \n",
    "    - (https://scribenet.com/articles/2016/03/04/how-to-get-the-most-out-of-ocr)\n",
    "- I think he means we should use a spellchecker to find misspelled words, and check with NER that they are not a name\n",
    "- Then maybe we don't throw out the misspelled word, it can be useful\n",
    "\n",
    "It feels like we're mostly making a spell/grammar checking model - that's fine\n",
    "\n",
    "- Richard thinks that character level embeddings can be useful here\n",
    "- Evaluation of performance: see what people usually use within OCR. Some ideas:\n",
    "\n",
    "    - https://www.aclweb.org/anthology/I17-1101.pdf\n",
    "    - https://loicbarrault.github.io/papers/afli_cicling2015.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Try out spellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from enchant.checker import SpellChecker\n",
    "\n",
    "spell = SpellChecker(\"en-UK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "words = line.split()\n",
    "\n",
    "for word in words:\n",
    "    if not spell.check(word):\n",
    "        print(word)\n",
    "        \n",
    "# what do we do with 's and similar words? they are correct. maybe just merge them with previous word, or remove them? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spell.check(\"cat's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spell.suggest(\"spel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in and corrupt the Europarl dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character elision is when letter pairs and individual letters are confused by the software. These types of errors occur any time pairs of letters are shaped similarly to other letters. Six common pairs are:\n",
    "\n",
    "rn <-> m, cl <-> d, vv <-> w, ol <-> d, li <-> h, nn <-> m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "elisionArray = []\n",
    "elisionArray.append(['rn', 'm'])\n",
    "elisionArray.append(['ol', 'd'])\n",
    "elisionArray.append(['cl', 'd'])\n",
    "elisionArray.append(['vv', 'w'])\n",
    "elisionArray.append(['li', 'h'])\n",
    "elisionArray.append(['nn', 'm'])\n",
    "elisionArray = np.array(elisionArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in gold standard text, and a (synthetic) corrupted OCR dataset\n",
    "\n",
    "def read_data(corpus_file, corpus_encoding, max_lines, elisionArray, elision_prob):\n",
    "    \n",
    "    total_errors = 0 \n",
    "    gold_standard = []\n",
    "    corrupted_data = []\n",
    "    \n",
    "    with open(corpus_file, encoding = corpus_encoding) as f:\n",
    "        \n",
    "        for d, line in enumerate(f):\n",
    "        \n",
    "            if d == max_lines:\n",
    "                break\n",
    "        \n",
    "            gold_standard.append(line) \n",
    "            new_line = line\n",
    "            \n",
    "            # Randomize search order, since some letter pairs overlap \n",
    "            elisionArray = np.random.permutation(elisionArray)\n",
    "            for pair in elisionArray:\n",
    "                \n",
    "                # Count number of times each letter pair has been corrupted (since this changes the line length)\n",
    "                n_errors = 0 \n",
    "                \n",
    "                for m in re.finditer(pair[0], new_line):\n",
    "                    \n",
    "                    rd = np.random.rand(1)\n",
    "                    if rd < elision_prob:\n",
    "                        \n",
    "                        print(\"Line \", d, \" - found \", pair[0], \" at position \", m.start())\n",
    "                        \n",
    "                        # Replace the letter pair and convert to a new line\n",
    "                        tmp = list(new_line)\n",
    "                        tmp[m.start()-n_errors:m.end()-n_errors] = \"%%\"\n",
    "                        new_line = \"\".join(tmp)\n",
    "                        new_line = new_line.replace(\"%%\", pair[1])\n",
    "\n",
    "                        # count number of replacements\n",
    "                        n_errors += 1\n",
    "                     \n",
    "            corrupted_data.append(new_line)\n",
    "\n",
    "    return gold_standard, corrupted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line  3  - found  ol  at position  59\n",
      "Line  5  - found  cl  at position  59\n"
     ]
    }
   ],
   "source": [
    "file = r\"C:\\Users\\saran\\OneDrive\\Dokument\\GitHub\\NLP\\Project5\\europarl.txt\"\n",
    "encoding = \"utf-8\"\n",
    "\n",
    "# Probability that a letter pair from our list will be confused if it is seen in the text\n",
    "elision_prob = 0.5\n",
    "\n",
    "#np.random.seed(None)\n",
    "original, corrupted = read_data(file, encoding, 6, elisionArray, elision_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I therefore agree with the European Parliament 's recommendations to the World Bank in this area .\\n\",\n",
       " 'The Commissioner responsible , Jacques Barrot , has promised to present an informative report by the end of July , and our group was keen to wait for this .\\n',\n",
       " 'Resumption of the session\\n',\n",
       " 'I am pleased that , in dialogue with the institutions , a solution has successfully been found that can satisfy everybody , or at least I hope it can , and I would thank you for your constructive work in this process .\\n',\n",
       " 'It has only done so when faced with intense pressure from dairy producers , the European Parliament and 21 Member States .\\n',\n",
       " 'These measures have also been relevant for the textile and clothing industry : for instance , the Globalisation Fund support has been used to reintegrate workers laid off in mostly small and medium-sized enterprises of the sector in Italy , Malta , Spain , Portugal , Lithuania and Belgium .\\n']"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I therefore agree with the European Parliament 's recommendations to the World Bank in this area .\\n\",\n",
       " 'The Commissioner responsible , Jacques Barrot , has promised to present an informative report by the end of July , and our group was keen to wait for this .\\n',\n",
       " 'Resumption of the session\\n',\n",
       " 'I am pleased that , in dialogue with the institutions , a sdution has successfully been found that can satisfy everybody , or at least I hope it can , and I would thank you for your constructive work in this process .\\n',\n",
       " 'It has only done so when faced with intense pressure from dairy producers , the European Parliament and 21 Member States .\\n',\n",
       " 'These measures have also been relevant for the textile and dothing industry : for instance , the Globalisation Fund support has been used to reintegrate workers laid off in mostly small and medium-sized enterprises of the sector in Italy , Malta , Spain , Portugal , Lithuania and Belgium .\\n']"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I scorn this little barn with my lilac yarn and smiling fern\n",
      "--> Replaced  rn  at position  5 \n",
      "I scom this little barn with my lilac yarn and smiling fern\n",
      "--> Replaced  rn  at position  41 \n",
      "I scom this little barn with my lilac yam and smiling fern\n",
      "--> Replaced  rn  at position  58 \n",
      "I scom this little barn with my lilac yam and smiling fem\n",
      "--> Replaced  li  at position  32 \n",
      "I scom this little barn with my hlac yam and smiling fem\n",
      "--> Replaced  li  at position  49 \n",
      "I scom this little barn with my hlac yam and smihng fem\n",
      "--------------------------------------------------------------------------------\n",
      "Total errors:  5\n",
      "[(0, [(1, 'scorn'), (7, 'lilac'), (8, 'yarn'), (10, 'smiling'), (11, 'fern')])]\n"
     ]
    }
   ],
   "source": [
    "# A test sentence that contains a lot of 'rn' and 'li'\n",
    "line = \"I scorn this little barn with my lilac yarn and smiling fern\"\n",
    "print(line)\n",
    "\n",
    "new_line = line\n",
    "elision_prob = 0.5\n",
    "\n",
    "# Randomize search order, since some letter pairs overlap \n",
    "elisionArray = np.random.permutation(elisionArray)\n",
    "for pair in elisionArray:\n",
    "    \n",
    "    n_errors = 0\n",
    "    \n",
    "    for m in re.finditer(pair[0], new_line):\n",
    "        \n",
    "        rd = np.random.rand(1)\n",
    "        if rd < elision_prob:\n",
    "            \n",
    "            # Note that the position can't be used later since the line changes length!\n",
    "            print(\"--> Replaced \", pair[0], \" at position \", m.start(), \"\")\n",
    "            \n",
    "            tmp = list(new_line)\n",
    "            tmp[m.start()-n_errors:m.end()-n_errors] = \"%%\"\n",
    "            new_line = \"\".join(tmp)\n",
    "            new_line = new_line.replace(\"%%\", pair[1])\n",
    "            \n",
    "            print(new_line)\n",
    "\n",
    "            # count number of replacements\n",
    "            n_errors += 1\n",
    "            \n",
    "print('-'*80)\n",
    "\n",
    "# Save location and correct spelling for the corrupted words\n",
    "\n",
    "line = list(line.split())\n",
    "new_line = list(new_line.split())\n",
    "\n",
    "total_errors = 0\n",
    "ground_truth = []\n",
    "\n",
    "# This will be the index of a document in the corpus\n",
    "doc_num = 0     \n",
    "\n",
    "# This contains the ground truth for each document \n",
    "tmp = []\n",
    "\n",
    "for j in range(len(line)):\n",
    "    if line[j] != new_line[j]:\n",
    "        \n",
    "        total_errors += 1\n",
    "        tmp.append((j, line[j]))\n",
    "        \n",
    "ground_truth.append((doc_num, tmp))\n",
    "print(\"Total errors: \", total_errors)\n",
    "print(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore = [\",\", \".\", '\"', \"(\", \")\", \"-\", \"'\", \"!\", \"?\", \":\", \";\", \"/\", \"n't\", \"'s\", \"'m\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = line.split()\n",
    "\n",
    "for word in words:\n",
    "    if not word in ignore and not spell.check(word):\n",
    "        \n",
    "        # also check that the 'misspelled' word is not someone's name - check with NER on the line\n",
    "        # (should probably not remove parts of the line before running the NER)\n",
    "        \n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "should save locations of the corrupted words, and what they were before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "two approaches: either throw away the misspelled word and try to fill it in from context, or use character level embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute baseline accuracy using spellcheck suggestions\n",
    "\n",
    "baseline: use top suggested word from enchant spellchecker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to fill in missing words using BERT \n",
    "(like in the article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try some other word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try some character level embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
