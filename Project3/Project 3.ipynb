{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep%2:42:07::\tkeep.v\t15\taction by the committee in pursuance of its mandate , the committee will continue to keep under review the situation relating to the question of palestine and participate in relevant meetings of the general assembly and the security council . the committee will also continue to monitor the situation on the ground and draw the attention of the international community to urgent developments in the occupied palestinian territory , including east jerusalem , requiring international action .\n",
      "\n",
      "?\tphysical.a\t58\tiaea pointed out that training and education were fundamental to the agency 's approach to enhancing physical protection systems in states . training courses , workshops and seminars that had been held on six continents had raised awareness and had provided hands-on experience of various subjects including the physical protection of research facilities , the practical operation of physical protection systems , and the engineering safety aspects of physical protection managing situations involving malevolent acts . in the area of physical protection 60 courses had been conducted in the past three years .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_path = \"a3_data/wsd_train.txt\"\n",
    "test_path = \"a3_data/wsd_test_blind.txt\"\n",
    "\n",
    "with open(train_path, encoding = \"utf-8\") as f:\n",
    "    for d, line in enumerate(f):\n",
    "        print(line.lower())\n",
    "        break\n",
    "        \n",
    "with open(test_path, encoding = \"utf-8\") as f:\n",
    "    for d, line in enumerate(f):\n",
    "        print(line.lower())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \n",
    "    sense_list = []\n",
    "    lemma_list = []\n",
    "    position_list = []\n",
    "    text_list = []\n",
    "\n",
    "    with open(file_path, encoding = \"utf-8\") as f:\n",
    "        for d, line in enumerate(f):\n",
    "\n",
    "            line = line.lower()\n",
    "\n",
    "            ix = line.find(\"\\t\")\n",
    "            sense_key = line[0:ix]\n",
    "            line = line[ix+1:]\n",
    "\n",
    "            ix = line.find(\"\\t\")\n",
    "            lemma = line[0:ix]\n",
    "            line = line[ix+1:]\n",
    "\n",
    "            ix = line.find(\"\\t\")\n",
    "            position = line[0:ix]\n",
    "            text = line[ix+1:].split()\n",
    "\n",
    "            #if d == 0:\n",
    "            #    print(\"sense_key \", sense_key)\n",
    "            #    print(\"lemma \", lemma)\n",
    "            #    print(\"position \", position)\n",
    "            #    print(\"text \", text)\n",
    "\n",
    "            sense_list.append(sense_key)\n",
    "            lemma_list.append(lemma)\n",
    "            position_list.append(position)\n",
    "            text_list.append(text)\n",
    "\n",
    "            #if d == 10000:\n",
    "            #    break\n",
    "\n",
    "    #print(d)\n",
    "    \n",
    "    df = pd.DataFrame(sense_list, columns = [\"Sense_key\"])\n",
    "    df[\"Lemma\"] = lemma_list\n",
    "    df[\"Position\"] = position_list\n",
    "    df[\"Text\"] = text_list\n",
    "\n",
    "    del sense_list, lemma_list, position_list\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self, df):\n",
    "        self.data = df\n",
    "        self.lemma = None\n",
    "        \n",
    "        self.line_length = None    \n",
    "        self.vocabulary = None\n",
    "        \n",
    "        self.x_tokenized = None\n",
    "        self.x_embedded = None\n",
    "        self.x_padded = None\n",
    "        self.x_raw = None\n",
    "        self.y = None\n",
    "        \n",
    "        self.x_train = None\n",
    "        self.x_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "    \n",
    "    def load_data(self):\n",
    "        # split into sentences (x) and sense key (y)\n",
    "        df = self.data\n",
    "        self.x_raw = df.Text.values\n",
    "        self.y = df.Sense_key.values\n",
    "        self.lemma = df.Lemma.iloc[0]\n",
    "        \n",
    "    def build_vocabulary(self):\n",
    "        # Builds the vocabulary \n",
    "        self.vocabulary = dict()\n",
    "        fdist = nltk.FreqDist()\n",
    "\n",
    "        for sentence in self.x_raw:\n",
    "            for word in sentence:\n",
    "                fdist[word] += 1\n",
    "\n",
    "        common_words = fdist.most_common()\n",
    "\n",
    "        for idx, word in enumerate(common_words):\n",
    "            self.vocabulary[word[0]] = (idx+1)\n",
    "            \n",
    "    def word_to_idx(self):\n",
    "        # By using the dictionary (vocabulary), it is transformed\n",
    "        # each token into its index based representatio\n",
    "        self.x_tokenized = list() \n",
    "\n",
    "        for sentence in self.x_raw:\n",
    "            temp_sentence = list()\n",
    "            for word in sentence:\n",
    "                if word in self.vocabulary.keys():\n",
    "                    temp_sentence.append(self.vocabulary[word])\n",
    "            self.x_tokenized.append(temp_sentence)\n",
    "        \n",
    "    def find_line_length(self):\n",
    "        \n",
    "        max_len = 0\n",
    "        for item in self.x_raw:\n",
    "    \n",
    "            if len(item) > max_len:\n",
    "                max_len = len(item)\n",
    "        \n",
    "        self.line_length = max_len\n",
    "    \n",
    "    def padding_sentences(self):\n",
    "        # Each sentence which does not fulfill the required length is padded with the index 0\n",
    "        pad_idx = 0\n",
    "        self.x_padded = list()\n",
    "\n",
    "        for sentence in self.x_tokenized:\n",
    "            while len(sentence) < self.line_length:\n",
    "                sentence.insert(len(sentence), pad_idx)\n",
    "\n",
    "            self.x_padded.append(sentence)\n",
    "            \n",
    "        self.x_padded = np.array(self.x_padded)\n",
    "    \n",
    "    def split_data(self):\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.x_padded, self.y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = df[df.Lemma == \"positive.a\"]\n",
    "\n",
    "data_pos = Preprocessing(df_pos)\n",
    "data_pos.load_data()\n",
    "data_pos.build_vocabulary()\n",
    "data_pos.word_to_idx()\n",
    "data_pos.find_line_length()\n",
    "data_pos.padding_sentences()\n",
    "data_pos.split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2990,    3,   55,   22,  751,  113,   60,   28,   45,  335,  990,\n",
       "         35,  204,   15,    2,  504,    7, 1064, 1065,    3,  336, 1558,\n",
       "         28, 2185,    3, 5049,    5, 1224,    4, 1225, 1430,    3,   82,\n",
       "       5050,  653,    6,    8,   59, 1431,    3,    2,  867,    4,    2,\n",
       "        370,   41,  277,   17,    7, 1226,  570,   10,  335,   18,  485,\n",
       "         18,  226,  200,   18,    3,    7,  718,    7,  259,   13,    2,\n",
       "        868,   31, 2991,    2, 2992,    4,  344,  371,    6,   25,    2,\n",
       "        270,  205,    3,    2,  370, 3716,    7, 1314, 1559,    5, 3717,\n",
       "       2496, 2497,   23, 1720,    4,  240,   12,  319,  719,    9, 2498,\n",
       "         97,   13,    2,   65,    4,    2, 2499,    6,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "          1,    1,    1,    1,    1])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pos.x_padded[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are stopwords able to change the sense of a word? I think so!\n",
    "\n",
    "- standing in line - waiting for something\n",
    "- standing in a line - they're just standing \n",
    "\n",
    "Based on this, I will not remove stopwords. I will also leave in punctuation, but it seems like a good idea to lowercase the entire text. We're not doing NER, and I don't want Line and line to end up having two meanings - the position alone should clarify the sense. CBoW seems like a terrible choice in this setting - the word senses will almost certainly get lost. Try representation with pre-trained GloVe vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idea: only embed the sentence containing the word in question (maybe later)\n",
    "use word position in an attention model, or for determining weights in a CNN/RNN (think that is an attention model)\n",
    " \n",
    "represent sentence/doc\n",
    "one-hot encode labels\n",
    "\n",
    "prediction: something with a softmax layer\n",
    "\n",
    "CNNs seem promising, as they can model interactions between words (exactly what we want). They also have a local structure, which is great. (can steal code from demo notebook if I want to use this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sense_key</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>Position</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>keep%2:42:07::</td>\n",
       "      <td>keep.v</td>\n",
       "      <td>15</td>\n",
       "      <td>[action, by, the, committee, in, pursuance, of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>national%3:01:00::</td>\n",
       "      <td>national.a</td>\n",
       "      <td>25</td>\n",
       "      <td>[a, guard, of, honour, stood, in, formation, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>build%2:31:03::</td>\n",
       "      <td>build.v</td>\n",
       "      <td>38</td>\n",
       "      <td>[the, principle, that, statistics, should, be,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>place%1:04:00::</td>\n",
       "      <td>place.n</td>\n",
       "      <td>36</td>\n",
       "      <td>[again, ,, he, appealed, for, additional, supp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>position%1:04:01::</td>\n",
       "      <td>position.n</td>\n",
       "      <td>76</td>\n",
       "      <td>[also, ,, the, iaea, has, the, lowest, number,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Sense_key       Lemma Position  \\\n",
       "0      keep%2:42:07::      keep.v       15   \n",
       "1  national%3:01:00::  national.a       25   \n",
       "2     build%2:31:03::     build.v       38   \n",
       "3     place%1:04:00::     place.n       36   \n",
       "4  position%1:04:01::  position.n       76   \n",
       "\n",
       "                                                Text  \n",
       "0  [action, by, the, committee, in, pursuance, of...  \n",
       "1  [a, guard, of, honour, stood, in, formation, i...  \n",
       "2  [the, principle, that, statistics, should, be,...  \n",
       "3  [again, ,, he, appealed, for, additional, supp...  \n",
       "4  [also, ,, the, iaea, has, the, lowest, number,...  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = load_data(train_path)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sense_key</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>Position</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?</td>\n",
       "      <td>physical.a</td>\n",
       "      <td>58</td>\n",
       "      <td>[iaea, pointed, out, that, training, and, educ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?</td>\n",
       "      <td>see.v</td>\n",
       "      <td>8</td>\n",
       "      <td>[aid, official, development, assistance, (, od...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?</td>\n",
       "      <td>line.n</td>\n",
       "      <td>39</td>\n",
       "      <td>[she, would, appreciate, receiving, informatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>?</td>\n",
       "      <td>keep.v</td>\n",
       "      <td>42</td>\n",
       "      <td>[we, look, forward, to, its, eventual, assessm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>?</td>\n",
       "      <td>national.a</td>\n",
       "      <td>57</td>\n",
       "      <td>[in, his, report, to, the, general, assembly, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sense_key       Lemma Position  \\\n",
       "0         ?  physical.a       58   \n",
       "1         ?       see.v        8   \n",
       "2         ?      line.n       39   \n",
       "3         ?      keep.v       42   \n",
       "4         ?  national.a       57   \n",
       "\n",
       "                                                Text  \n",
       "0  [iaea, pointed, out, that, training, and, educ...  \n",
       "1  [aid, official, development, assistance, (, od...  \n",
       "2  [she, would, appreciate, receiving, informatio...  \n",
       "3  [we, look, forward, to, its, eventual, assessm...  \n",
       "4  [in, his, report, to, the, general, assembly, ...  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = load_data(test_path)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start out simple! ignore position, see it as a document classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_word_frequencies(YOUR_FILE, ENCODING):\n",
    "    \n",
    "    freqs = Counter()\n",
    "    with open(YOUR_FILE, encoding = ENCODING) as f:\n",
    "        for line in f:\n",
    "            tokens = line.lower().split()\n",
    "            for token in tokens:\n",
    "                freqs[token] += 1\n",
    "                \n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_int(docs:list()) -> (list(), int, Counter()):\n",
    "    \n",
    "    '''\n",
    "    Function from assignment 2.\n",
    "    \n",
    "    Create bag of words from cleaned and smaller corpus.\n",
    "    Associate each word in bag with an unique integer,\n",
    "    ranging from 0 (most common word) to length of bag of words.\n",
    "    Map each token in docs to the respective int. Return this list of list of ints.\n",
    "    '''\n",
    "    \n",
    "    freqs = Counter()\n",
    "    for doc in docs:\n",
    "        for token in doc:\n",
    "            freqs[token] += 1\n",
    "    most_common = freqs.most_common()\n",
    "    \n",
    "    token_to_int = []\n",
    "    for i in range(len(most_common)):\n",
    "        token_to_int.append(most_common[i][0])\n",
    "\n",
    "    # Get pairs of elements    \n",
    "    mapping = zip(token_to_int, range(0,len(token_to_int)))\n",
    "    \n",
    "    # Make pairs into a dictionary\n",
    "    vocab = dict(mapping)\n",
    "        \n",
    "    # Match token to int\n",
    "    docs_int = list()\n",
    "    \n",
    "    for doc in docs:\n",
    "        docs_int.append(list([vocab.get(x) for x in doc]))\n",
    "        \n",
    "    return docs_int, len(vocab), vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_int, vocab_size, vocab = map_to_int(text_list)\n",
    "df[\"Text_int\"] = texts_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "see.v             6538\n",
       "line.n            5545\n",
       "keep.v            5393\n",
       "follow.v          3297\n",
       "hold.v            3126\n",
       "serve.v           3030\n",
       "force.n           2753\n",
       "lead.v            2549\n",
       "build.v           2495\n",
       "bring.v           2494\n",
       "extend.v          2452\n",
       "find.v            2399\n",
       "case.n            2356\n",
       "position.n        2221\n",
       "security.n        2164\n",
       "national.a        2146\n",
       "life.n            2141\n",
       "time.n            2121\n",
       "professional.a    2004\n",
       "order.n           2004\n",
       "regular.a         1974\n",
       "place.n           1931\n",
       "point.n           1913\n",
       "physical.a        1895\n",
       "common.a          1744\n",
       "bad.a             1732\n",
       "critical.a        1567\n",
       "major.a           1507\n",
       "active.a          1342\n",
       "positive.a        1216\n",
       "Name: Lemma, dtype: int64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Lemma.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with just one lemma - positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1216, 5)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos = df[df.Lemma == \"positive.a\"]\n",
    "df_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive%3:00:01::                  431\n",
       "positive%5:00:00:advantageous:00    292\n",
       "positive%5:00:00:plus:00            240\n",
       "positive%3:00:04::                  196\n",
       "positive%5:00:00:formal:01           57\n",
       "Name: Sense_key, dtype: int64"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos.Sense_key.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sense_key</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>Position</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>positive%5:00:00:advantageous:00</td>\n",
       "      <td>positive.a</td>\n",
       "      <td>51</td>\n",
       "      <td>[thirdly, ,, there, are, situations, where, na...</td>\n",
       "      <td>[3659, 2, 78, 22, 902, 165, 51, 20, 28, 835, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>positive%5:00:00:advantageous:00</td>\n",
       "      <td>positive.a</td>\n",
       "      <td>26</td>\n",
       "      <td>[the, executive, director, made, an, oral, pre...</td>\n",
       "      <td>[0, 600, 1045, 94, 29, 3441, 2193, 10, 152, 85...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>positive%5:00:00:plus:00</td>\n",
       "      <td>positive.a</td>\n",
       "      <td>18</td>\n",
       "      <td>[private, financial, flows, since, the, asian,...</td>\n",
       "      <td>[332, 148, 2130, 200, 0, 2018, 148, 843, 2, 33...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>positive%3:00:01::</td>\n",
       "      <td>positive.a</td>\n",
       "      <td>39</td>\n",
       "      <td>[she, emphasized, the, importance, of, governm...</td>\n",
       "      <td>[321, 1529, 0, 346, 1, 64, 924, 6, 7519, 24, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>positive%5:00:00:plus:00</td>\n",
       "      <td>positive.a</td>\n",
       "      <td>49</td>\n",
       "      <td>[central, banks, were, compelled, to, manage, ...</td>\n",
       "      <td>[463, 2097, 50, 5364, 4, 2223, 1414, 1675, 225...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Sense_key       Lemma Position  \\\n",
       "33   positive%5:00:00:advantageous:00  positive.a       51   \n",
       "185  positive%5:00:00:advantageous:00  positive.a       26   \n",
       "332          positive%5:00:00:plus:00  positive.a       18   \n",
       "355                positive%3:00:01::  positive.a       39   \n",
       "399          positive%5:00:00:plus:00  positive.a       49   \n",
       "\n",
       "                                                  Text  \\\n",
       "33   [thirdly, ,, there, are, situations, where, na...   \n",
       "185  [the, executive, director, made, an, oral, pre...   \n",
       "332  [private, financial, flows, since, the, asian,...   \n",
       "355  [she, emphasized, the, importance, of, governm...   \n",
       "399  [central, banks, were, compelled, to, manage, ...   \n",
       "\n",
       "                                              Text_int  \n",
       "33   [3659, 2, 78, 22, 902, 165, 51, 20, 28, 835, 1...  \n",
       "185  [0, 600, 1045, 94, 29, 3441, 2193, 10, 152, 85...  \n",
       "332  [332, 148, 2130, 200, 0, 2018, 148, 843, 2, 33...  \n",
       "355  [321, 1529, 0, 346, 1, 64, 924, 6, 7519, 24, 3...  \n",
       "399  [463, 2097, 50, 5364, 4, 2223, 1414, 1675, 225...  "
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will onehot encode the sense key. This makes the most sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sara\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Sara\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "for i, val in enumerate(df_pos.Sense_key.unique()):\n",
    "\n",
    "    col_name = \"Onehot_sense_\" + str(i)\n",
    "    df_pos[col_name] = df.loc[:, \"Sense_key\"] == val\n",
    "    df_pos[col_name] = df_pos[col_name].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sense_key</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>Position</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_int</th>\n",
       "      <th>Onehot_sense_0</th>\n",
       "      <th>Onehot_sense_1</th>\n",
       "      <th>Onehot_sense_2</th>\n",
       "      <th>Onehot_sense_3</th>\n",
       "      <th>Onehot_sense_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>positive%5:00:00:advantageous:00</td>\n",
       "      <td>positive.a</td>\n",
       "      <td>51</td>\n",
       "      <td>[thirdly, ,, there, are, situations, where, na...</td>\n",
       "      <td>[3659, 2, 78, 22, 902, 165, 51, 20, 28, 835, 1...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>positive%5:00:00:advantageous:00</td>\n",
       "      <td>positive.a</td>\n",
       "      <td>26</td>\n",
       "      <td>[the, executive, director, made, an, oral, pre...</td>\n",
       "      <td>[0, 600, 1045, 94, 29, 3441, 2193, 10, 152, 85...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Sense_key       Lemma Position  \\\n",
       "33   positive%5:00:00:advantageous:00  positive.a       51   \n",
       "185  positive%5:00:00:advantageous:00  positive.a       26   \n",
       "\n",
       "                                                  Text  \\\n",
       "33   [thirdly, ,, there, are, situations, where, na...   \n",
       "185  [the, executive, director, made, an, oral, pre...   \n",
       "\n",
       "                                              Text_int  Onehot_sense_0  \\\n",
       "33   [3659, 2, 78, 22, 902, 165, 51, 20, 28, 835, 1...               1   \n",
       "185  [0, 600, 1045, 94, 29, 3441, 2193, 10, 152, 85...               1   \n",
       "\n",
       "     Onehot_sense_1  Onehot_sense_2  Onehot_sense_3  Onehot_sense_4  \n",
       "33                0               0               0               0  \n",
       "185               0               0               0               0  "
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "for item in df_pos.Text_int:\n",
    "    \n",
    "    if len(item) > max_len:\n",
    "        max_len = len(item)\n",
    "        \n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df_pos.Text_int)):\n",
    "    \n",
    "    diff = max_len - len(df_pos.Text_int.iloc[i]) \n",
    "    \n",
    "    vec = df_pos.Text_int.iloc[i]\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_pos.Text_int.iloc\n",
    "    \n",
    "    print(diff)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3659,\n",
       " 2,\n",
       " 78,\n",
       " 22,\n",
       " 902,\n",
       " 165,\n",
       " 51,\n",
       " 20,\n",
       " 28,\n",
       " 835,\n",
       " 1068,\n",
       " 29,\n",
       " 399,\n",
       " 11,\n",
       " 0,\n",
       " 652,\n",
       " 4,\n",
       " 1728,\n",
       " 1743,\n",
       " 2,\n",
       " 296,\n",
       " 4643,\n",
       " 20,\n",
       " 5354,\n",
       " 2,\n",
       " 16505,\n",
       " 3,\n",
       " 2599,\n",
       " 1,\n",
       " 1189,\n",
       " 1429,\n",
       " 2,\n",
       " 67,\n",
       " 4229,\n",
       " 3953,\n",
       " 5,\n",
       " 6,\n",
       " 53,\n",
       " 780,\n",
       " 2,\n",
       " 0,\n",
       " 638,\n",
       " 1,\n",
       " 0,\n",
       " 276,\n",
       " 33,\n",
       " 233,\n",
       " 13,\n",
       " 4,\n",
       " 427,\n",
       " 272,\n",
       " 340,\n",
       " 835,\n",
       " 12,\n",
       " 614,\n",
       " 12,\n",
       " 258,\n",
       " 166,\n",
       " 12,\n",
       " 2,\n",
       " 4,\n",
       " 807,\n",
       " 4,\n",
       " 308,\n",
       " 10,\n",
       " 0,\n",
       " 1070,\n",
       " 24,\n",
       " 11879,\n",
       " 0,\n",
       " 6276,\n",
       " 1,\n",
       " 396,\n",
       " 273,\n",
       " 5,\n",
       " 23,\n",
       " 0,\n",
       " 230,\n",
       " 90,\n",
       " 2,\n",
       " 0,\n",
       " 276,\n",
       " 2926,\n",
       " 4,\n",
       " 1125,\n",
       " 1646,\n",
       " 3,\n",
       " 10258,\n",
       " 1182,\n",
       " 1808,\n",
       " 31,\n",
       " 1979,\n",
       " 1,\n",
       " 134,\n",
       " 8,\n",
       " 619,\n",
       " 544,\n",
       " 7,\n",
       " 5977,\n",
       " 439,\n",
       " 10,\n",
       " 0,\n",
       " 49,\n",
       " 1,\n",
       " 0,\n",
       " 1388,\n",
       " 5,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " array([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "        -1., -1., -1., -1., -1., -1., -1., -1., -1.])]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len - len(df_pos.Text_int.iloc[0])\n",
    "\n",
    "vec = df_pos.Text_int.iloc[0]\n",
    "\n",
    "append = -1*np.ones(diff)\n",
    "\n",
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want a training accuracy score for each network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could learn representation as I go, but there's not a lot of examples per unique sense_key, in some cases...\n",
    "Also, this is a pain. Since the WSD texts appear to be generic enough, pretrained GloVe vectors should be ok. \n",
    "Do I use these as an initial guess or what? Also, GloVe only encodes one word at a time - so do I apply a context window myself as well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for a 50-dim embedding of a 100-word document, we get a $100*50$ matrix. Seems to make sense to run a CNN over this! \n",
    "\n",
    "output layer size should depend on the number of distinct senses for each lemma, so this is a lemma-by-lemma approach\n",
    "\n",
    "or try summing up all the vectors to create one representation for the entire document, then input it into a deep neural net of size 50. however this is silly and a RNN is better, can then have feedback in time if we input one word at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify using a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://towardsdatascience.com/text-classification-with-cnns-in-pytorch-1113df31e79f\n",
    "\n",
    "class TextClassifier(nn.ModuleList):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        super(TextClassifier, self).__init__()\n",
    "\n",
    "        # Parameters regarding text preprocessing\n",
    "        self.seq_len = params.seq_len\n",
    "        self.num_words = params.num_words\n",
    "        self.embedding_size = params.embedding_size\n",
    "\n",
    "        # Dropout definition\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        # CNN parameters definition\n",
    "        # Kernel sizes\n",
    "        self.kernel_1 = 2\n",
    "        self.kernel_2 = 3\n",
    "        self.kernel_3 = 4\n",
    "        self.kernel_4 = 5\n",
    "\n",
    "        # Output size for each convolution\n",
    "        self.out_size = params.out_size\n",
    "        # Number of strides for each convolution\n",
    "        self.stride = params.stride\n",
    "\n",
    "        # Embedding layer definition\n",
    "        self.embedding = nn.Embedding(self.num_words + 1, self.embedding_size, padding_idx=0)\n",
    "\n",
    "        # Convolution layers definition\n",
    "        self.conv_1 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_1, self.stride)\n",
    "        self.conv_2 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_2, self.stride)\n",
    "        self.conv_3 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_3, self.stride)\n",
    "        self.conv_4 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_4, self.stride)\n",
    "\n",
    "        # Max pooling layers definition\n",
    "        self.pool_1 = nn.MaxPool1d(self.kernel_1, self.stride)\n",
    "        self.pool_2 = nn.MaxPool1d(self.kernel_2, self.stride)\n",
    "        self.pool_3 = nn.MaxPool1d(self.kernel_3, self.stride)\n",
    "        self.pool_4 = nn.MaxPool1d(self.kernel_4, self.stride)\n",
    "\n",
    "        # Fully connected layer definition\n",
    "        self.fc = nn.Linear(self.in_features_fc(), 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
