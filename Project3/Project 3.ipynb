{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep%2:42:07::\tkeep.v\t15\taction by the committee in pursuance of its mandate , the committee will continue to keep under review the situation relating to the question of palestine and participate in relevant meetings of the general assembly and the security council . the committee will also continue to monitor the situation on the ground and draw the attention of the international community to urgent developments in the occupied palestinian territory , including east jerusalem , requiring international action .\n",
      "\n",
      "?\tphysical.a\t58\tiaea pointed out that training and education were fundamental to the agency 's approach to enhancing physical protection systems in states . training courses , workshops and seminars that had been held on six continents had raised awareness and had provided hands-on experience of various subjects including the physical protection of research facilities , the practical operation of physical protection systems , and the engineering safety aspects of physical protection managing situations involving malevolent acts . in the area of physical protection 60 courses had been conducted in the past three years .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_path = \"a3_data/wsd_train.txt\"\n",
    "test_path = \"a3_data/wsd_test_blind.txt\"\n",
    "\n",
    "with open(train_path, encoding = \"utf-8\") as f:\n",
    "    for d, line in enumerate(f):\n",
    "        print(line.lower())\n",
    "        break\n",
    "        \n",
    "with open(test_path, encoding = \"utf-8\") as f:\n",
    "    for d, line in enumerate(f):\n",
    "        print(line.lower())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \n",
    "    sense_list = []\n",
    "    lemma_list = []\n",
    "    position_list = []\n",
    "    text_list = []\n",
    "\n",
    "    with open(file_path, encoding = \"utf-8\") as f:\n",
    "        for d, line in enumerate(f):\n",
    "\n",
    "            line = line.lower()\n",
    "\n",
    "            ix = line.find(\"\\t\")\n",
    "            sense_key = line[0:ix]\n",
    "            line = line[ix+1:]\n",
    "\n",
    "            ix = line.find(\"\\t\")\n",
    "            lemma = line[0:ix]\n",
    "            line = line[ix+1:]\n",
    "\n",
    "            ix = line.find(\"\\t\")\n",
    "            position = line[0:ix]\n",
    "            text = line[ix+1:].split()\n",
    "\n",
    "            #if d == 0:\n",
    "            #    print(\"sense_key \", sense_key)\n",
    "            #    print(\"lemma \", lemma)\n",
    "            #    print(\"position \", position)\n",
    "            #    print(\"text \", text)\n",
    "\n",
    "            sense_list.append(sense_key)\n",
    "            lemma_list.append(lemma)\n",
    "            position_list.append(position)\n",
    "            text_list.append(text)\n",
    "\n",
    "            #if d == 10000:\n",
    "            #    break\n",
    "\n",
    "    #print(d)\n",
    "    \n",
    "    df = pd.DataFrame(sense_list, columns = [\"Sense_key\"])\n",
    "    df[\"Lemma\"] = lemma_list\n",
    "    df[\"Position\"] = position_list\n",
    "    df[\"Text\"] = text_list\n",
    "\n",
    "    del sense_list, lemma_list, position_list\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self, df, num_words, seq_len):\n",
    "        self.data = df\n",
    "        self.num_words = num_words\n",
    "        self.seq_len = seq_len  \n",
    "        \n",
    "        self.vocabulary = None\n",
    "        self.x_tokenized = None\n",
    "        self.x_embedded = None\n",
    "        self.x_padded = None\n",
    "        self.x_raw = None\n",
    "        \n",
    "        self.lemma = None\n",
    "        self.n_outputs = None\n",
    "        self.le = None\n",
    "        self.y = None\n",
    "        \n",
    "        self.x_train = None\n",
    "        self.x_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "    \n",
    "    def load_data(self):\n",
    "        # split into sentences (x) and sense key (y)\n",
    "        df = self.data\n",
    "        self.x_raw = df.Text.values\n",
    "        self.lemma = df.Lemma.iloc[0]\n",
    "        self.n_outputs = len(df.Sense_key.unique())\n",
    "        \n",
    "        labels = np.asarray(df.Sense_key.values)\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        targets = le.fit_transform(labels)\n",
    "        self.y = targets\n",
    "        self.le = le\n",
    "        \n",
    "    def build_vocabulary(self):\n",
    "        # Builds the vocabulary \n",
    "        self.vocabulary = dict()\n",
    "        fdist = nltk.FreqDist()\n",
    "\n",
    "        for sentence in self.x_raw:\n",
    "            for word in sentence:\n",
    "                fdist[word] += 1\n",
    "\n",
    "        common_words = fdist.most_common(self.num_words)\n",
    "\n",
    "        for idx, word in enumerate(common_words):\n",
    "            self.vocabulary[word[0]] = (idx+1)\n",
    "            \n",
    "    def word_to_idx(self):\n",
    "        # By using the dictionary (vocabulary), it is transformed\n",
    "        # each token into its index based representatio\n",
    "        self.x_tokenized = list() \n",
    "\n",
    "        for sentence in self.x_raw:\n",
    "            temp_sentence = list()\n",
    "            for word in sentence:\n",
    "                if word in self.vocabulary.keys():\n",
    "                    temp_sentence.append(self.vocabulary[word])\n",
    "            self.x_tokenized.append(temp_sentence)\n",
    "        \n",
    "    def find_seq_len(self):\n",
    "        \n",
    "        max_len = 0\n",
    "        for item in self.x_raw:\n",
    "    \n",
    "            if len(item) > max_len:\n",
    "                max_len = len(item)\n",
    "        \n",
    "        self.seq_len = max_len\n",
    "    \n",
    "    def padding_sentences(self):\n",
    "        # Each sentence which does not fulfill the required length is padded with the index 0\n",
    "        pad_idx = 0\n",
    "        self.x_padded = list()\n",
    "\n",
    "        for sentence in self.x_tokenized:\n",
    "            while len(sentence) < self.seq_len:\n",
    "                sentence.insert(len(sentence), pad_idx)\n",
    "\n",
    "            self.x_padded.append(sentence)\n",
    "            \n",
    "        self.x_padded = np.array(self.x_padded) \n",
    "    \n",
    "    def split_data(self):\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.x_padded, self.y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "\n",
    "class Preprocessing:\n",
    "\n",
    "    def __init__(self, df, num_words, seq_len):\n",
    "        self.data = df\n",
    "        self.num_words = num_words\n",
    "        self.seq_len = seq_len\n",
    "        self.vocabulary = None\n",
    "        self.x_tokenized = None\n",
    "        self.x_padded = None\n",
    "        self.x_raw = None\n",
    "        self.y = None\n",
    "\n",
    "        self.x_train = None\n",
    "        self.x_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "\n",
    "    def load_data(self):\n",
    "        # Reads the raw df file and split into\n",
    "        # sentences (x) and target (y)\n",
    "        self.x_raw = df.Text.values\n",
    "        self.y = df.Sense_key.values\n",
    "\n",
    "    def clean_text(self):\n",
    "        # Removes special symbols and just keep\n",
    "        # words in lower or upper form\n",
    "\n",
    "        #self.x_raw = [x.lower() for x in self.x_raw]\n",
    "        self.x_raw = [re.sub(r'[^A-Za-z]+', ' ', x) for x in self.x_raw]\n",
    "\n",
    "    #def text_tokenization(self):\n",
    "        # Tokenizes each sentence by implementing the nltk tool\n",
    "        #self.x_raw = [word_tokenize(x) for x in self.x_raw]\n",
    "\n",
    "    def build_vocabulary(self):\n",
    "        # Builds the vocabulary and keeps the \"x\" most frequent words\n",
    "        self.vocabulary = dict()\n",
    "        fdist = nltk.FreqDist()\n",
    "\n",
    "        for sentence in self.x_raw:\n",
    "            for word in sentence:\n",
    "                fdist[word] += 1\n",
    "\n",
    "        common_words = fdist.most_common(self.num_words)\n",
    "\n",
    "        for idx, word in enumerate(common_words):\n",
    "            self.vocabulary[word[0]] = (idx+1)\n",
    "            \n",
    "    def word_to_idx(self):\n",
    "        # By using the dictionary (vocabulary), it is transformed\n",
    "        # each token into its index based representation\n",
    "\n",
    "        self.x_tokenized = list()\n",
    "\n",
    "        for sentence in self.x_raw:\n",
    "            temp_sentence = list()\n",
    "        for word in sentence:\n",
    "            if word in self.vocabulary.keys():\n",
    "                temp_sentence.append(self.vocabulary[word])\n",
    "        self.x_tokenized.append(temp_sentence)\n",
    "   \n",
    "    def padding_sentences(self):\n",
    "        # Each sentence which does not fulfill the required len\n",
    "        # it's padded with the index 0\n",
    "\n",
    "        pad_idx = 0\n",
    "        self.x_padded = list()\n",
    "\n",
    "        for sentence in self.x_tokenized:\n",
    "            while len(sentence) < self.seq_len:\n",
    "                sentence.insert(len(sentence), pad_idx)\n",
    "            self.x_padded.append(sentence)\n",
    "        self.x_padded = np.array(self.x_padded)\n",
    "   \n",
    "    def split_data(self):\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.x_padded, self.y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "sense_list = [\"positive%3:00:01::\", \"positive%5:00:00:advantageous:00\"]\n",
    "\n",
    "df_pos = df[df.Lemma == \"positive.a\"]\n",
    "df_pos = df_pos.loc[df_pos.Sense_key.isin(sense_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_pos.Sense_key.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "723"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = Preprocessing(df_pos, num_words = 6000, seq_len = 100)\n",
    "data_pos.load_data()\n",
    "data_pos.build_vocabulary()\n",
    "data_pos.word_to_idx()\n",
    "data_pos.find_seq_len()\n",
    "data_pos.padding_sentences()\n",
    "data_pos.split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 4, 0, 4, 1, 0, 1, 1, 2, 2, 4, 1, 1, 4, 0, 2, 4, 2, 1, 2, 2,\n",
       "       4, 4, 0, 4, 0, 2, 4, 4])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pos.y[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1840, 4624, 1923,    1,  459,  134,   10,  718,   44,   80,   64,\n",
       "        778,  109,   42,  333,  120,    5,   94,    2,   23,   20,   35,\n",
       "       1216,    6,   26,    8,  964, 1808,    3, 1622,    2,    4,  311,\n",
       "       4625,  335,    1,  459,  134,   45,  762, 3078,   53,    9,  585,\n",
       "          7,  237,    6, 4626,   53,  855,   91,    5,   10,  480,    2,\n",
       "          1,  125,  588,   11, 1857, 3461,   37,  301,    1, 4627,  957,\n",
       "          3,   39,   33,   91,   55,   18,  243,    6,   11, 2462,   12,\n",
       "         39,    2,  247,   67,   87, 1902, 1384,    1, 1331,    5,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pos.x_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(912, 236)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pos.x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Embedding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Are stopwords able to change the sense of a word? I think so!\n",
    "\n",
    "- standing in line - waiting for something\n",
    "- standing in a line - they're just standing \n",
    "\n",
    "Based on this, I will not remove stopwords. I will also leave in punctuation, but it seems like a good idea to lowercase the entire text. We're not doing NER, and I don't want Line and line to end up having two meanings - the position alone should clarify the sense. CBoW seems like a terrible choice in this setting - the word senses will almost certainly get lost. Try representation with pre-trained GloVe vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "idea: only embed the sentence containing the word in question (maybe later)\n",
    "use word position in an attention model, or for determining weights in a CNN/RNN (think that is an attention model)\n",
    " \n",
    "represent sentence/doc\n",
    "one-hot encode labels\n",
    "\n",
    "prediction: something with a softmax layer\n",
    "\n",
    "CNNs seem promising, as they can model interactions between words (exactly what we want). They also have a local structure, which is great. (can steal code from demo notebook if I want to use this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sense_key</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>Position</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>keep%2:42:07::</td>\n",
       "      <td>keep.v</td>\n",
       "      <td>15</td>\n",
       "      <td>[action, by, the, committee, in, pursuance, of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>national%3:01:00::</td>\n",
       "      <td>national.a</td>\n",
       "      <td>25</td>\n",
       "      <td>[a, guard, of, honour, stood, in, formation, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>build%2:31:03::</td>\n",
       "      <td>build.v</td>\n",
       "      <td>38</td>\n",
       "      <td>[the, principle, that, statistics, should, be,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>place%1:04:00::</td>\n",
       "      <td>place.n</td>\n",
       "      <td>36</td>\n",
       "      <td>[again, ,, he, appealed, for, additional, supp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>position%1:04:01::</td>\n",
       "      <td>position.n</td>\n",
       "      <td>76</td>\n",
       "      <td>[also, ,, the, iaea, has, the, lowest, number,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Sense_key       Lemma Position  \\\n",
       "0      keep%2:42:07::      keep.v       15   \n",
       "1  national%3:01:00::  national.a       25   \n",
       "2     build%2:31:03::     build.v       38   \n",
       "3     place%1:04:00::     place.n       36   \n",
       "4  position%1:04:01::  position.n       76   \n",
       "\n",
       "                                                Text  \n",
       "0  [action, by, the, committee, in, pursuance, of...  \n",
       "1  [a, guard, of, honour, stood, in, formation, i...  \n",
       "2  [the, principle, that, statistics, should, be,...  \n",
       "3  [again, ,, he, appealed, for, additional, supp...  \n",
       "4  [also, ,, the, iaea, has, the, lowest, number,...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = load_data(train_path)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sense_key</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>Position</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?</td>\n",
       "      <td>physical.a</td>\n",
       "      <td>58</td>\n",
       "      <td>[iaea, pointed, out, that, training, and, educ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?</td>\n",
       "      <td>see.v</td>\n",
       "      <td>8</td>\n",
       "      <td>[aid, official, development, assistance, (, od...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?</td>\n",
       "      <td>line.n</td>\n",
       "      <td>39</td>\n",
       "      <td>[she, would, appreciate, receiving, informatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>?</td>\n",
       "      <td>keep.v</td>\n",
       "      <td>42</td>\n",
       "      <td>[we, look, forward, to, its, eventual, assessm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>?</td>\n",
       "      <td>national.a</td>\n",
       "      <td>57</td>\n",
       "      <td>[in, his, report, to, the, general, assembly, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sense_key       Lemma Position  \\\n",
       "0         ?  physical.a       58   \n",
       "1         ?       see.v        8   \n",
       "2         ?      line.n       39   \n",
       "3         ?      keep.v       42   \n",
       "4         ?  national.a       57   \n",
       "\n",
       "                                                Text  \n",
       "0  [iaea, pointed, out, that, training, and, educ...  \n",
       "1  [aid, official, development, assistance, (, od...  \n",
       "2  [she, would, appreciate, receiving, informatio...  \n",
       "3  [we, look, forward, to, its, eventual, assessm...  \n",
       "4  [in, his, report, to, the, general, assembly, ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = load_data(test_path)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "start out simple! ignore position, see it as a document classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_word_frequencies(YOUR_FILE, ENCODING):\n",
    "    \n",
    "    freqs = Counter()\n",
    "    with open(YOUR_FILE, encoding = ENCODING) as f:\n",
    "        for line in f:\n",
    "            tokens = line.lower().split()\n",
    "            for token in tokens:\n",
    "                freqs[token] += 1\n",
    "                \n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def map_to_int(docs:list()) -> (list(), int, Counter()):\n",
    "    \n",
    "    '''\n",
    "    Function from assignment 2.\n",
    "    \n",
    "    Create bag of words from cleaned and smaller corpus.\n",
    "    Associate each word in bag with an unique integer,\n",
    "    ranging from 0 (most common word) to length of bag of words.\n",
    "    Map each token in docs to the respective int. Return this list of list of ints.\n",
    "    '''\n",
    "    \n",
    "    freqs = Counter()\n",
    "    for doc in docs:\n",
    "        for token in doc:\n",
    "            freqs[token] += 1\n",
    "    most_common = freqs.most_common()\n",
    "    \n",
    "    token_to_int = []\n",
    "    for i in range(len(most_common)):\n",
    "        token_to_int.append(most_common[i][0])\n",
    "\n",
    "    # Get pairs of elements    \n",
    "    mapping = zip(token_to_int, range(0,len(token_to_int)))\n",
    "    \n",
    "    # Make pairs into a dictionary\n",
    "    vocab = dict(mapping)\n",
    "        \n",
    "    # Match token to int\n",
    "    docs_int = list()\n",
    "    \n",
    "    for doc in docs:\n",
    "        docs_int.append(list([vocab.get(x) for x in doc]))\n",
    "        \n",
    "    return docs_int, len(vocab), vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Start with just one lemma - positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1216, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos = df[df.Lemma == \"positive.a\"]\n",
    "df_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_pos.Sense_key.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Will onehot encode the sense key. This makes the most sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i, val in enumerate(df_pos.Sense_key.unique()):\n",
    "\n",
    "    col_name = \"Onehot_sense_\" + str(i)\n",
    "    df_pos[col_name] = df.loc[:, \"Sense_key\"] == val\n",
    "    df_pos[col_name] = df_pos[col_name].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pos.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "\n",
    "for item in df_pos.Text_int:\n",
    "    \n",
    "    if len(item) > max_len:\n",
    "        max_len = len(item)\n",
    "        \n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(df_pos.Text_int)):\n",
    "    \n",
    "    diff = max_len - len(df_pos.Text_int.iloc[i]) \n",
    "    \n",
    "    vec = df_pos.Text_int.iloc[i]\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_pos.Text_int.iloc\n",
    "    \n",
    "    print(diff)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max_len - len(df_pos.Text_int.iloc[0])\n",
    "\n",
    "vec = df_pos.Text_int.iloc[0]\n",
    "\n",
    "append = -1*np.ones(diff)\n",
    "\n",
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# want a training accuracy score for each network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Could learn representation as I go, but there's not a lot of examples per unique sense_key, in some cases...\n",
    "Also, this is a pain. Since the WSD texts appear to be generic enough, pretrained GloVe vectors should be ok. \n",
    "Do I use these as an initial guess or what? Also, GloVe only encodes one word at a time - so do I apply a context window myself as well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "for a 50-dim embedding of a 100-word document, we get a $100*50$ matrix. Seems to make sense to run a CNN over this! \n",
    "\n",
    "output layer size should depend on the number of distinct senses for each lemma, so this is a lemma-by-lemma approach\n",
    "\n",
    "or try summing up all the vectors to create one representation for the entire document, then input it into a deep neural net of size 50. however this is silly and a RNN is better, can then have feedback in time if we input one word at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify using a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Parameters:\n",
    "\n",
    "    # Preprocessing parameeters\n",
    "    n_outputs: int = None\n",
    "    seq_len: int = None\n",
    "    num_words: int = 6000\n",
    "\n",
    "    # Model parameters\n",
    "    embedding_size: int = 64\n",
    "    out_size: int = 32\n",
    "    stride: int = 2\n",
    "\n",
    "    # Training parameters\n",
    "    epochs: int = 3\n",
    "    batch_size: int = 12\n",
    "    learning_rate: float = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.ModuleList):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        super(TextClassifier, self).__init__()\n",
    "\n",
    "        # Parameters regarding text preprocessing\n",
    "        self.seq_len = params.seq_len\n",
    "        self.num_words = params.num_words\n",
    "        self.embedding_size = params.embedding_size\n",
    "\n",
    "        # Dropout definition\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        # CNN parameters definition\n",
    "        # Kernel sizes\n",
    "        self.kernel_1 = 2\n",
    "        self.kernel_2 = 3\n",
    "        self.kernel_3 = 4\n",
    "        self.kernel_4 = 5\n",
    "\n",
    "        # Output size for each convolution\n",
    "        self.out_size = params.out_size\n",
    "        # Number of strides for each convolution\n",
    "        self.stride = params.stride\n",
    "\n",
    "        # Embedding layer definition\n",
    "        self.embedding = nn.Embedding(self.num_words + 1, self.embedding_size, padding_idx=0)\n",
    "\n",
    "        # Convolution layers definition\n",
    "        self.conv_1 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_1, self.stride)\n",
    "        self.conv_2 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_2, self.stride)\n",
    "        self.conv_3 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_3, self.stride)\n",
    "        self.conv_4 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_4, self.stride)\n",
    "\n",
    "        # Max pooling layers definition\n",
    "        self.pool_1 = nn.MaxPool1d(self.kernel_1, self.stride)\n",
    "        self.pool_2 = nn.MaxPool1d(self.kernel_2, self.stride)\n",
    "        self.pool_3 = nn.MaxPool1d(self.kernel_3, self.stride)\n",
    "        self.pool_4 = nn.MaxPool1d(self.kernel_4, self.stride)\n",
    "\n",
    "        # Fully connected layer definition\n",
    "        self.fc = nn.Linear(self.in_features_fc(), 1)\n",
    "\n",
    "\n",
    "    def in_features_fc(self):\n",
    "        '''Calculates the number of output features after Convolution + Max pooling\n",
    "\n",
    "        Convolved_Features = ((embedding_size + (2 * padding) - dilation * (kernel - 1) - 1) / stride) + 1\n",
    "        Pooled_Features = ((embedding_size + (2 * padding) - dilation * (kernel - 1) - 1) / stride) + 1\n",
    "\n",
    "        source: https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "        '''\n",
    "        \n",
    "        # Calculate size of convolved/pooled features for convolution_1/max_pooling_1 features\n",
    "        out_conv_1 = ((self.embedding_size - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n",
    "        out_conv_1 = math.floor(out_conv_1)\n",
    "        out_pool_1 = ((out_conv_1 - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n",
    "        out_pool_1 = math.floor(out_pool_1)\n",
    "\n",
    "        # Calcualte size of convolved/pooled features for convolution_2/max_pooling_2 features\n",
    "        out_conv_2 = ((self.embedding_size - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n",
    "        out_conv_2 = math.floor(out_conv_2)\n",
    "        out_pool_2 = ((out_conv_2 - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n",
    "        out_pool_2 = math.floor(out_pool_2)\n",
    "\n",
    "        # Calcualte size of convolved/pooled features for convolution_3/max_pooling_3 features\n",
    "        out_conv_3 = ((self.embedding_size - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n",
    "        out_conv_3 = math.floor(out_conv_3)\n",
    "        out_pool_3 = ((out_conv_3 - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n",
    "        out_pool_3 = math.floor(out_pool_3)\n",
    "\n",
    "        # Calcualte size of convolved/pooled features for convolution_4/max_pooling_4 features\n",
    "        out_conv_4 = ((self.embedding_size - 1 * (self.kernel_4 - 1) - 1) / self.stride) + 1\n",
    "        out_conv_4 = math.floor(out_conv_4)\n",
    "        out_pool_4 = ((out_conv_4 - 1 * (self.kernel_4 - 1) - 1) / self.stride) + 1\n",
    "        out_pool_4 = math.floor(out_pool_4)\n",
    "\n",
    "        # Returns \"flattened\" vector (input for fully connected layer)\n",
    "        return (out_pool_1 + out_pool_2 + out_pool_3 + out_pool_4) * self.out_size\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Sequence of tokes is filtered through an embedding layer\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Convolution layer 1 is applied\n",
    "        x1 = self.conv_1(x)\n",
    "        x1 = torch.relu(x1)\n",
    "        x1 = self.pool_1(x1)\n",
    "\n",
    "        # Convolution layer 2 is applied\n",
    "        x2 = self.conv_2(x)\n",
    "        x2 = torch.relu((x2))\n",
    "        x2 = self.pool_2(x2)\n",
    "\n",
    "        # Convolution layer 3 is applied\n",
    "        x3 = self.conv_3(x)\n",
    "        x3 = torch.relu(x3)\n",
    "        x3 = self.pool_3(x3)\n",
    "\n",
    "        # Convolution layer 4 is applied\n",
    "        x4 = self.conv_4(x)\n",
    "        x4 = torch.relu(x4)\n",
    "        x4 = self.pool_4(x4)\n",
    "\n",
    "        # The output of each convolutional layer is concatenated into a unique vector\n",
    "        union = torch.cat((x1, x2, x3, x4), 2)\n",
    "        union = union.reshape(union.size(0), -1)\n",
    "\n",
    "        # The \"flattened\" vector is passed through a fully connected layer\n",
    "        out = self.fc(union)\n",
    "        # Dropout is applied\n",
    "        out = self.dropout(out)\n",
    "        # Activation function is applied\n",
    "        out = torch.sigmoid(out)\n",
    "\n",
    "        if len(out) > 1:\n",
    "            return out.squeeze()\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetMaper(Dataset):\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "class Run:\n",
    "    '''Training, evaluation and metrics calculation'''\n",
    "\n",
    "    @staticmethod\n",
    "    def train(model, data, params):\n",
    "\n",
    "        # Initialize dataset maper\n",
    "        train = DatasetMaper(data['x_train'], data['y_train'])\n",
    "        test = DatasetMaper(data['x_test'], data['y_test'])\n",
    "\n",
    "        # Initialize loaders\n",
    "        loader_train = DataLoader(train, batch_size=params.batch_size)\n",
    "        loader_test = DataLoader(test, batch_size=params.batch_size)\n",
    "\n",
    "        # Define optimizer\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=params.learning_rate)\n",
    "\n",
    "        # Starts training phase\n",
    "        for epoch in range(params.epochs):\n",
    "            # Set model in training model\n",
    "            model.train()\n",
    "            predictions = []\n",
    "            # Starts batch training\n",
    "            for x_batch, y_batch in loader_train:\n",
    "\n",
    "                y_batch = y_batch.type(torch.FloatTensor)\n",
    "\n",
    "                # Feed the model\n",
    "                y_pred = model(x_batch.long())\n",
    "\n",
    "                # Loss calculation\n",
    "                loss = F.binary_cross_entropy(y_pred, y_batch)\n",
    "\n",
    "                # Clean gradientes\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Gradients calculation\n",
    "                loss.backward()\n",
    "\n",
    "                # Gradients update\n",
    "                optimizer.step()\n",
    "\n",
    "                # Save predictions\n",
    "                predictions += list(y_pred.detach().numpy())\n",
    "\n",
    "            # Evaluation phase\n",
    "            test_predictions = Run.evaluation(model, loader_test)\n",
    "            \n",
    "            # Metrics calculation\n",
    "            train_accuary = Run.calculate_accuracy(data['y_train'], predictions)\n",
    "            test_accuracy = Run.calculate_accuracy(data['y_test'], test_predictions)\n",
    "            print(\"Epoch: %d, loss: %.3f, Train accuracy: %.3f, Test accuracy: %.3f\" % (epoch+1, loss.item(), train_accuary, test_accuracy))\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluation(model, loader_test):\n",
    "\n",
    "        # Set the model in evaluation mode\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "\n",
    "        # Starst evaluation phase\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in loader_test:\n",
    "                y_pred = model(x_batch.long())\n",
    "                predictions += list(y_pred.detach().numpy())\n",
    "        return predictions\n",
    "        \n",
    "    @staticmethod\n",
    "    def calculate_accuracy(grand_truth, predictions):\n",
    "        # Metrics calculation - will need to change this! \n",
    "        true_positives = 0\n",
    "        true_negatives = 0\n",
    "        for true, pred in zip(grand_truth, predictions):\n",
    "            if (pred >= 0.5) and (true == 1):\n",
    "                true_positives += 1\n",
    "            elif (pred < 0.5) and (true == 0):\n",
    "                true_negatives += 1\n",
    "            else:\n",
    "                pass\n",
    "        # Return accuracy\n",
    "        return (true_positives+true_negatives) / len(grand_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 0.694, Train accuracy: 0.572, Test accuracy: 0.602\n",
      "Epoch: 2, loss: 0.224, Train accuracy: 0.777, Test accuracy: 0.602\n",
      "Epoch: 3, loss: 0.002, Train accuracy: 0.851, Test accuracy: 0.652\n"
     ]
    }
   ],
   "source": [
    "class Controller(Parameters):\n",
    "\n",
    "    def __init__(self):\n",
    "        # Preprocessing pipeline\n",
    "        self.data, seq_len, n_outputs = self.prepare_data(df_pos, Parameters.num_words, Parameters.seq_len)\n",
    "        Parameters.seq_len = seq_len\n",
    "        Parameters.n_outputs = n_outputs\n",
    "\n",
    "        # Initialize the model\n",
    "        self.model = TextClassifier(Parameters)\n",
    "\n",
    "        # Training - Evaluation pipeline\n",
    "        Run().train(self.model, self.data, Parameters)\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_data(df, num_words, seq_len):\n",
    "        # Preprocessing pipeline\n",
    "        pr = Preprocessing(df, num_words, seq_len)\n",
    "        pr.load_data()\n",
    "        pr.build_vocabulary()\n",
    "        pr.word_to_idx()\n",
    "        pr.find_seq_len()\n",
    "        pr.padding_sentences()\n",
    "        pr.split_data()\n",
    "\n",
    "        return {'x_train': pr.x_train, 'y_train': pr.y_train, 'x_test': pr.x_test, 'y_test': pr.y_test}, pr.seq_len, pr.n_outputs\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    controller = Controller()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(df, num_words, seq_len):\n",
    "    # Preprocessing pipeline\n",
    "    pr = Preprocessing(df, num_words, seq_len)\n",
    "    pr.load_data()\n",
    "    pr.build_vocabulary()\n",
    "    pr.word_to_idx()\n",
    "    pr.find_seq_len()\n",
    "    pr.padding_sentences()\n",
    "    pr.split_data()\n",
    "\n",
    "    return {'x_train': pr.x_train, 'y_train': pr.y_train, 'x_test': pr.x_test, 'y_test': pr.y_test}\n",
    "\n",
    "data = prepare_data(df_pos, num_words = 6000, seq_len = 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(542, 224)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"x_train\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "5\n",
      "Epoch: 1, loss: 0.69262, Train accuracy: 0.52583, Test accuracy: 0.47514\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "5\n",
      "Epoch: 2, loss: 0.46222, Train accuracy: 0.70295, Test accuracy: 0.59116\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "5\n",
      "Epoch: 3, loss: 0.36723, Train accuracy: 0.82288, Test accuracy: 0.51381\n"
     ]
    }
   ],
   "source": [
    "params = Parameters()\n",
    "model = TextClassifier(Parameters)\n",
    "\n",
    "# Initialize dataset maper\n",
    "train = DatasetMaper(data['x_train'], data['y_train'])\n",
    "test = DatasetMaper(data['x_test'], data['y_test'])\n",
    "\n",
    "# Initialize loaders\n",
    "loader_train = DataLoader(train, batch_size=16)\n",
    "loader_test = DataLoader(test, batch_size=16)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=params.learning_rate)\n",
    "\n",
    "# Starts training phase\n",
    "for epoch in range(params.epochs):\n",
    "    # Set model in training model\n",
    "    model.train()\n",
    "    predictions = []\n",
    "    # Starts batch training\n",
    "    for x_batch, y_batch in loader_train:\n",
    "\n",
    "        y_batch = y_batch.type(torch.FloatTensor)\n",
    "        \n",
    "        # Feed the model\n",
    "        y_pred = model(x_batch.long())\n",
    "\n",
    "        # Loss calculation\n",
    "        loss = F.binary_cross_entropy(y_pred, y_batch)\n",
    "\n",
    "        # Clean gradientes\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Gradients calculation\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradients update\n",
    "        optimizer.step()\n",
    "\n",
    "        # Save predictions\n",
    "        predictions += list(y_pred.detach().numpy())\n",
    "        \n",
    "    # Evaluation phase\n",
    "    test_predictions = Run.evaluation(model, loader_test)\n",
    "\n",
    "    # Metrics calculation\n",
    "    train_accuracy = Run.calculate_accuracy(data['y_train'], predictions)\n",
    "    test_accuracy = Run.calculate_accuracy(data['y_test'], test_predictions)\n",
    "    print(\"Epoch: %d, loss: %.5f, Train accuracy: %.5f, Test accuracy: %.5f\" % (epoch+1, loss.item(), train_accuracy, test_accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
