{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Have a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep%2:42:07::\tkeep.v\t15\taction by the committee in pursuance of its mandate , the committee will continue to keep under review the situation relating to the question of palestine and participate in relevant meetings of the general assembly and the security council . the committee will also continue to monitor the situation on the ground and draw the attention of the international community to urgent developments in the occupied palestinian territory , including east jerusalem , requiring international action .\n",
      "\n",
      "?\tphysical.a\t58\tiaea pointed out that training and education were fundamental to the agency 's approach to enhancing physical protection systems in states . training courses , workshops and seminars that had been held on six continents had raised awareness and had provided hands-on experience of various subjects including the physical protection of research facilities , the practical operation of physical protection systems , and the engineering safety aspects of physical protection managing situations involving malevolent acts . in the area of physical protection 60 courses had been conducted in the past three years .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Have a look at the data\n",
    "\n",
    "train_path = \"a3_data/wsd_train.txt\"\n",
    "test_path = \"a3_data/wsd_test_blind.txt\"\n",
    "\n",
    "with open(train_path, encoding = \"utf-8\") as f:\n",
    "    for d, line in enumerate(f):\n",
    "        print(line.lower())\n",
    "        break\n",
    "        \n",
    "with open(test_path, encoding = \"utf-8\") as f:\n",
    "    for d, line in enumerate(f):\n",
    "        print(line.lower())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \n",
    "    sense_list = []\n",
    "    lemma_list = []\n",
    "    position_list = []\n",
    "    text_list = []\n",
    "\n",
    "    with open(file_path, encoding = \"utf-8\") as f:\n",
    "        for d, line in enumerate(f):\n",
    "\n",
    "            line = line.lower()\n",
    "\n",
    "            ix = line.find(\"\\t\")\n",
    "            sense_key = line[0:ix]\n",
    "            line = line[ix+1:]\n",
    "\n",
    "            ix = line.find(\"\\t\")\n",
    "            lemma = line[0:ix]\n",
    "            line = line[ix+1:]\n",
    "\n",
    "            ix = line.find(\"\\t\")\n",
    "            position = line[0:ix]\n",
    "            text = line[ix+1:].split()\n",
    "\n",
    "            sense_list.append(sense_key)\n",
    "            lemma_list.append(lemma)\n",
    "            position_list.append(position)\n",
    "            text_list.append(text)\n",
    "    \n",
    "    df = pd.DataFrame(sense_list, columns = [\"Sense_key\"])\n",
    "    df[\"Lemma\"] = lemma_list\n",
    "    df[\"Position\"] = position_list\n",
    "    df[\"Text\"] = text_list\n",
    "\n",
    "    del sense_list, lemma_list, position_list\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self, df, num_words, seq_len):\n",
    "        self.data = df\n",
    "        self.num_words = num_words\n",
    "        self.seq_len = seq_len  \n",
    "        \n",
    "        self.vocabulary = None\n",
    "        self.x_tokenized = None\n",
    "        self.x_embedded = None\n",
    "        self.x_padded = None\n",
    "        self.x_raw = None\n",
    "        \n",
    "        self.lemma = None\n",
    "        self.n_outputs = None\n",
    "        self.le = None\n",
    "        self.y = None\n",
    "        self.y_onehot = None\n",
    "        \n",
    "        self.x_train = None\n",
    "        self.x_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None  \n",
    "    \n",
    "    def load_data(self):\n",
    "        # split into sentences (x) and sense key (y)\n",
    "        df = self.data\n",
    "        self.x_raw = df.Text.values\n",
    "        self.lemma = df.Lemma.iloc[0]\n",
    "        self.n_outputs = len(df.Sense_key.unique())\n",
    "        \n",
    "        labels = np.asarray(df.Sense_key.values)\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        self.y = le.fit_transform(labels)\n",
    "        self.le = le\n",
    "        \n",
    "    def build_vocabulary(self):\n",
    "        # Builds the vocabulary \n",
    "        self.vocabulary = dict()\n",
    "        fdist = nltk.FreqDist()\n",
    "\n",
    "        for sentence in self.x_raw:\n",
    "            for word in sentence:\n",
    "                fdist[word] += 1\n",
    "\n",
    "        common_words = fdist.most_common(self.num_words)\n",
    "\n",
    "        for idx, word in enumerate(common_words):\n",
    "            self.vocabulary[word[0]] = (idx+1)\n",
    "            \n",
    "    def word_to_idx(self):\n",
    "        # By using the dictionary each token is transformed into its index based representation\n",
    "        self.x_tokenized = list() \n",
    "\n",
    "        for sentence in self.x_raw:\n",
    "            temp_sentence = list()\n",
    "            for word in sentence:\n",
    "                if word in self.vocabulary.keys():\n",
    "                    temp_sentence.append(self.vocabulary[word])\n",
    "            self.x_tokenized.append(temp_sentence)\n",
    "        \n",
    "    def find_seq_len(self):\n",
    "        # Find length of the longest line in the data\n",
    "        max_len = 0\n",
    "        for item in self.x_raw:\n",
    "    \n",
    "            if len(item) > max_len:\n",
    "                max_len = len(item)\n",
    "        \n",
    "        self.seq_len = max_len\n",
    "    \n",
    "    def padding_sentences(self):\n",
    "        # Each sentence which does not fulfill the required length is padded with the index 0\n",
    "        pad_idx = 0\n",
    "        self.x_padded = list()\n",
    "\n",
    "        for sentence in self.x_tokenized:\n",
    "            while len(sentence) < self.seq_len:\n",
    "                sentence.insert(len(sentence), pad_idx)\n",
    "\n",
    "            self.x_padded.append(sentence)\n",
    "            \n",
    "        self.x_padded = np.array(self.x_padded) \n",
    "        \n",
    "    def onehot_encode(self):\n",
    "        # Create a onehot encoded representation of the targets\n",
    "        self.y_onehot = list()\n",
    "        y_idx = self.le.inverse_transform(self.y)\n",
    "        \n",
    "        for i in range(len(self.y)):\n",
    "            \n",
    "            tmp = np.zeros(self.n_outputs)\n",
    "        \n",
    "            for k in range(self.n_outputs):\n",
    "                if self.data.Sense_key.iloc[i] == y_idx[i]:\n",
    "                    tmp[self.y[i]] = 1\n",
    "                    \n",
    "            self.y_onehot.append(tmp)\n",
    "            \n",
    "        self.y_onehot = np.array(self.y_onehot)\n",
    "            \n",
    "    def split_data(self):\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = \\\n",
    "        train_test_split(self.x_padded, self.y_onehot, test_size=0.25, random_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Check that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = load_data(train_path)\n",
    "df_short = df[df.Lemma == \"positive.a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_pos = Preprocessing(df_short, num_words = 6000, seq_len = 200)\n",
    "data_pos.load_data()\n",
    "data_pos.build_vocabulary()\n",
    "data_pos.word_to_idx()\n",
    "data_pos.find_seq_len()\n",
    "data_pos.padding_sentences()\n",
    "data_pos.onehot_encode()\n",
    "data_pos.split_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "\n",
    "with open(\"glove.6B.50d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1232   ,  0.36199  ,  0.13819  ,  0.1923   , -0.93796  ,\n",
       "        0.70297  ,  0.57263  ,  0.91297  , -0.69626  , -0.054828 ,\n",
       "        1.2394   , -0.87465  ,  0.91791  , -0.28632  ,  0.71912  ,\n",
       "       -0.23525  ,  0.075219 , -0.14894  ,  0.41694  ,  1.0402   ,\n",
       "       -0.36619  , -1.3843   , -0.06398  ,  0.38334  ,  0.50793  ,\n",
       "       -1.3401   ,  0.81819  , -0.084923 , -0.83609  , -0.68585  ,\n",
       "        1.8433   ,  0.8701   , -0.13934  , -0.2725   , -1.367    ,\n",
       "        0.22925  , -0.041979 ,  0.80299  ,  0.038621 , -0.38195  ,\n",
       "        0.0072519,  0.20913  ,  1.0864   , -2.0325   , -0.46558  ,\n",
       "        0.52418  , -0.40482  ,  0.10702  ,  0.021184 , -1.1139   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dict[\"romania\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2989,\n",
       " 2,\n",
       " 54,\n",
       " 21,\n",
       " 750,\n",
       " 112,\n",
       " 59,\n",
       " 27,\n",
       " 44,\n",
       " 334,\n",
       " 989,\n",
       " 34,\n",
       " 203,\n",
       " 14,\n",
       " 1,\n",
       " 503,\n",
       " 6,\n",
       " 1063,\n",
       " 1064,\n",
       " 2,\n",
       " 335,\n",
       " 1557,\n",
       " 27,\n",
       " 2184,\n",
       " 2,\n",
       " 5048,\n",
       " 4,\n",
       " 1223,\n",
       " 3,\n",
       " 1224,\n",
       " 1429,\n",
       " 2,\n",
       " 81,\n",
       " 5049,\n",
       " 652,\n",
       " 5,\n",
       " 7,\n",
       " 58,\n",
       " 1430,\n",
       " 2,\n",
       " 1,\n",
       " 866,\n",
       " 3,\n",
       " 1,\n",
       " 369,\n",
       " 40,\n",
       " 276,\n",
       " 16,\n",
       " 6,\n",
       " 1225,\n",
       " 569,\n",
       " 9,\n",
       " 334,\n",
       " 17,\n",
       " 484,\n",
       " 17,\n",
       " 225,\n",
       " 199,\n",
       " 17,\n",
       " 2,\n",
       " 6,\n",
       " 717,\n",
       " 6,\n",
       " 258,\n",
       " 12,\n",
       " 1,\n",
       " 867,\n",
       " 30,\n",
       " 2990,\n",
       " 1,\n",
       " 2991,\n",
       " 3,\n",
       " 343,\n",
       " 370,\n",
       " 5,\n",
       " 24,\n",
       " 1,\n",
       " 269,\n",
       " 204,\n",
       " 2,\n",
       " 1,\n",
       " 369,\n",
       " 3715,\n",
       " 6,\n",
       " 1313,\n",
       " 1558,\n",
       " 4,\n",
       " 3716,\n",
       " 2495,\n",
       " 2496,\n",
       " 22,\n",
       " 1719,\n",
       " 3,\n",
       " 239,\n",
       " 11,\n",
       " 318,\n",
       " 718,\n",
       " 8,\n",
       " 2497,\n",
       " 96,\n",
       " 12,\n",
       " 1,\n",
       " 64,\n",
       " 3,\n",
       " 1,\n",
       " 2498,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pos.x_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.42115 ,  0.037718, -0.49961 , -0.48811 ,  0.21755 ,  0.45833 ,\n",
       "        0.84379 , -0.029328, -0.22836 , -0.50353 ,  0.29288 ,  0.88652 ,\n",
       "        0.075803, -0.89269 , -0.077111,  0.64782 ,  0.1009  , -0.2286  ,\n",
       "        1.2372  ,  0.29613 ,  0.49105 ,  0.3105  ,  0.37639 , -0.10743 ,\n",
       "        0.99254 ,  0.53388 ,  0.33742 , -0.72163 ,  0.6276  ,  0.17441 ,\n",
       "       -0.070688,  0.63826 , -0.096589, -0.24625 ,  0.22688 , -0.25842 ,\n",
       "       -0.19026 ,  0.47874 , -0.23385 ,  0.3112  , -0.24794 , -0.64864 ,\n",
       "        0.7182  , -0.46691 , -0.080093, -0.16302 ,  0.078378, -0.48693 ,\n",
       "        0.29361 ,  0.59439 ], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dict[data_pos.x_raw[0][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Embedding the data (not used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Are stopwords able to change the sense of a word? I think so!\n",
    "\n",
    "- standing in line - waiting for something\n",
    "- standing in a line - they're just standing \n",
    "\n",
    "Based on this, I will not remove stopwords. I will also leave in punctuation, but it seems like a good idea to lowercase the entire text. We're not doing NER, and I don't want Line and line to end up having two meanings - the position alone should clarify the sense. CBoW seems like a terrible choice in this setting - the word senses will almost certainly get lost. Try representation with pre-trained GloVe vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "idea: only embed the sentence containing the word in question (maybe later)\n",
    "use word position in an attention model, or for determining weights in a CNN/RNN (think that is an attention model)\n",
    " \n",
    "represent sentence/doc\n",
    "one-hot encode labels\n",
    "\n",
    "prediction: something with a softmax layer\n",
    "\n",
    "CNNs seem promising, as they can model interactions between words (exactly what we want). They also have a local structure, which is great. (can steal code from demo notebook if I want to use this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#train_df = load_data(train_path)\n",
    "#train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#test_df = load_data(test_path)\n",
    "#test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "start out simple! ignore position, see it as a document classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Will onehot encode the sense key. This makes the most sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "want a training accuracy score for each network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Could learn representation as I go, but there's not a lot of examples per unique sense_key, in some cases...\n",
    "Also, this is a pain. Since the WSD texts appear to be generic enough, pretrained GloVe vectors should be ok. \n",
    "Do I use these as an initial guess or what? Also, GloVe only encodes one word at a time - so do I apply a context window myself as well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "for a 50-dim embedding of a 100-word document, we get a $100*50$ matrix. Seems to make sense to run a CNN over this! \n",
    "\n",
    "output layer size should depend on the number of distinct senses for each lemma, so this is a lemma-by-lemma approach\n",
    "\n",
    "or try summing up all the vectors to create one representation for the entire document, then input it into a deep neural net of size 50. however this is silly and a RNN is better, can then have feedback in time if we input one word at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify using a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Parameters:\n",
    "\n",
    "    # Preprocessing parameeters\n",
    "    num_words: int = 8000\n",
    "\n",
    "    # Model parameters\n",
    "    embedding_size: int = 64\n",
    "    out_size: int = 32\n",
    "    stride: int = 2\n",
    "\n",
    "    # Training parameters\n",
    "    epochs: int = 2\n",
    "    batch_size: int = 12\n",
    "    learning_rate: float = 0.001\n",
    "    early_stopping_win = 5\n",
    "        \n",
    "    # Runtime parameters - will be different for each lemma\n",
    "    n_outputs: int = None\n",
    "    seq_len: int = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## TextClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class TextClassifier(nn.ModuleList):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        super(TextClassifier, self).__init__()\n",
    "\n",
    "        # Parameters regarding text preprocessing\n",
    "        self.seq_len = params.seq_len\n",
    "        self.num_words = params.num_words\n",
    "        self.embedding_size = params.embedding_size\n",
    "\n",
    "        # Dropout definition\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        # CNN parameters definition\n",
    "        # Kernel sizes\n",
    "        self.kernel_1 = 2\n",
    "        self.kernel_2 = 3\n",
    "        self.kernel_3 = 4\n",
    "        self.kernel_4 = 5\n",
    "\n",
    "        # Output size for each convolution\n",
    "        self.out_size = params.out_size\n",
    "        # Number of strides for each convolution\n",
    "        self.stride = params.stride\n",
    "\n",
    "        # Embedding layer definition\n",
    "        self.embedding = nn.Embedding(self.num_words + 1, self.embedding_size, padding_idx=0)\n",
    "\n",
    "        # Convolution layers definition\n",
    "        self.conv_1 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_1, self.stride)\n",
    "        self.conv_2 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_2, self.stride)\n",
    "        self.conv_3 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_3, self.stride)\n",
    "        self.conv_4 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_4, self.stride)\n",
    "\n",
    "        # Max pooling layers definition\n",
    "        self.pool_1 = nn.MaxPool1d(self.kernel_1, self.stride)\n",
    "        self.pool_2 = nn.MaxPool1d(self.kernel_2, self.stride)\n",
    "        self.pool_3 = nn.MaxPool1d(self.kernel_3, self.stride)\n",
    "        self.pool_4 = nn.MaxPool1d(self.kernel_4, self.stride)\n",
    "\n",
    "        # Fully connected layer definition\n",
    "        self.fc = nn.Linear(self.in_features_fc(), params.n_outputs)\n",
    "        \n",
    "        # Softmax output layer definition\n",
    "        self.log_softmax = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "    def in_features_fc(self):\n",
    "        '''Calculates the number of output features after Convolution + Max pooling\n",
    "\n",
    "        Convolved_Features = ((embedding_size + (2 * padding) - dilation * (kernel - 1) - 1) / stride) + 1\n",
    "        Pooled_Features = ((embedding_size + (2 * padding) - dilation * (kernel - 1) - 1) / stride) + 1\n",
    "\n",
    "        source: https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "        '''\n",
    "        \n",
    "        # Calculate size of convolved/pooled features for convolution_1/max_pooling_1 features\n",
    "        out_conv_1 = ((self.embedding_size - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n",
    "        out_conv_1 = math.floor(out_conv_1)\n",
    "        out_pool_1 = ((out_conv_1 - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n",
    "        out_pool_1 = math.floor(out_pool_1)\n",
    "\n",
    "        # Calculate size of convolved/pooled features for convolution_2/max_pooling_2 features\n",
    "        out_conv_2 = ((self.embedding_size - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n",
    "        out_conv_2 = math.floor(out_conv_2)\n",
    "        out_pool_2 = ((out_conv_2 - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n",
    "        out_pool_2 = math.floor(out_pool_2)\n",
    "\n",
    "        # Calculate size of convolved/pooled features for convolution_3/max_pooling_3 features\n",
    "        out_conv_3 = ((self.embedding_size - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n",
    "        out_conv_3 = math.floor(out_conv_3)\n",
    "        out_pool_3 = ((out_conv_3 - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n",
    "        out_pool_3 = math.floor(out_pool_3)\n",
    "\n",
    "        # Calculate size of convolved/pooled features for convolution_4/max_pooling_4 features\n",
    "        out_conv_4 = ((self.embedding_size - 1 * (self.kernel_4 - 1) - 1) / self.stride) + 1\n",
    "        out_conv_4 = math.floor(out_conv_4)\n",
    "        out_pool_4 = ((out_conv_4 - 1 * (self.kernel_4 - 1) - 1) / self.stride) + 1\n",
    "        out_pool_4 = math.floor(out_pool_4)\n",
    "\n",
    "        # Returns \"flattened\" vector (input for fully connected layer)\n",
    "        return (out_pool_1 + out_pool_2 + out_pool_3 + out_pool_4) * self.out_size\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Sequence of tokens is filtered through an embedding layer\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Convolution layer 1 is applied\n",
    "        x1 = self.conv_1(x)\n",
    "        x1 = torch.relu(x1)\n",
    "        x1 = self.pool_1(x1)\n",
    "\n",
    "        # Convolution layer 2 is applied\n",
    "        x2 = self.conv_2(x)\n",
    "        x2 = torch.relu((x2))\n",
    "        x2 = self.pool_2(x2)\n",
    "\n",
    "        # Convolution layer 3 is applied\n",
    "        x3 = self.conv_3(x)\n",
    "        x3 = torch.relu(x3)\n",
    "        x3 = self.pool_3(x3)\n",
    "\n",
    "        # Convolution layer 4 is applied\n",
    "        x4 = self.conv_4(x)\n",
    "        x4 = torch.relu(x4)\n",
    "        x4 = self.pool_4(x4)\n",
    "\n",
    "        # The output of each convolutional layer is concatenated into a unique vector\n",
    "        union = torch.cat((x1, x2, x3, x4), 2)\n",
    "        union = union.reshape(union.size(0), -1)\n",
    "        \n",
    "        # The \"flattened\" vector is passed through a fully connected layer\n",
    "        out = self.fc(union)\n",
    "        # Dropout is applied\n",
    "        out = self.dropout(out)\n",
    "        out = self.log_softmax(out)\n",
    "\n",
    "        # Use this, or there's a dim-0 error when a batch contains only one value\n",
    "        if len(out) > 1:\n",
    "            return out.squeeze()\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Learned about the softmax outputs and loss function from here: https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetMapper(Dataset):\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "class Run:\n",
    "    '''Training, evaluation and metrics calculation'''\n",
    "\n",
    "    @staticmethod\n",
    "    def train(model, data, params):\n",
    "\n",
    "        # Initialize dataset maper\n",
    "        train = DatasetMapper(data['x_train'], data['y_train'])\n",
    "        test = DatasetMapper(data['x_test'], data['y_test'])\n",
    "\n",
    "        # Initialize loaders\n",
    "        loader_train = DataLoader(train, batch_size=params.batch_size)\n",
    "        loader_test = DataLoader(test, batch_size=params.batch_size)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        loss_function = nn.NLLLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params.learning_rate)\n",
    "        \n",
    "        # Define vector for early stopping\n",
    "        prev_loss = np.zeros(params.early_stopping_win)\n",
    "\n",
    "        # Starts training phase\n",
    "        for epoch in range(params.epochs):\n",
    "            # Set model in training model\n",
    "            model.train()\n",
    "            predictions = []\n",
    "            # Starts batch training\n",
    "            for x_batch, y_batch in loader_train:\n",
    "\n",
    "                y_batch = y_batch.type(torch.FloatTensor)\n",
    "\n",
    "                # Feed the model\n",
    "                y_pred = model(x_batch.long())\n",
    "                          \n",
    "                # Transform back from onehot encoded targets\n",
    "                y_true = np.zeros(y_batch.shape[0])\n",
    "                \n",
    "                for i in range(y_batch.shape[0]):\n",
    "                    for j in range(y_batch.shape[1]):\n",
    "                        if y_batch[i,j] == 1:\n",
    "                            y_true[i] = j\n",
    "            \n",
    "                y_true = torch.from_numpy(y_true).long()\n",
    "\n",
    "                # Loss calculation\n",
    "                loss = loss_function(y_pred, y_true)\n",
    "\n",
    "                # Clean gradientes\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Gradients calculation\n",
    "                loss.backward()\n",
    "\n",
    "                # Gradients update\n",
    "                optimizer.step()\n",
    "\n",
    "                # Save predictions\n",
    "                predictions += list(y_pred.detach().numpy())\n",
    "                \n",
    "            # Evaluation phase\n",
    "            test_predictions = Run.evaluation(model, loader_test)\n",
    "            \n",
    "            # Metrics calculation\n",
    "            train_accuracy = Run.calculate_accuracy(data['y_train'], predictions)\n",
    "            test_accuracy = Run.calculate_accuracy(data['y_test'], test_predictions)\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                print(\"Epoch: %d, loss: %.4f, Train accuracy: %.4f, Test accuracy: %.4f\" % \\\n",
    "                      (epoch, loss.item(), train_accuracy, test_accuracy))\n",
    "            \n",
    "            # Early stopping check\n",
    "            if epoch > 10:\n",
    "                if loss.item() < min(prev_loss):\n",
    "                    prev_loss = prev_loss[1:]\n",
    "                    prev_loss = np.append(prev_loss, loss.item())\n",
    "                else:\n",
    "                    break\n",
    "                \n",
    "        return train_accuracy, test_accuracy\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluation(model, loader_test):\n",
    "\n",
    "        # Set the model in evaluation mode\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "\n",
    "        # Start evaluation phase\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in loader_test:\n",
    "                y_pred = model(x_batch.long())\n",
    "                predictions += list(y_pred.detach().numpy())\n",
    "        return predictions\n",
    "        \n",
    "    @staticmethod\n",
    "    def calculate_accuracy(grand_truth, predictions):\n",
    "        # Metrics calculation\n",
    "        correct = 0\n",
    "        \n",
    "        for true, pred in zip(grand_truth, predictions):\n",
    "    \n",
    "            for i, element in enumerate(pred):\n",
    "                if element == max(pred) and true[i] == 1:\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "        # Return accuracy\n",
    "        return (correct) / len(grand_truth)\n",
    "    \n",
    "    @staticmethod\n",
    "    def prediction(model, data, le, params):\n",
    "        \n",
    "        # Initialize loader\n",
    "        loader = DataLoader(data, batch_size=params.batch_size, shuffle=False)\n",
    "        \n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                #print(\"In prediction, batch: \", batch.shape)\n",
    "                pred = model(batch.long())\n",
    "                predictions += list(pred.detach().numpy())\n",
    "                \n",
    "        sense_pred = []        \n",
    "        for line in predictions:\n",
    "            for i, val in enumerate(line):\n",
    "                if val == max(line):\n",
    "                    sense_pred.append(i)\n",
    "                    \n",
    "        sense_pred = le.inverse_transform(sense_pred)\n",
    "        \n",
    "        # Return the predicted senses\n",
    "        return sense_pred\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller(Parameters):\n",
    "\n",
    "    def __init__(self, df, validation_df):\n",
    "        \n",
    "        self.lemma = None\n",
    "        self.train_accuracy = None\n",
    "        self.test_accuracy = None\n",
    "        \n",
    "        # Preprocessing pipeline\n",
    "        self.data, lemma, n_outputs, le, vocabulary, seq_len = self.prepare_data(df, Parameters.num_words, Parameters.seq_len)\n",
    "        \n",
    "        self.le = le\n",
    "        self.lemma = lemma\n",
    "        self.vocabulary = vocabulary\n",
    "        Parameters.seq_len = seq_len\n",
    "        Parameters.n_outputs = n_outputs  \n",
    "\n",
    "        # Initialize the model\n",
    "        self.model = TextClassifier(Parameters)\n",
    "\n",
    "        # Training - Evaluation pipeline\n",
    "        train_accuracy, test_accuracy = Run().train(self.model, self.data, Parameters)\n",
    "\n",
    "        # Save accuracies\n",
    "        self.train_accuracy = train_accuracy\n",
    "        self.test_accuracy = test_accuracy\n",
    "        \n",
    "        # Make predictions on valdiation dataset\n",
    "        self.validation_data = self.prepare_validation_data(validation_df, self.vocabulary, Parameters.seq_len)\n",
    "        #self.sense_pred = Run().prediction(self.model, self.validation_data, self.le, Parameters)\n",
    " \n",
    "    @staticmethod\n",
    "    def prepare_data(df, num_words, seq_len):\n",
    "        # Preprocessing pipeline\n",
    "        pr = Preprocessing(df, num_words, seq_len)\n",
    "        pr.load_data()\n",
    "        pr.build_vocabulary()\n",
    "        pr.word_to_idx()\n",
    "        pr.find_seq_len()\n",
    "        pr.padding_sentences()\n",
    "        pr.onehot_encode()\n",
    "        pr.split_data()\n",
    "\n",
    "        return ({'x_train': pr.x_train, 'y_train': pr.y_train, 'x_test': pr.x_test, 'y_test': pr.y_test}, \\\n",
    "                pr.lemma, pr.n_outputs, pr.le, pr.vocabulary, pr.seq_len)\n",
    "   \n",
    "    @staticmethod\n",
    "    def prepare_validation_data(df, vocabulary, seq_len):\n",
    "        \n",
    "        num_words = len(vocabulary)\n",
    "\n",
    "        pr = Preprocessing(test_short, num_words, seq_len)\n",
    "        pr.load_data()\n",
    "        pr.vocabulary = vocabulary\n",
    "        pr.word_to_idx()\n",
    "        pr.seq_len = seq_len\n",
    "        pr.padding_sentences()\n",
    "\n",
    "        return pr.x_padded\n",
    "\n",
    "    # if __name__ == '__main__':\n",
    "    #    controller = Controller(df_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesn't work for 'hold.v', 'common.a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 2.2451, Train accuracy: 0.1587, Test accuracy: 0.1790\n",
      "------------------------------------------------------------\n",
      "Lemma: hold.v, Final training accuracy: 0.4650, Final test accuracy: 0.2417\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_lemma = 'hold.v'\n",
    "\n",
    "df_short = df[df.Lemma == test_lemma]\n",
    "test_short = test_df[test_df.Lemma == test_lemma]\n",
    "controller = Controller(df_short, test_short)\n",
    "\n",
    "print('-'*60)\n",
    "print(\"Lemma: %s, Final training accuracy: %.4f, Final test accuracy: %.4f\" % \\\n",
    "              (controller.lemma, controller.train_accuracy, controller.test_accuracy))\n",
    "print('-'*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'long'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-215-00846c94f1f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msense_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontroller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontroller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontroller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msense_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-201-22ff32b3e015>\u001b[0m in \u001b[0;36mprediction\u001b[1;34m(model, data, le, params)\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m                 \u001b[1;31m#print(\"In prediction, batch: \", batch.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m                 \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m                 \u001b[0mpredictions\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'long'"
     ]
    }
   ],
   "source": [
    "sense_pred = Run().prediction(controller.model, controller.validation_data, controller.le, Parameters)\n",
    "sense_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DatasetMapper at 0x20bd63e3e08>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In prediction, batch:  244\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'long'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-250-a712d0370ae4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"In prediction, batch: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'long'"
     ]
    }
   ],
   "source": [
    "le = controller.le\n",
    "data = controller.validation_data\n",
    "model = controller.model    \n",
    "\n",
    "loader = DataLoader(data, batch_size=Parameters.batch_size, shuffle=False)\n",
    "  \n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in loader:\n",
    "        print(\"In prediction, batch: \", len(batch))\n",
    "        pred = model(batch.long())\n",
    "        predictions += list(pred.detach().numpy())\n",
    "        \n",
    "sense_pred = []        \n",
    "for line in predictions:\n",
    "    for i, val in enumerate(line):\n",
    "        if val == max(line):\n",
    "            sense_pred.append(i)\n",
    "\n",
    "sense_pred = le.inverse_transform(sense_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([244, 12])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if isinstance(batch, list):\n",
    "    tensor_batch = []\n",
    "\n",
    "    for line in batch:\n",
    "        tmp = line.numpy()\n",
    "        tensor_batch.append(tmp)\n",
    "\n",
    "    tensor_batch = torch.from_numpy(np.array(tensor_batch)).long()\n",
    "    \n",
    "tensor_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 244, 2], expected input[244, 12, 64] to have 244 channels, but got 12 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-267-5d4a4348e716>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-c39f9f801418>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;31m# Convolution layer 1 is applied\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m         \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    257\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[0;32m    258\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 259\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 244, 2], expected input[244, 12, 64] to have 244 channels, but got 12 channels instead"
     ]
    }
   ],
   "source": [
    "pred = model(tensor_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sense_key</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>Position</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?</td>\n",
       "      <td>physical.a</td>\n",
       "      <td>58</td>\n",
       "      <td>[iaea, pointed, out, that, training, and, educ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?</td>\n",
       "      <td>see.v</td>\n",
       "      <td>8</td>\n",
       "      <td>[aid, official, development, assistance, (, od...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?</td>\n",
       "      <td>line.n</td>\n",
       "      <td>39</td>\n",
       "      <td>[she, would, appreciate, receiving, informatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>?</td>\n",
       "      <td>keep.v</td>\n",
       "      <td>42</td>\n",
       "      <td>[we, look, forward, to, its, eventual, assessm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>?</td>\n",
       "      <td>national.a</td>\n",
       "      <td>57</td>\n",
       "      <td>[in, his, report, to, the, general, assembly, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sense_key       Lemma Position  \\\n",
       "0         ?  physical.a       58   \n",
       "1         ?       see.v        8   \n",
       "2         ?      line.n       39   \n",
       "3         ?      keep.v       42   \n",
       "4         ?  national.a       57   \n",
       "\n",
       "                                                Text  \n",
       "0  [iaea, pointed, out, that, training, and, educ...  \n",
       "1  [aid, official, development, assistance, (, od...  \n",
       "2  [she, would, appreciate, receiving, informatio...  \n",
       "3  [we, look, forward, to, its, eventual, assessm...  \n",
       "4  [in, his, report, to, the, general, assembly, ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_data(train_path)\n",
    "test_df = load_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hold%2:36:00::    475\n",
       "hold%2:40:00::    442\n",
       "hold%2:31:01::    442\n",
       "hold%2:35:03::    436\n",
       "hold%2:40:04::    431\n",
       "hold%2:32:11::    267\n",
       "hold%2:42:00::    250\n",
       "hold%2:31:10::    191\n",
       "hold%2:40:02::     91\n",
       "hold%2:35:00::     55\n",
       "hold%2:41:15::     46\n",
       "Name: Sense_key, dtype: int64"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.Lemma == test_lemma].Sense_key.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Loop over all lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 2.1964, Train accuracy: 0.2385, Test accuracy: 0.2767\n",
      "------------------------------------------------------------\n",
      "Lemma: find.v, Final training accuracy: 0.2385, Final test accuracy: 0.2767\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 2.0176, Train accuracy: 0.2480, Test accuracy: 0.2351\n",
      "------------------------------------------------------------\n",
      "Lemma: life.n, Final training accuracy: 0.2480, Final test accuracy: 0.2351\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.7722, Train accuracy: 0.2848, Test accuracy: 0.2934\n",
      "------------------------------------------------------------\n",
      "Lemma: order.n, Final training accuracy: 0.2848, Final test accuracy: 0.2934\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 2.1438, Train accuracy: 0.2000, Test accuracy: 0.2131\n",
      "------------------------------------------------------------\n",
      "Lemma: bring.v, Final training accuracy: 0.2000, Final test accuracy: 0.2131\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.4278, Train accuracy: 0.3429, Test accuracy: 0.3601\n",
      "------------------------------------------------------------\n",
      "Lemma: active.a, Final training accuracy: 0.3429, Final test accuracy: 0.3601\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.6334, Train accuracy: 0.2148, Test accuracy: 0.2170\n",
      "------------------------------------------------------------\n",
      "Lemma: extend.v, Final training accuracy: 0.2148, Final test accuracy: 0.2170\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.4816, Train accuracy: 0.2156, Test accuracy: 0.2275\n",
      "------------------------------------------------------------\n",
      "Lemma: case.n, Final training accuracy: 0.2156, Final test accuracy: 0.2275\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 2.0125, Train accuracy: 0.1863, Test accuracy: 0.1677\n",
      "------------------------------------------------------------\n",
      "Lemma: lead.v, Final training accuracy: 0.1863, Final test accuracy: 0.1677\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.5270, Train accuracy: 0.2987, Test accuracy: 0.3189\n",
      "------------------------------------------------------------\n",
      "Lemma: critical.a, Final training accuracy: 0.2987, Final test accuracy: 0.3189\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.6791, Train accuracy: 0.3257, Test accuracy: 0.2785\n",
      "------------------------------------------------------------\n",
      "Lemma: major.a, Final training accuracy: 0.3257, Final test accuracy: 0.2785\n",
      "------------------------------------------------------------\n",
      "Elapsed time:  123.22242569923401\n"
     ]
    }
   ],
   "source": [
    "lemma_vec = []\n",
    "train_accuracy_vec = []\n",
    "test_accuracy_vec = []\n",
    "predicted_df = test_df.copy()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for lemma in df.Lemma.unique()[20:]:\n",
    "    \n",
    "    df_short = df[df.Lemma == lemma]\n",
    "    test_short = test_df[test_df.Lemma == lemma]\n",
    "    controller = Controller(df_short, test_short)\n",
    "    \n",
    "    print('-'*60)\n",
    "    print(\"Lemma: %s, Final training accuracy: %.4f, Final test accuracy: %.4f\" % \\\n",
    "                  (controller.lemma, controller.train_accuracy, controller.test_accuracy))\n",
    "    print('-'*60)\n",
    "    \n",
    "    # Append accuracies for each lemma\n",
    "    lemma_vec.append(controller.lemma)\n",
    "    train_accuracy_vec.append(controller.train_accuracy)\n",
    "    test_accuracy_vec.append(controller.test_accuracy)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = controller.sense_pred\n",
    "    for k, idx in enumerate(test_short.index):\n",
    "        predicted_df.iloc[idx].Sense_key = predictions[k]\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: \", elapsed_time)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
