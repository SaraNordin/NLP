{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep%2:42:07::\tkeep.v\t15\taction by the committee in pursuance of its mandate , the committee will continue to keep under review the situation relating to the question of palestine and participate in relevant meetings of the general assembly and the security council . the committee will also continue to monitor the situation on the ground and draw the attention of the international community to urgent developments in the occupied palestinian territory , including east jerusalem , requiring international action .\n",
      "\n",
      "?\tphysical.a\t58\tiaea pointed out that training and education were fundamental to the agency 's approach to enhancing physical protection systems in states . training courses , workshops and seminars that had been held on six continents had raised awareness and had provided hands-on experience of various subjects including the physical protection of research facilities , the practical operation of physical protection systems , and the engineering safety aspects of physical protection managing situations involving malevolent acts . in the area of physical protection 60 courses had been conducted in the past three years .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_path = \"a3_data/wsd_train.txt\"\n",
    "test_path = \"a3_data/wsd_test_blind.txt\"\n",
    "\n",
    "with open(train_path, encoding = \"utf-8\") as f:\n",
    "    for d, line in enumerate(f):\n",
    "        print(line.lower())\n",
    "        break\n",
    "        \n",
    "with open(test_path, encoding = \"utf-8\") as f:\n",
    "    for d, line in enumerate(f):\n",
    "        print(line.lower())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the best way to represent a piece of text for this task? CBoW, CNN, RNN, attention, ...?\n",
    "\n",
    "Are stopwords able to change the sense of a word? I think so!\n",
    "\n",
    "- standing in line - waiting for something\n",
    "- standing in a line - they're just standing \n",
    "\n",
    "Based on this, I will not remove stopwords. I will also leave in punctuation, but it seems like a good idea to lowercase the entire text. We're not doing NER, and I don't want Line and line to end up having two meanings - the position alone should clarify the sense.\n",
    "\n",
    "CBoW seems like a terrible choice in this setting - the word senses will almost certainly get lost. \n",
    "Since we know the position of the word, using a context window approach seems interesting. Try representation with GloVe\n",
    "CNNs seem promising, as they can model interactions between words (exactly what we want). They also have a local structure, which is great. (can steal code from demo notebook if I want to use this)\n",
    "\n",
    "Look at lecture before determining whether it's good to use RNN or attention models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the position of the target word, or not? can either include it in the embedding or just shove it all into a NN\n",
    "# already need to use two different NN architectures, so maybe it's best to do a smart embedding straight away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea: only embed the sentence containing the word in question (a bit fiddly, maybe later)\n",
    "\n",
    "# represent sentence/doc\n",
    "# one-hot encode labels\n",
    "\n",
    "# prediction: something with a softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sense_key  keep%2:42:07::\n",
      "lemma  keep.v\n",
      "position  15\n",
      "text  ['action', 'by', 'the', 'committee', 'in', 'pursuance', 'of', 'its', 'mandate', ',', 'the', 'committee', 'will', 'continue', 'to', 'keep', 'under', 'review', 'the', 'situation', 'relating', 'to', 'the', 'question', 'of', 'palestine', 'and', 'participate', 'in', 'relevant', 'meetings', 'of', 'the', 'general', 'assembly', 'and', 'the', 'security', 'council', '.', 'the', 'committee', 'will', 'also', 'continue', 'to', 'monitor', 'the', 'situation', 'on', 'the', 'ground', 'and', 'draw', 'the', 'attention', 'of', 'the', 'international', 'community', 'to', 'urgent', 'developments', 'in', 'the', 'occupied', 'palestinian', 'territory', ',', 'including', 'east', 'jerusalem', ',', 'requiring', 'international', 'action', '.']\n",
      "76048\n"
     ]
    }
   ],
   "source": [
    "sense_list = []\n",
    "lemma_list = []\n",
    "position_list = []\n",
    "text_list = []\n",
    "\n",
    "with open(train_path, encoding = \"utf-8\") as f:\n",
    "    for d, line in enumerate(f):\n",
    "        \n",
    "        line = line.lower()\n",
    "        \n",
    "        ix = line.find(\"\\t\")\n",
    "        sense_key = line[0:ix]\n",
    "        line = line[ix+1:]\n",
    "        \n",
    "        ix = line.find(\"\\t\")\n",
    "        lemma = line[0:ix]\n",
    "        line = line[ix+1:]\n",
    "        \n",
    "        ix = line.find(\"\\t\")\n",
    "        position = line[0:ix]\n",
    "        text = line[ix+1:].split()\n",
    "        \n",
    "        if d == 0:\n",
    "            print(\"sense_key \", sense_key)\n",
    "            print(\"lemma \", lemma)\n",
    "            print(\"position \", position)\n",
    "            print(\"text \", text)\n",
    "         \n",
    "        sense_list.append(sense_key)\n",
    "        lemma_list.append(lemma)\n",
    "        position_list.append(position)\n",
    "        text_list.append(text)\n",
    "        \n",
    "        #if d == 10000:\n",
    "        #    break\n",
    "        \n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sense_list, columns = [\"Sense_key\"])\n",
    "df[\"Lemma\"] = lemma_list\n",
    "df[\"Position\"] = position_list\n",
    "df[\"Text\"] = text_list\n",
    "\n",
    "del sense_list, lemma_list, position_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sense_key</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>Position</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>keep%2:42:07::</td>\n",
       "      <td>keep.v</td>\n",
       "      <td>15</td>\n",
       "      <td>[action, by, the, committee, in, pursuance, of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>national%3:01:00::</td>\n",
       "      <td>national.a</td>\n",
       "      <td>25</td>\n",
       "      <td>[a, guard, of, honour, stood, in, formation, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>build%2:31:03::</td>\n",
       "      <td>build.v</td>\n",
       "      <td>38</td>\n",
       "      <td>[the, principle, that, statistics, should, be,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>place%1:04:00::</td>\n",
       "      <td>place.n</td>\n",
       "      <td>36</td>\n",
       "      <td>[again, ,, he, appealed, for, additional, supp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>position%1:04:01::</td>\n",
       "      <td>position.n</td>\n",
       "      <td>76</td>\n",
       "      <td>[also, ,, the, iaea, has, the, lowest, number,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Sense_key       Lemma Position  \\\n",
       "0      keep%2:42:07::      keep.v       15   \n",
       "1  national%3:01:00::  national.a       25   \n",
       "2     build%2:31:03::     build.v       38   \n",
       "3     place%1:04:00::     place.n       36   \n",
       "4  position%1:04:01::  position.n       76   \n",
       "\n",
       "                                                Text  \n",
       "0  [action, by, the, committee, in, pursuance, of...  \n",
       "1  [a, guard, of, honour, stood, in, formation, i...  \n",
       "2  [the, principle, that, statistics, should, be,...  \n",
       "3  [again, ,, he, appealed, for, additional, supp...  \n",
       "4  [also, ,, the, iaea, has, the, lowest, number,...  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['action',\n",
       " 'by',\n",
       " 'the',\n",
       " 'committee',\n",
       " 'in',\n",
       " 'pursuance',\n",
       " 'of',\n",
       " 'its',\n",
       " 'mandate',\n",
       " ',',\n",
       " 'the',\n",
       " 'committee',\n",
       " 'will',\n",
       " 'continue',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'under',\n",
       " 'review',\n",
       " 'the',\n",
       " 'situation',\n",
       " 'relating',\n",
       " 'to',\n",
       " 'the',\n",
       " 'question',\n",
       " 'of',\n",
       " 'palestine',\n",
       " 'and',\n",
       " 'participate',\n",
       " 'in',\n",
       " 'relevant',\n",
       " 'meetings',\n",
       " 'of',\n",
       " 'the',\n",
       " 'general',\n",
       " 'assembly',\n",
       " 'and',\n",
       " 'the',\n",
       " 'security',\n",
       " 'council',\n",
       " '.',\n",
       " 'the',\n",
       " 'committee',\n",
       " 'will',\n",
       " 'also',\n",
       " 'continue',\n",
       " 'to',\n",
       " 'monitor',\n",
       " 'the',\n",
       " 'situation',\n",
       " 'on',\n",
       " 'the',\n",
       " 'ground',\n",
       " 'and',\n",
       " 'draw',\n",
       " 'the',\n",
       " 'attention',\n",
       " 'of',\n",
       " 'the',\n",
       " 'international',\n",
       " 'community',\n",
       " 'to',\n",
       " 'urgent',\n",
       " 'developments',\n",
       " 'in',\n",
       " 'the',\n",
       " 'occupied',\n",
       " 'palestinian',\n",
       " 'territory',\n",
       " ',',\n",
       " 'including',\n",
       " 'east',\n",
       " 'jerusalem',\n",
       " ',',\n",
       " 'requiring',\n",
       " 'international',\n",
       " 'action',\n",
       " '.']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start out simple! ignore position, see it as a document classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def count_word_frequencies(YOUR_FILE, ENCODING):\n",
    "    \n",
    "    freqs = Counter()\n",
    "    with open(YOUR_FILE, encoding = ENCODING) as f:\n",
    "        for line in f:\n",
    "            tokens = line.lower().split()\n",
    "            for token in tokens:\n",
    "                freqs[token] += 1\n",
    "                \n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_int(docs:list()) -> (list(), int, Counter()):\n",
    "    \n",
    "    '''\n",
    "    Function from assignment 2.\n",
    "    \n",
    "    Create bag of words from cleaned and smaller corpus.\n",
    "    Associate each word in bag with an unique integer,\n",
    "    ranging from 0 (most common word) to length of bag of words.\n",
    "    Map each token in docs to the respective int. Return this list of list of ints.\n",
    "    '''\n",
    "    \n",
    "    freqs = Counter()\n",
    "    for doc in docs:\n",
    "        for token in doc:\n",
    "            freqs[token] += 1\n",
    "    most_common = freqs.most_common()\n",
    "    \n",
    "    token_to_int = []\n",
    "    for i in range(len(most_common)):\n",
    "        token_to_int.append(most_common[i][0])\n",
    "\n",
    "    # Get pairs of elements    \n",
    "    mapping = zip(token_to_int, range(0,len(token_to_int)))\n",
    "    \n",
    "    # Make pairs into a dictionary\n",
    "    vocab = dict(mapping)\n",
    "        \n",
    "    # Match token to int\n",
    "    docs_int = list()\n",
    "    \n",
    "    for doc in docs:\n",
    "        docs_int.append(list([vocab.get(x) for x in doc]))\n",
    "        \n",
    "    return docs_int, len(vocab), vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_int, vocab_size, vocab = map_to_int(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_int[0][15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[\"keep\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-8d43e4fa5af4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Text_int\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtexts_int\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   2936\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2937\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2938\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2940\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   2998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2999\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3000\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3001\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3002\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[1;34m(self, key, value, broadcast)\u001b[0m\n\u001b[0;32m   3634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3635\u001b[0m             \u001b[1;31m# turn me into an ndarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3636\u001b[1;33m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3637\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3638\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[1;34m(data, index, copy)\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Length of values does not match length of index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "df[\"Text_int\"] = texts_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1155\n",
      "205\n",
      "425\n",
      "33\n",
      "436\n",
      "442\n",
      "429\n",
      "442\n",
      "4720\n",
      "4103\n",
      "529\n",
      "490\n",
      "448\n",
      "281\n",
      "436\n",
      "429\n",
      "420\n",
      "218\n",
      "448\n",
      "278\n",
      "435\n",
      "437\n",
      "425\n",
      "425\n",
      "425\n",
      "292\n",
      "50\n",
      "170\n",
      "437\n",
      "38\n",
      "427\n",
      "680\n",
      "28\n",
      "556\n",
      "433\n",
      "481\n",
      "426\n",
      "32\n",
      "440\n",
      "429\n",
      "456\n",
      "440\n",
      "528\n",
      "198\n",
      "2114\n",
      "177\n",
      "427\n",
      "115\n",
      "442\n",
      "815\n",
      "436\n",
      "554\n",
      "117\n",
      "440\n",
      "429\n",
      "267\n",
      "426\n",
      "428\n",
      "217\n",
      "475\n",
      "858\n",
      "323\n",
      "441\n",
      "456\n",
      "426\n",
      "414\n",
      "435\n",
      "426\n",
      "480\n",
      "430\n",
      "250\n",
      "1052\n",
      "436\n",
      "428\n",
      "426\n",
      "703\n",
      "437\n",
      "427\n",
      "177\n",
      "433\n",
      "458\n",
      "32\n",
      "454\n",
      "437\n",
      "469\n",
      "448\n",
      "433\n",
      "228\n",
      "192\n",
      "442\n",
      "425\n",
      "410\n",
      "96\n",
      "431\n",
      "456\n",
      "445\n",
      "425\n",
      "448\n",
      "430\n",
      "91\n",
      "80\n",
      "429\n",
      "591\n",
      "210\n",
      "430\n",
      "473\n",
      "425\n",
      "426\n",
      "451\n",
      "470\n",
      "121\n",
      "442\n",
      "379\n",
      "457\n",
      "255\n",
      "148\n",
      "429\n",
      "418\n",
      "436\n",
      "428\n",
      "439\n",
      "428\n",
      "426\n",
      "242\n",
      "326\n",
      "431\n",
      "442\n",
      "425\n",
      "425\n",
      "199\n",
      "441\n",
      "279\n",
      "129\n",
      "440\n",
      "428\n",
      "430\n",
      "191\n",
      "382\n",
      "180\n",
      "240\n",
      "430\n",
      "24\n",
      "431\n",
      "431\n",
      "44\n",
      "179\n",
      "252\n",
      "470\n",
      "80\n",
      "442\n",
      "112\n",
      "426\n",
      "196\n",
      "448\n",
      "259\n",
      "80\n",
      "430\n",
      "91\n",
      "48\n",
      "120\n",
      "430\n",
      "472\n",
      "430\n",
      "136\n",
      "439\n",
      "55\n",
      "89\n",
      "208\n",
      "130\n",
      "51\n",
      "48\n",
      "426\n",
      "40\n",
      "191\n",
      "124\n",
      "25\n",
      "63\n",
      "45\n",
      "147\n",
      "30\n",
      "427\n",
      "481\n",
      "34\n",
      "108\n",
      "122\n",
      "51\n",
      "55\n",
      "24\n",
      "87\n",
      "68\n",
      "430\n",
      "129\n",
      "131\n",
      "69\n",
      "26\n",
      "69\n",
      "23\n",
      "53\n",
      "92\n",
      "38\n",
      "43\n",
      "33\n",
      "92\n",
      "31\n",
      "58\n",
      "22\n",
      "55\n",
      "57\n",
      "51\n",
      "34\n",
      "63\n",
      "37\n",
      "44\n",
      "22\n",
      "29\n",
      "36\n",
      "46\n",
      "29\n",
      "30\n",
      "41\n",
      "22\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "for sense in df.Sense_key.unique():\n",
    "    print(len(df[df.Sense_key == sense]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do I have to learn my own representation? Or can GloVe solve everything?\n",
    "I don't control the context windows but maybe they've already solved this problem better than I can..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could learn representation as I go, but there's not a lot of examples per unique sense_key, in some cases...\n",
    "Also, this is a pain. Since the WSD texts appear to be generic enough, pretrained GloVe vectors should be ok. \n",
    "Do I use these as an initial guess or what? Also, GloVe only encodes one word at a time - so do I apply a context window myself as well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for a 50-dim embedding of a 100-word document, we get a $100*50$ matrix. Seems to make sense to run a CNN over this! \n",
    "If the key word is on position 30, maybe have higher weights (gaussian?) for words close to it, so words 20-40 are weighted higher. \n",
    "\n",
    "output layer size should depend on the number of distinct senses for each lemma, so this is a lemma-by-lemma approach\n",
    "\n",
    "or try summing up all the vectors to create one representation for the entire document, then input it into a "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
