{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word sense disambiguation - Sara Nordin HÃ¤llgren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions on how to run this notebook: change the filepaths under __Load the data__ if needed. I have referenced GloVe but ended up not using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports - should just be standard packages\n",
    "\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep%2:42:07::\tkeep.v\t15\taction by the committee in pursuance of its mandate , the committee will continue to keep under review the situation relating to the question of palestine and participate in relevant meetings of the general assembly and the security council . the committee will also continue to monitor the situation on the ground and draw the attention of the international community to urgent developments in the occupied palestinian territory , including east jerusalem , requiring international action .\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'a3_data/wsd_test.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-102214e78736>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'a3_data/wsd_test.txt'"
     ]
    }
   ],
   "source": [
    "train_path = \"a3_data/wsd_train.txt\"\n",
    "test_path = \"a3_data/wsd_test_blind.txt\"\n",
    "validation_path = \"a3_data/wsd_test.txt\"\n",
    "\n",
    "with open(train_path, encoding = \"utf-8\") as f:\n",
    "    for d, line in enumerate(f):\n",
    "        print(line.lower())\n",
    "        break\n",
    "        \n",
    "with open(validation_path, encoding = \"utf-8\") as f:\n",
    "    for d, line in enumerate(f):\n",
    "        print(line.lower())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \n",
    "    sense_list = []\n",
    "    lemma_list = []\n",
    "    position_list = []\n",
    "    text_list = []\n",
    "\n",
    "    with open(file_path, encoding = \"utf-8\") as f:\n",
    "        \n",
    "        for d, line in enumerate(f):\n",
    "\n",
    "            line = line.lower()\n",
    "\n",
    "            # Extract sense key\n",
    "            ix = line.find(\"\\t\")\n",
    "            sense_key = line[0:ix]\n",
    "            line = line[ix+1:]\n",
    "\n",
    "            # Extract lemma\n",
    "            ix = line.find(\"\\t\")\n",
    "            lemma = line[0:ix]\n",
    "            line = line[ix+1:]\n",
    "\n",
    "            # Extract position\n",
    "            ix = line.find(\"\\t\")\n",
    "            position = line[0:ix]\n",
    "            text = line[ix+1:].split()\n",
    "\n",
    "            # Extract text\n",
    "            sense_list.append(sense_key)\n",
    "            lemma_list.append(lemma)\n",
    "            position_list.append(position)\n",
    "            text_list.append(text)\n",
    "    \n",
    "    # Convert to df\n",
    "    df = pd.DataFrame(sense_list, columns = [\"Sense_key\"])\n",
    "    df[\"Lemma\"] = lemma_list\n",
    "    df[\"Position\"] = position_list\n",
    "    df[\"Text\"] = text_list\n",
    "\n",
    "    del sense_list, lemma_list, position_list\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the given WordNet data, the task is to implement WSD as a classification task. This is implemented with two different kinds of neural network: a (word-based) convolutional neural network and and a regular deep network. They are discussed more in detail below; first I discuss some general choices:\n",
    "\n",
    "__Design choices__\n",
    "\n",
    "First of all, it made sense to convert each text to lower case to avoid having the same word appear twice in the vocabulary. However I was debating whether to remove stopwords like we did in assignment 2. In the end, I decided to keep the stopwords since they are able to change the sense of a word. An example is:\n",
    "\n",
    "- Standing in line (waiting for something specific)\n",
    "- Standing in a line (they are just standing)\n",
    "\n",
    "The second decision was on how to represent each line in the documents numerically. A CBoW approach would be a good fit for assigning topics to documents but seems like a bad choice in this setting - the word senses will almost certainly get lost. I have used the nn.Embedding from PyTorch, and also created the infrastructure for using the GloVe embedding. One could learn representations as they go, but there's not a lot of examples per unique sense_key, in some cases. Since the WSD texts appear to be generic enough, pretrained embedding vectors should be good enough.  \n",
    "\n",
    "Note that the longest line in any of the documents contains 284 words, but that many are shorter. In the CNN case it made sense to zero pad the data, so that each document is represented by a $284 \\times 64$ matrix (where 64 is the embedding dimension). CNNs used in image analysis settings are able to handle pictures of similar dimensions easily. Since I had to train one CNN for each unique lemma, 30 in total, it was possible to adapt the input dimensions by only using the longest line for each lemma in the dataset. This was found to slightly speed up computations but it does not appear worth the trouble here - however this would be a good approach if some documents are much longer than others. \n",
    "\n",
    "Since we have one key word per document and we know its position, it seems like a shame not to use the actual position of the word. Also, since the deep network does not accept a 2D input each document needs to be flattened. Flattening a $284 \\times 64$ matrix leads to a very long vector, which leads to an extremely large number of nodes in the network. Because of this I instead used a window of 10 words on each side of the lemma. \n",
    "\n",
    "Lastly, the output dimension of both the CNN and DNN are determined by the number of possible senses for each lemma. I have onehot encoded the sense labels and filtered the network output through a log softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify using a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNs seemed promising for this kind of task, as they can model interactions between words (exactly what we want). In this case, we ignore the position of the lemma and see this as a document classification problem. The main structure of my CNN code is based on a text classification tutorial I found on GitHub, where the goal is to distinguish between two different languages:\n",
    "\n",
    "- https://github.com/FernandoLpz/Text-Classification-CNN-PyTorch\n",
    "\n",
    "I also saw an example of how to utilize the log softmax outputs with a suitable loss function in this tutorial:\n",
    "\n",
    "- https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html\n",
    "\n",
    "The idea with a network built from feature maps is to learn local features. In image analysis, this could be an edge, a corner or a leopard spot. The same kernel is applied to different parts of the data, with the same weights and thresholds. In the WSD setting, one hopes that it will be able to pick up on the patterns different words appear in, and learn to predict a sense key based on this. \n",
    "\n",
    "My CNN has one convolution layer consisting of 4 feature maps with different receptive field sizes (2, 3, 4, and 5). They all have stride 2, a ReLU activation function, and a max pooling layer. They all apply a one-dimensional convolution: the entire embedding for each word is always read in, but they focus on a different set of words at a time. The max pooling outputs are concatenated and fed through a fully connected layer. Finally, 25% dropout is applied, after which the output is passed through a log-softmax layer that works well with the NLLLoss function - Negative Log Likelihood.\n",
    "\n",
    "I have implemented early stopping, so that training will abort if the loss has not decreased for 5 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self, df, num_words, seq_len):\n",
    "        self.data = df\n",
    "        self.num_words = num_words\n",
    "        self.seq_len = seq_len  \n",
    "        \n",
    "        self.vocabulary = None\n",
    "        self.x_tokenized = None\n",
    "        self.x_padded = None\n",
    "        self.x_raw = None\n",
    "        \n",
    "        self.lemma = None\n",
    "        self.n_outputs = None\n",
    "        self.le = None\n",
    "        self.y = None\n",
    "        self.y_onehot = None\n",
    "        \n",
    "        self.x_train = None\n",
    "        self.x_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None  \n",
    "    \n",
    "    def load_data(self):\n",
    "        # split into sentences (x) and sense key (y)\n",
    "        df = self.data\n",
    "        self.x_raw = df.Text.values\n",
    "        self.lemma = df.Lemma.iloc[0]\n",
    "        self.n_outputs = len(df.Sense_key.unique())\n",
    "        \n",
    "        labels = np.asarray(df.Sense_key.values)\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        self.y = le.fit_transform(labels)\n",
    "        self.le = le\n",
    "        \n",
    "    def build_vocabulary(self):\n",
    "        # Builds the vocabulary \n",
    "        self.vocabulary = dict()\n",
    "        fdist = nltk.FreqDist()\n",
    "\n",
    "        for sentence in self.x_raw:\n",
    "            for word in sentence:\n",
    "                fdist[word] += 1\n",
    "\n",
    "        common_words = fdist.most_common(self.num_words)\n",
    "\n",
    "        for idx, word in enumerate(common_words):\n",
    "            self.vocabulary[word[0]] = (idx+1)\n",
    "            \n",
    "    def word_to_idx(self):\n",
    "        # By using the dictionary each token is transformed into its index based representation\n",
    "        self.x_tokenized = list() \n",
    "\n",
    "        for sentence in self.x_raw:\n",
    "            temp_sentence = list()\n",
    "            for word in sentence:\n",
    "                if word in self.vocabulary.keys():\n",
    "                    temp_sentence.append(self.vocabulary[word])\n",
    "            self.x_tokenized.append(temp_sentence)\n",
    "    \n",
    "    def padding_sentences(self):\n",
    "        # Each sentence which does not fulfill the required length is padded with the index 0\n",
    "        pad_idx = 0\n",
    "        self.x_padded = list()\n",
    "\n",
    "        for sentence in self.x_tokenized:\n",
    "            while len(sentence) < self.seq_len:\n",
    "                sentence.insert(len(sentence), pad_idx)\n",
    "\n",
    "            self.x_padded.append(sentence)\n",
    "            \n",
    "        self.x_padded = np.array(self.x_padded) \n",
    "        \n",
    "    def onehot_encode(self):\n",
    "        # Create a onehot encoded representation of the targets\n",
    "        self.y_onehot = list()\n",
    "        y_idx = self.le.inverse_transform(self.y)\n",
    "        \n",
    "        for i in range(len(self.y)):\n",
    "            \n",
    "            tmp = np.zeros(self.n_outputs)\n",
    "        \n",
    "            for k in range(self.n_outputs):\n",
    "                if self.data.Sense_key.iloc[i] == y_idx[i]:\n",
    "                    tmp[self.y[i]] = 1\n",
    "                    \n",
    "            self.y_onehot.append(tmp)\n",
    "            \n",
    "        self.y_onehot = np.array(self.y_onehot)\n",
    "            \n",
    "    def split_data(self):\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = \\\n",
    "        train_test_split(self.x_padded, self.y_onehot, test_size=0.25, random_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Parameters:\n",
    "\n",
    "    # Preprocessing parameters\n",
    "    num_words: int = 8000\n",
    "    seq_len = 284 \n",
    "\n",
    "    # Model parameters\n",
    "    embedding_size: int = 64\n",
    "    out_size: int = 32\n",
    "    stride: int = 2\n",
    "\n",
    "    # Training parameters\n",
    "    epochs: int = 100\n",
    "    batch_size: int = 12\n",
    "    learning_rate: float = 0.001\n",
    "    early_stopping_win = 5\n",
    "        \n",
    "    # Runtime parameters - will be different for each lemma\n",
    "    n_outputs: int = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## TextClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class TextClassifier(nn.ModuleList):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        super(TextClassifier, self).__init__()\n",
    "\n",
    "        # Parameters regarding text preprocessing\n",
    "        self.seq_len = params.seq_len\n",
    "        self.num_words = params.num_words\n",
    "        self.embedding_size = params.embedding_size\n",
    "\n",
    "        # Dropout definition\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        # CNN parameters definition\n",
    "        # Kernel sizes\n",
    "        self.kernel_1 = 2\n",
    "        self.kernel_2 = 3\n",
    "        self.kernel_3 = 4\n",
    "        self.kernel_4 = 5\n",
    "\n",
    "        # Output size for each convolution\n",
    "        self.out_size = params.out_size\n",
    "        # Number of strides for each convolution\n",
    "        self.stride = params.stride\n",
    "\n",
    "        # Embedding layer definition\n",
    "        self.embedding = nn.Embedding(self.num_words + 1, self.embedding_size, padding_idx=0)\n",
    "\n",
    "        # Convolution layers definition\n",
    "        self.conv_1 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_1, self.stride)\n",
    "        self.conv_2 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_2, self.stride)\n",
    "        self.conv_3 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_3, self.stride)\n",
    "        self.conv_4 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_4, self.stride)\n",
    "\n",
    "        # Max pooling layers definition\n",
    "        self.pool_1 = nn.MaxPool1d(self.kernel_1, self.stride)\n",
    "        self.pool_2 = nn.MaxPool1d(self.kernel_2, self.stride)\n",
    "        self.pool_3 = nn.MaxPool1d(self.kernel_3, self.stride)\n",
    "        self.pool_4 = nn.MaxPool1d(self.kernel_4, self.stride)\n",
    "\n",
    "        # Fully connected layer definition\n",
    "        self.fc = nn.Linear(self.in_features_fc(), params.n_outputs)\n",
    "        \n",
    "        # Softmax output layer definition\n",
    "        self.log_softmax = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "    def in_features_fc(self):\n",
    "        '''Calculates the number of output features after Convolution + Max pooling\n",
    "\n",
    "        Convolved_Features = ((embedding_size + (2 * padding) - dilation * (kernel - 1) - 1) / stride) + 1\n",
    "        Pooled_Features = ((embedding_size + (2 * padding) - dilation * (kernel - 1) - 1) / stride) + 1\n",
    "\n",
    "        source: https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "        '''\n",
    "        \n",
    "        # Calculate size of convolved/pooled features for convolution_1/max_pooling_1 features\n",
    "        out_conv_1 = ((self.embedding_size - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n",
    "        out_conv_1 = math.floor(out_conv_1)\n",
    "        out_pool_1 = ((out_conv_1 - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n",
    "        out_pool_1 = math.floor(out_pool_1)\n",
    "\n",
    "        # Calculate size of convolved/pooled features for convolution_2/max_pooling_2 features\n",
    "        out_conv_2 = ((self.embedding_size - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n",
    "        out_conv_2 = math.floor(out_conv_2)\n",
    "        out_pool_2 = ((out_conv_2 - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n",
    "        out_pool_2 = math.floor(out_pool_2)\n",
    "\n",
    "        # Calculate size of convolved/pooled features for convolution_3/max_pooling_3 features\n",
    "        out_conv_3 = ((self.embedding_size - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n",
    "        out_conv_3 = math.floor(out_conv_3)\n",
    "        out_pool_3 = ((out_conv_3 - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n",
    "        out_pool_3 = math.floor(out_pool_3)\n",
    "\n",
    "        # Calculate size of convolved/pooled features for convolution_4/max_pooling_4 features\n",
    "        out_conv_4 = ((self.embedding_size - 1 * (self.kernel_4 - 1) - 1) / self.stride) + 1\n",
    "        out_conv_4 = math.floor(out_conv_4)\n",
    "        out_pool_4 = ((out_conv_4 - 1 * (self.kernel_4 - 1) - 1) / self.stride) + 1\n",
    "        out_pool_4 = math.floor(out_pool_4)\n",
    "\n",
    "        # Returns \"flattened\" vector (input for fully connected layer)\n",
    "        return (out_pool_1 + out_pool_2 + out_pool_3 + out_pool_4) * self.out_size\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Sequence of tokens is filtered through an embedding layer\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Convolution layer 1 is applied\n",
    "        x1 = self.conv_1(x)\n",
    "        x1 = torch.relu(x1)\n",
    "        x1 = self.pool_1(x1)\n",
    "\n",
    "        # Convolution layer 2 is applied\n",
    "        x2 = self.conv_2(x)\n",
    "        x2 = torch.relu((x2))\n",
    "        x2 = self.pool_2(x2)\n",
    "\n",
    "        # Convolution layer 3 is applied\n",
    "        x3 = self.conv_3(x)\n",
    "        x3 = torch.relu(x3)\n",
    "        x3 = self.pool_3(x3)\n",
    "\n",
    "        # Convolution layer 4 is applied\n",
    "        x4 = self.conv_4(x)\n",
    "        x4 = torch.relu(x4)\n",
    "        x4 = self.pool_4(x4)\n",
    "\n",
    "        # The output of each convolutional layer is concatenated into a unique vector\n",
    "        union = torch.cat((x1, x2, x3, x4), 2)\n",
    "        union = union.reshape(union.size(0), -1)\n",
    "        \n",
    "        # The \"flattened\" vector is passed through a fully connected layer\n",
    "        out = self.fc(union)\n",
    "        # Dropout is applied\n",
    "        out = self.dropout(out)\n",
    "        # Log softmax is applied\n",
    "        out = self.log_softmax(out)\n",
    "\n",
    "        # Use this, or there's a dim-0 error when a batch contains only one value\n",
    "        if len(out) > 1:\n",
    "            return out.squeeze()\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DatasetMapper(Dataset):\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "class Run:\n",
    "    '''Training, evaluation and metrics calculation'''\n",
    "\n",
    "    @staticmethod\n",
    "    def train(model, data, params):\n",
    "\n",
    "        # Initialize dataset maper\n",
    "        train = DatasetMapper(data['x_train'], data['y_train'])\n",
    "        test = DatasetMapper(data['x_test'], data['y_test'])\n",
    "\n",
    "        # Initialize loaders\n",
    "        loader_train = DataLoader(train, batch_size=params.batch_size)\n",
    "        loader_test = DataLoader(test, batch_size=params.batch_size)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        loss_function = nn.NLLLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params.learning_rate)\n",
    "        \n",
    "        # Define vector for early stopping\n",
    "        prev_loss = np.zeros(params.early_stopping_win)\n",
    "\n",
    "        # Starts training phase\n",
    "        for epoch in range(params.epochs):\n",
    "            # Set model in training model\n",
    "            model.train()\n",
    "            predictions = []\n",
    "            # Starts batch training\n",
    "            for x_batch, y_batch in loader_train:\n",
    "\n",
    "                y_batch = y_batch.type(torch.FloatTensor)\n",
    "\n",
    "                # Feed the model\n",
    "                y_pred = model(x_batch.long())\n",
    "                                          \n",
    "                # Transform back from onehot encoded targets\n",
    "                y_true = np.zeros(y_batch.shape[0])\n",
    "                \n",
    "                for i in range(y_batch.shape[0]):\n",
    "                    for j in range(y_batch.shape[1]):\n",
    "                        if y_batch[i,j] == 1:\n",
    "                            y_true[i] = j\n",
    "            \n",
    "                y_true = torch.from_numpy(y_true).long()\n",
    "\n",
    "                # Loss calculation\n",
    "                loss = loss_function(y_pred, y_true)\n",
    "\n",
    "                # Clean gradientes\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Gradients calculation\n",
    "                loss.backward()\n",
    "\n",
    "                # Gradients update\n",
    "                optimizer.step()\n",
    "\n",
    "                # Save predictions\n",
    "                predictions += list(y_pred.detach().numpy())\n",
    "                \n",
    "            # Evaluation phase\n",
    "            test_predictions = Run.evaluation(model, loader_test)\n",
    "            \n",
    "            # Metrics calculation\n",
    "            train_accuracy = Run.calculate_accuracy(data['y_train'], predictions)\n",
    "            test_accuracy = Run.calculate_accuracy(data['y_test'], test_predictions)\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                print(\"Epoch: %d, loss: %.4f, Train accuracy: %.4f, Test accuracy: %.4f\" % \\\n",
    "                      (epoch, loss.item(), train_accuracy, test_accuracy))\n",
    "            \n",
    "            # Early stopping check\n",
    "            if epoch > 10:\n",
    "                if loss.item() < min(prev_loss):\n",
    "                    prev_loss = prev_loss[1:]\n",
    "                    prev_loss = np.append(prev_loss, loss.item())\n",
    "                else:\n",
    "                    break\n",
    "                \n",
    "        return train_accuracy, test_accuracy\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluation(model, loader_test):\n",
    "\n",
    "        # Set the model in evaluation mode\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "\n",
    "        # Start evaluation phase\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in loader_test:\n",
    "                y_pred = model(x_batch.long())\n",
    "                predictions += list(y_pred.detach().numpy())\n",
    "        return predictions\n",
    "        \n",
    "    @staticmethod\n",
    "    def calculate_accuracy(grand_truth, predictions):\n",
    "        # Metrics calculation\n",
    "        correct = 0\n",
    "        \n",
    "        for true, pred in zip(grand_truth, predictions):\n",
    "    \n",
    "            for i, element in enumerate(pred):\n",
    "                if element == max(pred) and true[i] == 1:\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "        # Return accuracy\n",
    "        return (correct) / len(grand_truth)\n",
    "    \n",
    "    @staticmethod\n",
    "    def prediction(model, data, le, params):\n",
    "        \n",
    "        # Initialize loader\n",
    "        loader = DataLoader(data, batch_size=Parameters.batch_size, shuffle=False)\n",
    "        \n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x_batch in loader:\n",
    "                pred = model(x_batch.long())\n",
    "                predictions += list(pred.detach().numpy())\n",
    "                \n",
    "        sense_pred = []        \n",
    "        for line in predictions:\n",
    "            for i, val in enumerate(line):\n",
    "                if val == max(line):\n",
    "                    sense_pred.append(i)\n",
    "                    \n",
    "        sense_pred = le.inverse_transform(sense_pred)\n",
    "        \n",
    "        # Return the predicted senses\n",
    "        return sense_pred\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Controller(Parameters):\n",
    "\n",
    "    def __init__(self, df, validation_df):\n",
    "        \n",
    "        self.lemma = None\n",
    "        self.train_accuracy = None\n",
    "        self.test_accuracy = None\n",
    "        self.sense_pred = None\n",
    "        \n",
    "        # Preprocessing pipeline\n",
    "        self.data, lemma, n_outputs, le, vocabulary = self.prepare_data(df, Parameters.num_words, Parameters.seq_len)\n",
    "        \n",
    "        self.le = le\n",
    "        self.lemma = lemma\n",
    "        self.vocabulary = vocabulary\n",
    "        Parameters.n_outputs = n_outputs  \n",
    "\n",
    "        # Initialize the model\n",
    "        self.model = TextClassifier(Parameters)\n",
    "\n",
    "        # Training - Evaluation pipeline\n",
    "        train_accuracy, test_accuracy = Run().train(self.model, self.data, Parameters)\n",
    "\n",
    "        # Save accuracies\n",
    "        self.train_accuracy = train_accuracy\n",
    "        self.test_accuracy = test_accuracy\n",
    "        \n",
    "        # Make predictions on valdiation dataset\n",
    "        self.validation_data = self.prepare_validation_data(validation_df, self.vocabulary, Parameters.seq_len)\n",
    "        self.sense_pred = Run().prediction(self.model, self.validation_data, self.le, Parameters)\n",
    " \n",
    "    @staticmethod\n",
    "    def prepare_data(df, num_words, seq_len):\n",
    "        \n",
    "        # Preprocessing pipeline\n",
    "        pr = Preprocessing(df, num_words, seq_len)\n",
    "        pr.load_data()\n",
    "        pr.build_vocabulary()\n",
    "        pr.word_to_idx()\n",
    "        pr.padding_sentences()\n",
    "        pr.onehot_encode()\n",
    "        pr.split_data()\n",
    "\n",
    "        return ({'x_train': pr.x_train, 'y_train': pr.y_train, 'x_test': pr.x_test, 'y_test': pr.y_test}, \\\n",
    "                pr.lemma, pr.n_outputs, pr.le, pr.vocabulary)\n",
    "   \n",
    "    @staticmethod\n",
    "    def prepare_validation_data(df, vocabulary, seq_len):\n",
    "        \n",
    "        num_words = len(vocabulary)\n",
    "\n",
    "        pr = Preprocessing(test_short, num_words, seq_len)\n",
    "        pr.vocabulary = vocabulary\n",
    "        pr.seq_len = seq_len\n",
    "        pr.load_data()\n",
    "        pr.word_to_idx()\n",
    "        pr.padding_sentences()\n",
    "\n",
    "        return pr.x_padded\n",
    "\n",
    "    # if __name__ == '__main__':\n",
    "    #    controller = Controller(df_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'a3_test/wsd_test.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-734da400ae89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mvalidation_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-6ff6aaedacba>\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtext_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'a3_test/wsd_test.txt'"
     ]
    }
   ],
   "source": [
    "df = load_data(train_path)\n",
    "test_df = load_data(test_path)\n",
    "validation_df = load_data(validation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 2.0270, Train accuracy: 0.3838, Test accuracy: 0.3981\n",
      "Epoch: 5, loss: 0.1496, Train accuracy: 0.9723, Test accuracy: 0.5360\n",
      "Epoch: 10, loss: 0.5671, Train accuracy: 0.9931, Test accuracy: 0.5478\n",
      "------------------------------------------------------------\n",
      "Lemma: keep.v, Final training accuracy: 0.9948, Final test accuracy: 0.5411\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 0.8269, Train accuracy: 0.2405, Test accuracy: 0.2011\n",
      "Epoch: 5, loss: 1.1100, Train accuracy: 0.9652, Test accuracy: 0.2272\n",
      "Epoch: 10, loss: 0.0000, Train accuracy: 0.9988, Test accuracy: 0.2570\n",
      "------------------------------------------------------------\n",
      "Lemma: national.a, Final training accuracy: 0.9988, Final test accuracy: 0.2682\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 2.3445, Train accuracy: 0.1871, Test accuracy: 0.1426\n",
      "Epoch: 5, loss: 0.3378, Train accuracy: 0.9877, Test accuracy: 0.1971\n",
      "Epoch: 10, loss: 0.4952, Train accuracy: 0.9979, Test accuracy: 0.2035\n",
      "------------------------------------------------------------\n",
      "Lemma: build.v, Final training accuracy: 0.9989, Final test accuracy: 0.2308\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.3172, Train accuracy: 0.3467, Test accuracy: 0.3872\n",
      "Epoch: 5, loss: 0.1943, Train accuracy: 0.9827, Test accuracy: 0.4389\n",
      "Epoch: 10, loss: 0.3646, Train accuracy: 0.9986, Test accuracy: 0.4596\n",
      "------------------------------------------------------------\n",
      "Lemma: place.n, Final training accuracy: 0.9979, Final test accuracy: 0.4741\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.6430, Train accuracy: 0.2246, Test accuracy: 0.2338\n",
      "Epoch: 5, loss: 0.3354, Train accuracy: 0.9898, Test accuracy: 0.3129\n",
      "Epoch: 10, loss: 0.6044, Train accuracy: 0.9988, Test accuracy: 0.2950\n",
      "------------------------------------------------------------\n",
      "Lemma: position.n, Final training accuracy: 0.9982, Final test accuracy: 0.2986\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 2.5460, Train accuracy: 0.1761, Test accuracy: 0.1807\n",
      "Epoch: 5, loss: 0.3517, Train accuracy: 0.9740, Test accuracy: 0.2454\n",
      "Epoch: 10, loss: 0.2777, Train accuracy: 0.9991, Test accuracy: 0.2797\n",
      "------------------------------------------------------------\n",
      "Lemma: serve.v, Final training accuracy: 0.9965, Final test accuracy: 0.2691\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 2.1899, Train accuracy: 0.1527, Test accuracy: 0.1841\n",
      "Epoch: 5, loss: 0.3682, Train accuracy: 0.9654, Test accuracy: 0.2864\n",
      "Epoch: 10, loss: 0.6835, Train accuracy: 0.9966, Test accuracy: 0.3069\n",
      "------------------------------------------------------------\n",
      "Lemma: hold.v, Final training accuracy: 0.9945, Final test accuracy: 0.3095\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 2.1801, Train accuracy: 0.8391, Test accuracy: 0.8616\n",
      "Epoch: 5, loss: 0.1856, Train accuracy: 0.9945, Test accuracy: 0.8753\n",
      "Epoch: 10, loss: 0.1838, Train accuracy: 0.9995, Test accuracy: 0.8760\n",
      "------------------------------------------------------------\n",
      "Lemma: line.n, Final training accuracy: 0.9964, Final test accuracy: 0.8774\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.7855, Train accuracy: 0.5533, Test accuracy: 0.6343\n",
      "Epoch: 5, loss: 0.0154, Train accuracy: 0.9708, Test accuracy: 0.6575\n",
      "Epoch: 10, loss: 0.0054, Train accuracy: 0.9969, Test accuracy: 0.6587\n",
      "------------------------------------------------------------\n",
      "Lemma: see.v, Final training accuracy: 0.9896, Final test accuracy: 0.6630\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.5598, Train accuracy: 0.2786, Test accuracy: 0.2825\n",
      "Epoch: 5, loss: 0.1935, Train accuracy: 0.9692, Test accuracy: 0.2429\n",
      "Epoch: 10, loss: 0.3704, Train accuracy: 0.9981, Test accuracy: 0.2994\n",
      "------------------------------------------------------------\n",
      "Lemma: time.n, Final training accuracy: 0.9962, Final test accuracy: 0.2674\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 2.1061, Train accuracy: 0.2660, Test accuracy: 0.2321\n",
      "Epoch: 5, loss: 0.0074, Train accuracy: 0.9880, Test accuracy: 0.3439\n",
      "Epoch: 10, loss: 0.4187, Train accuracy: 0.9993, Test accuracy: 0.4008\n",
      "------------------------------------------------------------\n",
      "Lemma: physical.a, Final training accuracy: 0.9993, Final test accuracy: 0.3987\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 2.3424, Train accuracy: 0.1549, Test accuracy: 0.2024\n",
      "Epoch: 5, loss: 0.2979, Train accuracy: 0.9636, Test accuracy: 0.3285\n",
      "Epoch: 10, loss: 0.2365, Train accuracy: 0.9996, Test accuracy: 0.3297\n",
      "------------------------------------------------------------\n",
      "Lemma: follow.v, Final training accuracy: 0.9992, Final test accuracy: 0.3418\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.4895, Train accuracy: 0.2473, Test accuracy: 0.2733\n",
      "Epoch: 5, loss: 1.0044, Train accuracy: 0.9899, Test accuracy: 0.3401\n",
      "Epoch: 10, loss: 0.1771, Train accuracy: 1.0000, Test accuracy: 0.3583\n",
      "------------------------------------------------------------\n",
      "Lemma: regular.a, Final training accuracy: 1.0000, Final test accuracy: 0.3462\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 0.9631, Train accuracy: 0.5989, Test accuracy: 0.6282\n",
      "Epoch: 5, loss: 0.0000, Train accuracy: 0.9992, Test accuracy: 0.6513\n",
      "Epoch: 10, loss: 0.0006, Train accuracy: 0.9992, Test accuracy: 0.6420\n",
      "------------------------------------------------------------\n",
      "Lemma: bad.a, Final training accuracy: 0.9992, Final test accuracy: 0.6351\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.9942, Train accuracy: 0.1836, Test accuracy: 0.2003\n",
      "Epoch: 5, loss: 0.0595, Train accuracy: 0.9845, Test accuracy: 0.3179\n",
      "Epoch: 10, loss: 0.1422, Train accuracy: 0.9971, Test accuracy: 0.3179\n",
      "------------------------------------------------------------\n",
      "Lemma: force.n, Final training accuracy: 0.9990, Final test accuracy: 0.3237\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.5278, Train accuracy: 0.2954, Test accuracy: 0.2894\n",
      "Epoch: 5, loss: 0.0031, Train accuracy: 0.9854, Test accuracy: 0.3553\n",
      "Epoch: 10, loss: 0.0001, Train accuracy: 0.9960, Test accuracy: 0.3693\n",
      "------------------------------------------------------------\n",
      "Lemma: professional.a, Final training accuracy: 0.9933, Final test accuracy: 0.3473\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.6294, Train accuracy: 0.2465, Test accuracy: 0.2440\n",
      "Epoch: 5, loss: 0.1205, Train accuracy: 0.9815, Test accuracy: 0.4418\n",
      "Epoch: 10, loss: 0.0014, Train accuracy: 0.9994, Test accuracy: 0.4769\n",
      "------------------------------------------------------------\n",
      "Lemma: security.n, Final training accuracy: 1.0000, Final test accuracy: 0.5046\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.4493, Train accuracy: 0.3531, Test accuracy: 0.3980\n",
      "Epoch: 5, loss: 0.0487, Train accuracy: 0.9989, Test accuracy: 0.4342\n",
      "Epoch: 10, loss: 0.1053, Train accuracy: 1.0000, Test accuracy: 0.4507\n",
      "------------------------------------------------------------\n",
      "Lemma: positive.a, Final training accuracy: 1.0000, Final test accuracy: 0.4572\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.4220, Train accuracy: 0.3173, Test accuracy: 0.3758\n",
      "Epoch: 5, loss: 0.2413, Train accuracy: 0.9916, Test accuracy: 0.4280\n",
      "Epoch: 10, loss: 0.0064, Train accuracy: 0.9972, Test accuracy: 0.4426\n",
      "------------------------------------------------------------\n",
      "Lemma: point.n, Final training accuracy: 0.9979, Final test accuracy: 0.4196\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.4105, Train accuracy: 0.3089, Test accuracy: 0.3326\n",
      "Epoch: 5, loss: 0.4179, Train accuracy: 0.9946, Test accuracy: 0.3739\n",
      "Epoch: 10, loss: 0.2182, Train accuracy: 0.9985, Test accuracy: 0.3876\n",
      "------------------------------------------------------------\n",
      "Lemma: common.a, Final training accuracy: 0.9977, Final test accuracy: 0.3784\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 2.0954, Train accuracy: 0.2196, Test accuracy: 0.2700\n",
      "Epoch: 5, loss: 0.3982, Train accuracy: 0.9800, Test accuracy: 0.3583\n",
      "Epoch: 10, loss: 0.3296, Train accuracy: 0.9994, Test accuracy: 0.3600\n",
      "------------------------------------------------------------\n",
      "Lemma: find.v, Final training accuracy: 1.0000, Final test accuracy: 0.3600\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.5949, Train accuracy: 0.2442, Test accuracy: 0.2929\n",
      "Epoch: 5, loss: 0.1864, Train accuracy: 0.9813, Test accuracy: 0.3489\n",
      "Epoch: 10, loss: 0.2517, Train accuracy: 0.9969, Test accuracy: 0.3881\n",
      "------------------------------------------------------------\n",
      "Lemma: life.n, Final training accuracy: 0.9963, Final test accuracy: 0.3787\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.8659, Train accuracy: 0.2701, Test accuracy: 0.3174\n",
      "Epoch: 5, loss: 0.0368, Train accuracy: 0.9967, Test accuracy: 0.3972\n",
      "Epoch: 10, loss: 0.2393, Train accuracy: 1.0000, Test accuracy: 0.4092\n",
      "------------------------------------------------------------\n",
      "Lemma: order.n, Final training accuracy: 1.0000, Final test accuracy: 0.4112\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 2.0253, Train accuracy: 0.2150, Test accuracy: 0.1891\n",
      "Epoch: 5, loss: 0.1898, Train accuracy: 0.9797, Test accuracy: 0.2676\n",
      "Epoch: 10, loss: 0.4857, Train accuracy: 0.9984, Test accuracy: 0.2196\n",
      "------------------------------------------------------------\n",
      "Lemma: bring.v, Final training accuracy: 0.9952, Final test accuracy: 0.2324\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.5350, Train accuracy: 0.3419, Test accuracy: 0.3601\n",
      "Epoch: 5, loss: 0.0423, Train accuracy: 0.9950, Test accuracy: 0.4048\n",
      "Epoch: 10, loss: 0.2246, Train accuracy: 1.0000, Test accuracy: 0.4107\n",
      "------------------------------------------------------------\n",
      "Lemma: active.a, Final training accuracy: 1.0000, Final test accuracy: 0.4048\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.9269, Train accuracy: 0.2365, Test accuracy: 0.2137\n",
      "Epoch: 5, loss: 0.5202, Train accuracy: 0.9837, Test accuracy: 0.4062\n",
      "Epoch: 10, loss: 0.0012, Train accuracy: 0.9995, Test accuracy: 0.4307\n",
      "------------------------------------------------------------\n",
      "Lemma: extend.v, Final training accuracy: 0.9989, Final test accuracy: 0.4274\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.5162, Train accuracy: 0.1969, Test accuracy: 0.2071\n",
      "Epoch: 5, loss: 0.2315, Train accuracy: 0.9717, Test accuracy: 0.2462\n",
      "Epoch: 10, loss: 0.0083, Train accuracy: 0.9989, Test accuracy: 0.2937\n",
      "------------------------------------------------------------\n",
      "Lemma: case.n, Final training accuracy: 0.9943, Final test accuracy: 0.2869\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.7368, Train accuracy: 0.1842, Test accuracy: 0.1771\n",
      "Epoch: 5, loss: 0.3816, Train accuracy: 0.9838, Test accuracy: 0.2241\n",
      "Epoch: 10, loss: 0.0017, Train accuracy: 0.9990, Test accuracy: 0.2210\n",
      "------------------------------------------------------------\n",
      "Lemma: lead.v, Final training accuracy: 0.9984, Final test accuracy: 0.2179\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.5054, Train accuracy: 0.3132, Test accuracy: 0.3342\n",
      "Epoch: 5, loss: 0.3192, Train accuracy: 0.9949, Test accuracy: 0.3827\n",
      "Epoch: 10, loss: 0.0695, Train accuracy: 0.9983, Test accuracy: 0.3801\n",
      "------------------------------------------------------------\n",
      "Lemma: critical.a, Final training accuracy: 1.0000, Final test accuracy: 0.3776\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.4171, Train accuracy: 0.3168, Test accuracy: 0.2812\n",
      "Epoch: 5, loss: 0.3619, Train accuracy: 0.9956, Test accuracy: 0.3236\n",
      "Epoch: 10, loss: 0.0000, Train accuracy: 1.0000, Test accuracy: 0.3581\n",
      "------------------------------------------------------------\n",
      "Lemma: major.a, Final training accuracy: 1.0000, Final test accuracy: 0.3528\n",
      "------------------------------------------------------------\n",
      "Elapsed time:  2634.3463385105133\n",
      "Validation accuracy: 0.42568\n"
     ]
    }
   ],
   "source": [
    "# Loop over all lemmas\n",
    "\n",
    "lemma_vec = []\n",
    "train_accuracy_vec = []\n",
    "test_accuracy_vec = []\n",
    "predicted_df = test_df.copy()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for lemma in df.Lemma.unique():\n",
    "    \n",
    "    df_short = df[df.Lemma == lemma]\n",
    "    test_short = test_df[test_df.Lemma == lemma]\n",
    "    controller = Controller(df_short, test_short)\n",
    "    \n",
    "    print('-'*60)\n",
    "    print(\"Lemma: %s, Final training accuracy: %.4f, Final test accuracy: %.4f\" % \\\n",
    "                  (controller.lemma, controller.train_accuracy, controller.test_accuracy))\n",
    "    print('-'*60)\n",
    "    \n",
    "    # Append accuracies for each lemma\n",
    "    lemma_vec.append(controller.lemma)\n",
    "    train_accuracy_vec.append(controller.train_accuracy)\n",
    "    test_accuracy_vec.append(controller.test_accuracy)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = controller.sense_pred\n",
    "    for k, idx in enumerate(test_short.index):\n",
    "        predicted_df.iloc[idx].Sense_key = predictions[k]\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: \", elapsed_time)  \n",
    "\n",
    "evaluation_df = pd.DataFrame(lemma_vec, columns = [\"Lemma\"])\n",
    "evaluation_df['training_acc'] = train_accuracy_vec\n",
    "evaluation_df['test_acc'] = test_accuracy_vec\n",
    "evaluation_df.to_csv('CNN_evaluation.csv', index=False)\n",
    "predicted_df.to_csv('CNN_predictions.csv', index=False)\n",
    "\n",
    "accuracy = np.sum(validation_df.Sense_key == predicted_df.Sense_key)/(len(validation_df))\n",
    "print(\"Validation accuracy: %.5f\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and predicting a model for each lemma, all of them stop training early after around 10-15 iterations. Loss and accuracy scores are printed out on a lemma-by-lemma basis, to make comparison between models easier. We notice that for most (if not all) lemmas, the training accuracy comes close to 100% while the test accuracy varies a lot. For some words, such as **lead** and **build**, the test accuracy is as low as 20% when training stops. For **line** we get a test accuracy as high as 88%, implying that there is a clearer local structure around different senses of this particular lemma.\n",
    "\n",
    "The accuracy on the validation set is 42%, which is definitely above the MFS baseline accuracy. Since I have applied a quite simple model to a quite complicated problem, 42% does not seem all that bad.  In an image analysis setting we would commonly see several convolution layers stacked on top of each other. This could improve performance here too, but probably not too much. CNNs do learn a local structure but they have no sense of backpropagation, and will probably miss more subtle language structures no matter how many layers we stack. \n",
    "\n",
    "I see no noticeable changes in the validation accuracy after running the code for a couple of times - of course, this could be analysed further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify using deep network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also wanted to evaluate the performance of a simple deep network. As mentioned above, I use a window of 20 words from each document, centered on the position of the lemma. The embedding dimension here is 50, meaning that the reshaped vector for each embedded document will have length 1000. This is the dimension of the input layer in the neural network: the following layers are of dimensions 200, 100, 50, with the output dimension again being determined by the number of unique senses for the lemma. \n",
    "\n",
    "The Preprocessing_2 class is almost identical to the Preprocessing class above; in retrospect this did not need to be a separate class at all. The main difference is that I have added (and commented out) infrastructure for using GloVe pretrained vectors. It was interesting to try GloVe but I did not notice any performance differences compared to the nn.Embedding - one can only get so far with a classical deep net, no matter how good the embedding is. \n",
    "\n",
    "In this network as well, I have used 25% dropout followed by a log softmax function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "embeddings_dict = {}\n",
    "\n",
    "with open(\"glove.6B.50d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Preprocessing_2:\n",
    "    def __init__(self, df, num_words, win_size, embedding_size):\n",
    "        self.data = df\n",
    "        self.num_words = num_words        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.win_size = win_size\n",
    "        self.seq_len = 2*win_size\n",
    "        \n",
    "        self.vocabulary = None\n",
    "        self.x_tokenized = None\n",
    "        self.x_embedded = None\n",
    "        self.x_padded = None\n",
    "        self.x_raw = None\n",
    "        \n",
    "        self.lemma = None\n",
    "        self.n_outputs = None\n",
    "        self.le = None\n",
    "        self.y = None\n",
    "        self.y_onehot = None\n",
    "        \n",
    "        self.x_train = None\n",
    "        self.x_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None  \n",
    "    \n",
    "    def load_data(self):\n",
    "        # split into sentences (x) and sense key (y)\n",
    "        df = self.data\n",
    "        self.x_raw = df.Text.values\n",
    "        self.lemma = df.Lemma.iloc[0]\n",
    "        self.n_outputs = len(df.Sense_key.unique())\n",
    "        \n",
    "        labels = np.asarray(df.Sense_key.values)\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        self.y = le.fit_transform(labels)\n",
    "        self.le = le\n",
    "        \n",
    "    def remove_text(self):\n",
    "        win_size = self.win_size\n",
    "\n",
    "        for i, pos in enumerate(self.data.Position):\n",
    "\n",
    "            line_len = len(self.x_raw[i])\n",
    "            pos = int(pos)\n",
    "\n",
    "            while pos < win_size:\n",
    "                pos += 1\n",
    "            while pos > line_len - win_size:\n",
    "                pos -= 1\n",
    "\n",
    "            self.x_raw[i] = self.x_raw[i][pos-win_size:pos+win_size]\n",
    "        \n",
    "#     def glove_embedding(self, embeddings_dict):\n",
    "        \n",
    "#         embedding_size = self.embedding_size\n",
    "#         seq_len = self.seq_len\n",
    "\n",
    "#         padding = np.zeros(embedding_size)\n",
    "#         embedded_matrix = []\n",
    "\n",
    "#         for line in pr.x_raw:\n",
    "\n",
    "#             embedded_line = []\n",
    "\n",
    "#             for word in line:\n",
    "#                 try:\n",
    "#                     embedded_line.append(embeddings_dict[word])\n",
    "#                 except KeyError:\n",
    "#                     continue\n",
    "\n",
    "#             while len(embedded_line) < seq_len:\n",
    "#                 embedded_line.append(padding)\n",
    "\n",
    "#             embedded_line = np.array(embedded_line)\n",
    "#             embedded_matrix.append(embedded_line)\n",
    "\n",
    "#        self.embedded_matrix = np.array(embedded_matrix)   \n",
    "        \n",
    "    def build_vocabulary(self):\n",
    "        # Builds the vocabulary \n",
    "        self.vocabulary = dict()\n",
    "        fdist = nltk.FreqDist()\n",
    "\n",
    "        for sentence in self.x_raw:\n",
    "            for word in sentence:\n",
    "                fdist[word] += 1\n",
    "\n",
    "        common_words = fdist.most_common(self.num_words)\n",
    "\n",
    "        for idx, word in enumerate(common_words):\n",
    "            self.vocabulary[word[0]] = (idx+1)\n",
    "            \n",
    "    def word_to_idx(self):\n",
    "        # By using the dictionary each token is transformed into its index based representation\n",
    "        self.x_tokenized = list() \n",
    "\n",
    "        for sentence in self.x_raw:\n",
    "            temp_sentence = list()\n",
    "            for word in sentence:\n",
    "                if word in self.vocabulary.keys():\n",
    "                    temp_sentence.append(self.vocabulary[word])\n",
    "            self.x_tokenized.append(temp_sentence)\n",
    "    \n",
    "    def padding_sentences(self):\n",
    "        # Each sentence which does not fulfill the required length is padded with the index 0\n",
    "        pad_idx = 0\n",
    "        self.x_padded = list()\n",
    "\n",
    "        for sentence in self.x_tokenized:\n",
    "            while len(sentence) < self.seq_len:\n",
    "                sentence.insert(len(sentence), pad_idx)\n",
    "\n",
    "            self.x_padded.append(sentence)\n",
    "            \n",
    "        self.x_padded = np.array(self.x_padded) \n",
    "        \n",
    "    def onehot_encode(self):\n",
    "        # Create a onehot encoded representation of the targets\n",
    "        self.y_onehot = list()\n",
    "        y_idx = self.le.inverse_transform(self.y)\n",
    "        \n",
    "        for i in range(len(self.y)):\n",
    "            \n",
    "            tmp = np.zeros(self.n_outputs)\n",
    "        \n",
    "            for k in range(self.n_outputs):\n",
    "                if self.data.Sense_key.iloc[i] == y_idx[i]:\n",
    "                    tmp[self.y[i]] = 1\n",
    "                    \n",
    "            self.y_onehot.append(tmp)\n",
    "            \n",
    "        self.y_onehot = np.array(self.y_onehot)\n",
    "            \n",
    "    def split_data(self):\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = \\\n",
    "        train_test_split(self.x_padded, self.y_onehot, test_size=0.25, random_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Parameters_2:\n",
    "\n",
    "    # Preprocessing parameters\n",
    "    num_words: int = 8000\n",
    "    win_size = 10\n",
    "    seq_len = 2*win_size\n",
    "    embedding_size: int = 50\n",
    "\n",
    "    # Model parameters\n",
    "    out_size_1 = 200\n",
    "    out_size_2 = 100\n",
    "    out_size_3 = 50\n",
    "  \n",
    "    # Training parameters\n",
    "    epochs: int = 100\n",
    "    batch_size: int = 12\n",
    "    learning_rate: float = 0.001\n",
    "    early_stopping_win = 5\n",
    "        \n",
    "    # Runtime parameters - will be different for each lemma\n",
    "    n_outputs: int = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Deep classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DeepClassifier(nn.ModuleList):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        super(DeepClassifier, self).__init__()\n",
    "\n",
    "        # Parameters regarding text preprocessing\n",
    "        self.seq_len = params.seq_len\n",
    "        self.num_words = params.num_words\n",
    "        self.embedding_size = params.embedding_size\n",
    "\n",
    "        # Dropout definition\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        # Define fully connected layers\n",
    "        self.fc_1 = nn.Linear(self.seq_len*params.embedding_size, params.out_size_1)\n",
    "        self.fc_2 = nn.Linear(params.out_size_1, params.out_size_2)\n",
    "        self.fc_3 = nn.Linear(params.out_size_2, params.out_size_3)\n",
    "        self.fc_4 = nn.Linear(params.out_size_3, params.n_outputs)\n",
    "\n",
    "        # Embedding layer definition\n",
    "        self.embedding = nn.Embedding(self.num_words + 1, self.embedding_size, padding_idx=0)\n",
    "        \n",
    "        # Softmax output layer definition\n",
    "        self.log_softmax = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Sequence of tokens is filtered through an embedding layer\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Reshape to one dimension\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "\n",
    "        # Pass the embedded vector through the fully connected layers\n",
    "        x1 = self.fc_1(x)\n",
    "        x2 = self.fc_2(x1)\n",
    "        x3 = self.fc_3(x2)\n",
    "        out = self.fc_4(x3)\n",
    "        \n",
    "        # Dropout is applied\n",
    "        out = self.dropout(out)\n",
    "        out = self.log_softmax(out)\n",
    "\n",
    "        # Use this, or there's a dim-0 error when a batch contains only one value\n",
    "        if len(out) > 1:\n",
    "            return out.squeeze()\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DatasetMapper_2(Dataset):\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "class Run_2:\n",
    "    '''Training, evaluation and metrics calculation'''\n",
    "\n",
    "    @staticmethod\n",
    "    def train(model, data, params):\n",
    "\n",
    "        # Initialize dataset mapper\n",
    "        train = DatasetMapper_2(data['x_train'], data['y_train'])\n",
    "        test = DatasetMapper_2(data['x_test'], data['y_test'])\n",
    "\n",
    "        # Initialize loaders\n",
    "        loader_train = DataLoader(train, batch_size=params.batch_size)\n",
    "        loader_test = DataLoader(test, batch_size=params.batch_size)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        loss_function = nn.NLLLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params.learning_rate)\n",
    "        \n",
    "        # Define vector for early stopping\n",
    "        prev_loss = np.zeros(params.early_stopping_win)\n",
    "\n",
    "        # Starts training phase\n",
    "        for epoch in range(params.epochs):\n",
    "            # Set model in training model\n",
    "            model.train()\n",
    "            predictions = []\n",
    "            # Starts batch training\n",
    "            for x_batch, y_batch in loader_train:\n",
    "\n",
    "                y_batch = y_batch.type(torch.FloatTensor)\n",
    "\n",
    "                # Feed the model\n",
    "                y_pred = model(x_batch.long())\n",
    "                                          \n",
    "                # Transform back from onehot encoded targets\n",
    "                y_true = np.zeros(y_batch.shape[0])\n",
    "                \n",
    "                for i in range(y_batch.shape[0]):\n",
    "                    for j in range(y_batch.shape[1]):\n",
    "                        if y_batch[i,j] == 1:\n",
    "                            y_true[i] = j\n",
    "            \n",
    "                y_true = torch.from_numpy(y_true).long()\n",
    "\n",
    "                # Loss calculation\n",
    "                loss = loss_function(y_pred, y_true)\n",
    "\n",
    "                # Clean gradientes\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Gradients calculation\n",
    "                loss.backward()\n",
    "\n",
    "                # Gradients update\n",
    "                optimizer.step()\n",
    "\n",
    "                # Save predictions\n",
    "                predictions += list(y_pred.detach().numpy())\n",
    "                \n",
    "            # Evaluation phase\n",
    "            test_predictions = Run_2.evaluation(model, loader_test)\n",
    "            \n",
    "            # Metrics calculation\n",
    "            train_accuracy = Run_2.calculate_accuracy(data['y_train'], predictions)\n",
    "            test_accuracy = Run_2.calculate_accuracy(data['y_test'], test_predictions)\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                print(\"Epoch: %d, loss: %.4f, Train accuracy: %.4f, Test accuracy: %.4f\" % \\\n",
    "                      (epoch, loss.item(), train_accuracy, test_accuracy))\n",
    "            \n",
    "            # Early stopping check\n",
    "            if epoch > 10:\n",
    "                if loss.item() < min(prev_loss):\n",
    "                    prev_loss = prev_loss[1:]\n",
    "                    prev_loss = np.append(prev_loss, loss.item())\n",
    "                else:\n",
    "                    break\n",
    "                \n",
    "        return train_accuracy, test_accuracy\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluation(model, loader_test):\n",
    "\n",
    "        # Set the model in evaluation mode\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "\n",
    "        # Start evaluation phase\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in loader_test:\n",
    "                y_pred = model(x_batch.long())\n",
    "                predictions += list(y_pred.detach().numpy())\n",
    "        return predictions\n",
    "        \n",
    "    @staticmethod\n",
    "    def calculate_accuracy(grand_truth, predictions):\n",
    "        # Metrics calculation\n",
    "        correct = 0\n",
    "        \n",
    "        for true, pred in zip(grand_truth, predictions):\n",
    "    \n",
    "            for i, element in enumerate(pred):\n",
    "                if element == max(pred) and true[i] == 1:\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "        # Return accuracy\n",
    "        return (correct) / len(grand_truth)\n",
    "    \n",
    "    @staticmethod\n",
    "    def prediction(model, data, le, params):\n",
    "        \n",
    "        # Initialize loader\n",
    "        loader = DataLoader(data, batch_size=Parameters.batch_size, shuffle=False)\n",
    "        \n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x_batch in loader:\n",
    "                pred = model(x_batch.long())\n",
    "                predictions += list(pred.detach().numpy())\n",
    "                \n",
    "        sense_pred = []        \n",
    "        for line in predictions:\n",
    "            for i, val in enumerate(line):\n",
    "                if val == max(line):\n",
    "                    sense_pred.append(i)\n",
    "                    \n",
    "        sense_pred = le.inverse_transform(sense_pred)\n",
    "        \n",
    "        # Return the predicted senses\n",
    "        return sense_pred\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Controller_2(Parameters_2):\n",
    "\n",
    "    def __init__(self, df, validation_df):\n",
    "        \n",
    "        self.lemma = None\n",
    "        self.train_accuracy = None\n",
    "        self.test_accuracy = None\n",
    "        self.sense_pred = None\n",
    "        \n",
    "        # Preprocessing pipeline\n",
    "        self.data, lemma, n_outputs, le, vocabulary = self.prepare_data(df, Parameters_2)\n",
    "        \n",
    "        self.le = le\n",
    "        self.lemma = lemma\n",
    "        self.vocabulary = vocabulary\n",
    "        Parameters_2.n_outputs = n_outputs  \n",
    "\n",
    "        # Initialize the model\n",
    "        self.model = DeepClassifier(Parameters_2)\n",
    "\n",
    "        # Training - Evaluation pipeline\n",
    "        train_accuracy, test_accuracy = Run_2().train(self.model, self.data, Parameters_2)\n",
    "\n",
    "        # Save accuracies\n",
    "        self.train_accuracy = train_accuracy\n",
    "        self.test_accuracy = test_accuracy\n",
    "        \n",
    "        # Make predictions on valdiation dataset\n",
    "        self.validation_data = self.prepare_validation_data(validation_df, self.vocabulary, Parameters_2)\n",
    "        self.sense_pred = Run_2().prediction(self.model, self.validation_data, self.le, Parameters_2)\n",
    " \n",
    "    @staticmethod\n",
    "    def prepare_data(df, params):\n",
    "        \n",
    "        # Preprocessing pipeline\n",
    "        pr = Preprocessing_2(df, params.num_words, params.win_size, params.embedding_size)\n",
    "        pr.load_data()\n",
    "        pr.remove_text()\n",
    "        pr.build_vocabulary()\n",
    "        pr.word_to_idx()\n",
    "        pr.padding_sentences()\n",
    "        pr.onehot_encode()\n",
    "        pr.split_data()\n",
    "        \n",
    "        return ({'x_train': pr.x_train, 'y_train': pr.y_train, 'x_test': pr.x_test, 'y_test': pr.y_test}, \\\n",
    "                pr.lemma, pr.n_outputs, pr.le, pr.vocabulary)\n",
    "   \n",
    "    @staticmethod\n",
    "    def prepare_validation_data(df, vocabulary, params):\n",
    "        \n",
    "        num_words = len(vocabulary)\n",
    "\n",
    "        pr = Preprocessing_2(test_short, num_words, params.win_size, params.embedding_size)\n",
    "        pr.vocabulary = vocabulary\n",
    "        pr.seq_len = params.seq_len\n",
    "        \n",
    "        pr.load_data()\n",
    "        pr.remove_text()\n",
    "        pr.word_to_idx()\n",
    "        pr.padding_sentences()\n",
    "\n",
    "        return pr.x_padded\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(train_path)\n",
    "test_df = load_data(test_path)\n",
    "validation_df = load_data(validation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 1.3227, Train accuracy: 0.5873, Test accuracy: 0.6590\n",
      "Epoch: 5, loss: 0.5443, Train accuracy: 0.8872, Test accuracy: 0.6664\n",
      "Epoch: 10, loss: 0.6236, Train accuracy: 0.9508, Test accuracy: 0.7013\n",
      "------------------------------------------------------------\n",
      "Lemma: keep.v, Final training accuracy: 0.9520, Final test accuracy: 0.7042\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.7453, Train accuracy: 0.4357, Test accuracy: 0.4860\n",
      "Epoch: 5, loss: 0.0000, Train accuracy: 0.9105, Test accuracy: 0.5847\n",
      "Epoch: 10, loss: 0.0000, Train accuracy: 0.9323, Test accuracy: 0.5624\n",
      "------------------------------------------------------------\n",
      "Lemma: national.a, Final training accuracy: 0.9534, Final test accuracy: 0.5493\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 2.0070, Train accuracy: 0.3282, Test accuracy: 0.3942\n",
      "Epoch: 5, loss: 0.3522, Train accuracy: 0.8001, Test accuracy: 0.3718\n",
      "Epoch: 10, loss: 0.2950, Train accuracy: 0.9289, Test accuracy: 0.3606\n",
      "------------------------------------------------------------\n",
      "Lemma: build.v, Final training accuracy: 0.9385, Final test accuracy: 0.3446\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 0.7637, Train accuracy: 0.5670, Test accuracy: 0.6936\n",
      "Epoch: 5, loss: 0.4779, Train accuracy: 0.9351, Test accuracy: 0.6977\n",
      "Epoch: 10, loss: 0.1381, Train accuracy: 0.9551, Test accuracy: 0.6791\n",
      "------------------------------------------------------------\n",
      "Lemma: place.n, Final training accuracy: 0.9468, Final test accuracy: 0.6398\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.4403, Train accuracy: 0.3850, Test accuracy: 0.5090\n",
      "Epoch: 5, loss: 0.9840, Train accuracy: 0.8811, Test accuracy: 0.5000\n",
      "Epoch: 10, loss: 0.2205, Train accuracy: 0.9610, Test accuracy: 0.5126\n",
      "------------------------------------------------------------\n",
      "Lemma: position.n, Final training accuracy: 0.9568, Final test accuracy: 0.4946\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.5690, Train accuracy: 0.4423, Test accuracy: 0.5330\n",
      "Epoch: 5, loss: 0.3213, Train accuracy: 0.8684, Test accuracy: 0.5594\n",
      "Epoch: 10, loss: 0.2999, Train accuracy: 0.9322, Test accuracy: 0.5435\n",
      "------------------------------------------------------------\n",
      "Lemma: serve.v, Final training accuracy: 0.9467, Final test accuracy: 0.5567\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 0.9598, Train accuracy: 0.3571, Test accuracy: 0.5051\n",
      "Epoch: 5, loss: 0.0213, Train accuracy: 0.8511, Test accuracy: 0.5166\n",
      "Epoch: 10, loss: 0.0026, Train accuracy: 0.9053, Test accuracy: 0.5192\n",
      "------------------------------------------------------------\n",
      "Lemma: hold.v, Final training accuracy: 0.9232, Final test accuracy: 0.5153\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.0898, Train accuracy: 0.8942, Test accuracy: 0.9250\n",
      "Epoch: 5, loss: 0.6044, Train accuracy: 0.9810, Test accuracy: 0.9286\n",
      "Epoch: 10, loss: 0.3525, Train accuracy: 0.9822, Test accuracy: 0.9402\n",
      "------------------------------------------------------------\n",
      "Lemma: line.n, Final training accuracy: 0.9911, Final test accuracy: 0.9387\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 0.5597, Train accuracy: 0.6512, Test accuracy: 0.7107\n",
      "Epoch: 5, loss: 0.6585, Train accuracy: 0.8895, Test accuracy: 0.7058\n",
      "Epoch: 10, loss: 0.1591, Train accuracy: 0.9360, Test accuracy: 0.6936\n",
      "------------------------------------------------------------\n",
      "Lemma: see.v, Final training accuracy: 0.9478, Final test accuracy: 0.7076\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.4199, Train accuracy: 0.5440, Test accuracy: 0.6403\n",
      "Epoch: 5, loss: 0.0393, Train accuracy: 0.9201, Test accuracy: 0.6309\n",
      "Epoch: 10, loss: 0.1791, Train accuracy: 0.9541, Test accuracy: 0.6083\n",
      "------------------------------------------------------------\n",
      "Lemma: time.n, Final training accuracy: 0.9497, Final test accuracy: 0.6309\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.8790, Train accuracy: 0.4581, Test accuracy: 0.5295\n",
      "Epoch: 5, loss: 0.1167, Train accuracy: 0.9233, Test accuracy: 0.5232\n",
      "Epoch: 10, loss: 0.3589, Train accuracy: 0.9557, Test accuracy: 0.5802\n",
      "------------------------------------------------------------\n",
      "Lemma: physical.a, Final training accuracy: 0.9662, Final test accuracy: 0.5781\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 2.2142, Train accuracy: 0.4316, Test accuracy: 0.5115\n",
      "Epoch: 5, loss: 0.7744, Train accuracy: 0.8390, Test accuracy: 0.5479\n",
      "Epoch: 10, loss: 0.4665, Train accuracy: 0.9256, Test accuracy: 0.5309\n",
      "------------------------------------------------------------\n",
      "Lemma: follow.v, Final training accuracy: 0.9252, Final test accuracy: 0.5236\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.0203, Train accuracy: 0.4155, Test accuracy: 0.4190\n",
      "Epoch: 5, loss: 0.0641, Train accuracy: 0.8912, Test accuracy: 0.4818\n",
      "Epoch: 10, loss: 0.7430, Train accuracy: 0.9209, Test accuracy: 0.5000\n",
      "------------------------------------------------------------\n",
      "Lemma: regular.a, Final training accuracy: 0.9459, Final test accuracy: 0.4676\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 0.9315, Train accuracy: 0.6982, Test accuracy: 0.7021\n",
      "Epoch: 5, loss: 0.2347, Train accuracy: 0.9507, Test accuracy: 0.6651\n",
      "Epoch: 10, loss: 0.0001, Train accuracy: 0.9777, Test accuracy: 0.7021\n",
      "------------------------------------------------------------\n",
      "Lemma: bad.a, Final training accuracy: 0.9630, Final test accuracy: 0.6928\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.2043, Train accuracy: 0.5984, Test accuracy: 0.6996\n",
      "Epoch: 5, loss: 0.2888, Train accuracy: 0.9356, Test accuracy: 0.7358\n",
      "Epoch: 10, loss: 0.7364, Train accuracy: 0.9714, Test accuracy: 0.7344\n",
      "------------------------------------------------------------\n",
      "Lemma: force.n, Final training accuracy: 0.9603, Final test accuracy: 0.7025\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.9896, Train accuracy: 0.5848, Test accuracy: 0.7246\n",
      "Epoch: 5, loss: 0.4682, Train accuracy: 0.9421, Test accuracy: 0.7545\n",
      "Epoch: 10, loss: 0.2314, Train accuracy: 0.9827, Test accuracy: 0.7365\n",
      "------------------------------------------------------------\n",
      "Lemma: professional.a, Final training accuracy: 0.9860, Final test accuracy: 0.7625\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 0.9434, Train accuracy: 0.6174, Test accuracy: 0.7837\n",
      "Epoch: 5, loss: 0.0001, Train accuracy: 0.9507, Test accuracy: 0.7967\n",
      "Epoch: 10, loss: 0.0000, Train accuracy: 0.9618, Test accuracy: 0.7726\n",
      "------------------------------------------------------------\n",
      "Lemma: security.n, Final training accuracy: 0.9661, Final test accuracy: 0.7985\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.1893, Train accuracy: 0.4276, Test accuracy: 0.5428\n",
      "Epoch: 5, loss: 0.3234, Train accuracy: 0.9342, Test accuracy: 0.5855\n",
      "Epoch: 10, loss: 0.7588, Train accuracy: 0.9353, Test accuracy: 0.5855\n",
      "------------------------------------------------------------\n",
      "Lemma: positive.a, Final training accuracy: 0.9594, Final test accuracy: 0.6020\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 0.7330, Train accuracy: 0.5816, Test accuracy: 0.6952\n",
      "Epoch: 5, loss: 0.0615, Train accuracy: 0.9282, Test accuracy: 0.6827\n",
      "Epoch: 10, loss: 0.2285, Train accuracy: 0.9575, Test accuracy: 0.7098\n",
      "------------------------------------------------------------\n",
      "Lemma: point.n, Final training accuracy: 0.9644, Final test accuracy: 0.7098\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 0.9148, Train accuracy: 0.4709, Test accuracy: 0.5161\n",
      "Epoch: 5, loss: 0.3077, Train accuracy: 0.9151, Test accuracy: 0.5573\n",
      "Epoch: 10, loss: 0.5971, Train accuracy: 0.9763, Test accuracy: 0.5780\n",
      "------------------------------------------------------------\n",
      "Lemma: common.a, Final training accuracy: 0.9557, Final test accuracy: 0.5505\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 1.4057, Train accuracy: 0.4503, Test accuracy: 0.6150\n",
      "Epoch: 5, loss: 0.3632, Train accuracy: 0.8860, Test accuracy: 0.5333\n",
      "Epoch: 10, loss: 0.2160, Train accuracy: 0.9466, Test accuracy: 0.5883\n",
      "------------------------------------------------------------\n",
      "Lemma: find.v, Final training accuracy: 0.9511, Final test accuracy: 0.5517\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.3837, Train accuracy: 0.5040, Test accuracy: 0.5784\n",
      "Epoch: 5, loss: 0.0641, Train accuracy: 0.9184, Test accuracy: 0.6138\n",
      "Epoch: 10, loss: 0.3143, Train accuracy: 0.9570, Test accuracy: 0.5914\n",
      "------------------------------------------------------------\n",
      "Lemma: life.n, Final training accuracy: 0.9502, Final test accuracy: 0.5970\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.3417, Train accuracy: 0.5163, Test accuracy: 0.6327\n",
      "Epoch: 5, loss: 0.0507, Train accuracy: 0.9261, Test accuracy: 0.6567\n",
      "Epoch: 10, loss: 0.6647, Train accuracy: 0.9548, Test accuracy: 0.6906\n",
      "------------------------------------------------------------\n",
      "Lemma: order.n, Final training accuracy: 0.9667, Final test accuracy: 0.6926\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.7965, Train accuracy: 0.4048, Test accuracy: 0.4904\n",
      "Epoch: 5, loss: 0.7829, Train accuracy: 0.8620, Test accuracy: 0.4936\n",
      "Epoch: 10, loss: 0.4846, Train accuracy: 0.9444, Test accuracy: 0.4776\n",
      "------------------------------------------------------------\n",
      "Lemma: bring.v, Final training accuracy: 0.9326, Final test accuracy: 0.4936\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.1864, Train accuracy: 0.5229, Test accuracy: 0.6756\n",
      "Epoch: 5, loss: 0.4647, Train accuracy: 0.9473, Test accuracy: 0.6756\n",
      "Epoch: 10, loss: 0.2603, Train accuracy: 0.9553, Test accuracy: 0.6637\n",
      "------------------------------------------------------------\n",
      "Lemma: active.a, Final training accuracy: 0.9751, Final test accuracy: 0.6786\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.1341, Train accuracy: 0.4492, Test accuracy: 0.5302\n",
      "Epoch: 5, loss: 1.7379, Train accuracy: 0.8613, Test accuracy: 0.5237\n",
      "Epoch: 10, loss: 0.0011, Train accuracy: 0.9434, Test accuracy: 0.5432\n",
      "------------------------------------------------------------\n",
      "Lemma: extend.v, Final training accuracy: 0.9500, Final test accuracy: 0.5514\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.9929, Train accuracy: 0.3486, Test accuracy: 0.4873\n",
      "Epoch: 5, loss: 1.1931, Train accuracy: 0.8364, Test accuracy: 0.4329\n",
      "Epoch: 10, loss: 0.2652, Train accuracy: 0.9389, Test accuracy: 0.4414\n",
      "------------------------------------------------------------\n",
      "Lemma: case.n, Final training accuracy: 0.9462, Final test accuracy: 0.4295\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.6793, Train accuracy: 0.3637, Test accuracy: 0.4342\n",
      "Epoch: 5, loss: 0.6393, Train accuracy: 0.8404, Test accuracy: 0.4655\n",
      "Epoch: 10, loss: 0.4622, Train accuracy: 0.9325, Test accuracy: 0.4389\n",
      "------------------------------------------------------------\n",
      "Lemma: lead.v, Final training accuracy: 0.9335, Final test accuracy: 0.4404\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.1312, Train accuracy: 0.4757, Test accuracy: 0.6020\n",
      "Epoch: 5, loss: 0.1347, Train accuracy: 0.9396, Test accuracy: 0.5995\n",
      "Epoch: 10, loss: 0.2971, Train accuracy: 0.9728, Test accuracy: 0.6071\n",
      "------------------------------------------------------------\n",
      "Lemma: critical.a, Final training accuracy: 0.9702, Final test accuracy: 0.6148\n",
      "------------------------------------------------------------\n",
      "Epoch: 0, loss: 1.0924, Train accuracy: 0.4292, Test accuracy: 0.4960\n",
      "Epoch: 5, loss: 0.0002, Train accuracy: 0.8469, Test accuracy: 0.4801\n",
      "Epoch: 10, loss: 0.6931, Train accuracy: 0.9699, Test accuracy: 0.4801\n",
      "------------------------------------------------------------\n",
      "Lemma: major.a, Final training accuracy: 0.9805, Final test accuracy: 0.4854\n",
      "------------------------------------------------------------\n",
      "Elapsed time:  1120.9669444561005\n",
      "Validation accuracy: 0.51537\n"
     ]
    }
   ],
   "source": [
    "# Loop over all lemmas\n",
    "\n",
    "lemma_vec = []\n",
    "train_accuracy_vec = []\n",
    "test_accuracy_vec = []\n",
    "predicted_df = test_df.copy()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for lemma in df.Lemma.unique():\n",
    "    \n",
    "    df_short = df[df.Lemma == lemma]\n",
    "    test_short = test_df[test_df.Lemma == lemma]\n",
    "    controller = Controller_2(df_short, test_short)\n",
    "    \n",
    "    print('-'*60)\n",
    "    print(\"Lemma: %s, Final training accuracy: %.4f, Final test accuracy: %.4f\" % \\\n",
    "                  (controller.lemma, controller.train_accuracy, controller.test_accuracy))\n",
    "    print('-'*60)\n",
    "    \n",
    "    # Append accuracies for each lemma\n",
    "    lemma_vec.append(controller.lemma)\n",
    "    train_accuracy_vec.append(controller.train_accuracy)\n",
    "    test_accuracy_vec.append(controller.test_accuracy)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = controller.sense_pred\n",
    "    for k, idx in enumerate(test_short.index):\n",
    "        predicted_df.iloc[idx].Sense_key = predictions[k]\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: \", elapsed_time)  \n",
    "\n",
    "evaluation_df = pd.DataFrame(lemma_vec, columns = [\"Lemma\"])\n",
    "evaluation_df['training_acc'] = train_accuracy_vec\n",
    "evaluation_df['test_acc'] = test_accuracy_vec\n",
    "evaluation_df.to_csv('Deep_evaluation.csv', index=False)\n",
    "predicted_df.to_csv('Deep_predictions.csv', index=False)\n",
    "\n",
    "accuracy = np.sum(validation_df.Sense_key == predicted_df.Sense_key)/(len(validation_df))\n",
    "print(\"Validation accuracy: %.5f\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the final validation accuracy is higher for the regular deep network! It is also noticeably faster to train. Validation accuracy is around 51% with respect to a couple of runs (again, this could be explored further). The deep network's prediction across a 20-word window in the validation set will be based on whether it's seen similar words in the same order before. In other words, it has a less sophisticated approach than CNN but it still manages to do quite well - possibly because I have filtered out a lot of unneccessary information in the preprocessing stage. Words closer to the lemma are more likely to be related to the sense. One more consistent way of filtering information would be to only look at the sentence containing the lemma, and disregard the rest of the documents. \n",
    "\n",
    "This data processing approach could be tried with CNNs as well, but even more interestingly it could be inputted into a more sophisticated model. GRU, LSTM and attention models are all much better suited for this task and it would be very interesting to explore how they perform. A structure that allows for feedback through and gives different importance to different tasks of the text would have really good chances of doing well in WSD. However, due to lack of time I will have to look into this some other time. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
