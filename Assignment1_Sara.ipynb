{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import seaborn as sns\n",
    "\n",
    "from spacy.vocab import Vocab\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Warmup: Computing word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def count_word_frequencies(YOUR_FILE, ENCODING):\n",
    "\n",
    "    freqs = Counter()\n",
    "    with open(YOUR_FILE, encoding = ENCODING) as f:\n",
    "        for line in f:\n",
    "            tokens = line.lower().split()\n",
    "            for token in tokens:\n",
    "                freqs[token] += 1\n",
    "                \n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def count_word_pairs(YOUR_FILE, ENCODING):\n",
    "    \n",
    "    freqs = defaultdict(Counter)\n",
    "    with open(YOUR_FILE, encoding = ENCODING) as f:\n",
    "        for line in f:\n",
    "            tokens = line.lower().split()\n",
    "            for t1, t2 in zip(tokens, tokens[1:]):\n",
    "                freqs[t1][t2] += 1\n",
    "    \n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wiki_freqs = count_word_frequencies(\"a1_data/wikipedia.txt\", 'utf-8')\n",
    "wiki_freqs.most_common()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "euro_freqs = count_word_frequencies(\"a1_data/europarl.txt\", 'utf-8')\n",
    "euro_freqs.most_common()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "book_freqs = count_word_frequencies(\"a1_data/books.txt\", 'ISO-8859-1')\n",
    "book_freqs.most_common()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wiki_pairs = count_word_pairs(\"a1_data/wikipedia.txt\", 'utf-8')\n",
    "wiki_pairs[\"red\"].most_common()[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "euro_pairs = count_word_pairs(\"a1_data/europarl.txt\", 'utf-8')\n",
    "euro_pairs[\"red\"].most_common()[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "book_pairs = count_word_pairs(\"a1_data/books.txt\", 'ISO-8859-1')\n",
    "book_pairs[\"red\"].most_common()[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Investigating the word frequency distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_rank_frequency(freqs, plot_length):\n",
    "\n",
    "    fig_size = (15, 5)\n",
    "    font_size = 15\n",
    "    x_vec = []\n",
    "    y_vec = []\n",
    "    \n",
    "    common_words = freqs.most_common()[0:plot_length+1]\n",
    "\n",
    "    for i in range(plot_length):\n",
    "        x_vec.append(i)\n",
    "        y_vec.append(common_words[i][1])\n",
    "\n",
    "    f,(ax1, ax2) = plt.subplots(1, 2, figsize= fig_size)\n",
    "    ax1.plot(x_vec, y_vec)\n",
    "    ax1.set_xlabel(\"Rank of the word\")\n",
    "    ax1.set_ylabel(\"Frequency of the word\")\n",
    "    ax1.set_title(\"Rank/frequency plot of the \" + str(plot_length) + \" most common words.\", fontsize = font_size)\n",
    "\n",
    "    ax2.loglog(x_vec, y_vec)\n",
    "    ax2.set_xlabel(\"Rank of the word\")\n",
    "    ax2.set_ylabel(\"Frequency of the word\")\n",
    "    ax2.set_title(\"Rank/frequency plot, in log-log scale.\", fontsize = font_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_rank_frequency(book_freqs, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_rank_frequency(wiki_freqs, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_rank_frequency(euro_freqs, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Reflection\n",
    "\n",
    "The rank/frequency plots show for each corpus shows that a few short words occur very frequently, but that the frequency decreases quickly for less common words. This uneven distribution should make it harder to train machine learning models on language data - key words occur rarely in training data, but are very important for the model's understanding. \n",
    "\n",
    "Zipf's law states that for a given corpus, the frequency of any word is inversely proportional to its rank in the frequency table. Thus, on a log-log scale we expect the rank/frequency plot to take the form of a straigt line (with a negative slope). This appears to be a good fit for the Wikipedia corpus. For both the European Parlaiment and the Book Reviews corpora, the rank/frequency plot has a smaller slope initially but follows Zipf's law well for higer rank words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Comparing corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# What words are \"typical\" of the European Parliament corpus when we compare it to the book review corpus, \n",
    "# or vice versa? You will have to come up with your own operationalization of the notion of \"typical\" here.\n",
    "\n",
    "# My idea: find 50 most common words for each corpus\n",
    "# check which words are only on the top 50 list for one of the corpora\n",
    "\n",
    "compare_length = 50\n",
    "\n",
    "euro_common = euro_freqs.most_common()[0:compare_length+1]\n",
    "book_common = book_freqs.most_common()[0:compare_length+1]\n",
    "\n",
    "euro_remove = []\n",
    "book_remove = []\n",
    "common_words = []\n",
    "\n",
    "# Make a list of words that occur in both lists\n",
    "for element1 in book_common:\n",
    "    word1 = element1[0]\n",
    "    for element2 in euro_common:\n",
    "        word2 = element2[0]\n",
    "        if word1 == word2:\n",
    "            common_words.append(word1)\n",
    "    \n",
    "# Make a list of what to remove from book list    \n",
    "for i in range(len(book_common)):\n",
    "    element = book_common[i][0]\n",
    "    if element in common_words:\n",
    "        book_remove.append(book_common[i])\n",
    "\n",
    "# Make a list of what to remove from euro list         \n",
    "for i in range(len(euro_common)):\n",
    "    element = euro_common[i][0]\n",
    "    if element in common_words:\n",
    "        euro_remove.append(euro_common[i])      \n",
    "        \n",
    "# Remove common words from book list\n",
    "for element in book_remove:\n",
    "    book_common.remove(element)\n",
    "\n",
    "# Remove common words from euro list    \n",
    "for element in euro_remove:\n",
    "    euro_common.remove(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# These words are typical for the European Parlaiment corpus, compared to the Book Review corpus:\n",
    "euro_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# These words are typical for the Book Review corpus, compared to the European Parlaiment corpus:\n",
    "book_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing text for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_voc_size = 1000\n",
    "word_freqs = wiki_freqs\n",
    "word_list = []\n",
    "\n",
    "if len(word_freqs.most_common()) > max_voc_size:\n",
    "    vocab = word_freqs.most_common()[0:max_voc_size]\n",
    "    \n",
    "else:\n",
    "    vocab = word_freqs\n",
    "    \n",
    "for i in range(len(vocab)):\n",
    "    word_list.append(vocab[i][0])\n",
    "\n",
    "# Get pairs of elements    \n",
    "tmp = zip(word_list, range(1,max_voc_size+1))\n",
    "# Make pairs into a dictionary\n",
    "vocab = dict(tmp)   \n",
    "vocab2 = defaultdict(int)\n",
    "vocab2.update(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"a1_data/wikipedia.txt\", encoding = 'utf-8') as f:\n",
    "    #for line in f:\n",
    "        #print(line)\n",
    "        #tokens = line.lower().split()\n",
    "        #for token in tokens:\n",
    "        #    freqs[token] += 1\n",
    "\n",
    "#return freqs\n",
    "\n",
    "count = len(open(\"a1_data/wikipedia.txt\", encoding = 'utf-8').readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2200000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = ... something ...\n",
    "\n",
    "with open(\"a1_data/wikipedia.txt\", encoding = 'utf-8') as f:\n",
    "    voc = Vocab(f, prune_by_total=1000, batch_size=8)\n",
    "\n",
    "    # go through the lines and build the vocabulary\n",
    "    #voc.build_vocab(f)\n",
    "\n",
    "with open(\"a1_data/wikipedia.txt\", encoding = 'utf-8') as f:\n",
    "    for b in voc.batches(f):\n",
    "        # b is a matrix of shape (max_length, batch_size)\n",
    "        # where max_length is the length of the longest\n",
    "        # line in the batch\n",
    "        print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
