{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assignment 2\n",
    "## NLP\n",
    "## Test adnan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import time\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Import stopwords\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "\n",
    "corpus = 'books.txt'\n",
    "encoding = 'ISO-8859-1'\n",
    "\n",
    "alpha = 0.1 # Hyperparameter\n",
    "beta = 0.1 # Hyperparameter\n",
    "K = 20 # Nr of topics\n",
    "M = 10**5 # Corpus size\n",
    "it = 50 # Nr of iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = list(string.punctuation)\n",
    "for x in stopwords.words('english'):\n",
    "    stop.append(x)\n",
    "\n",
    "'''\n",
    "Add a couple of more stopwords which were observed manually\n",
    "'''\n",
    "for x in ['-PRON-','\\n','...','..',\"'d'\",\"n't\"]:\n",
    "    stop.append(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_to_int(corpus:str, encoding:str, nr_tokens:int, stop:list) -> (list(), Counter()):\n",
    "    \n",
    "    token_count = 0\n",
    "    docs = list()\n",
    "    docs_count = 0\n",
    "    \n",
    "    # Find unique words (removal of stopwords and punctuations and other stuff.)\n",
    "    freqs = Counter()   \n",
    "    \n",
    "    with open(corpus, encoding = encoding) as f:\n",
    "        \n",
    "        while token_count < nr_tokens:\n",
    "            \n",
    "            doc = f.readline()\n",
    "            docs_count += 1\n",
    "            temp_doc = list()\n",
    "            \n",
    "            # Lemmatizion of tokens\n",
    "            result = nlp(doc.lower())\n",
    "            \n",
    "            for token in result:\n",
    "                if len(token.lemma_) > 2:\n",
    "                    if token.lemma_ not in stop:\n",
    "                        token_count += 1\n",
    "                        freqs[token.lemma_] += 1\n",
    "                        temp_doc.append(token.lemma_)\n",
    "                        \n",
    "            docs.append(temp_doc)\n",
    "\n",
    "    print('Nr of tokens:' + str(token_count))\n",
    "    print('Nr of docs:' + str(docs_count))\n",
    "    \n",
    "    # Create bag of words\n",
    "    bow = []\n",
    "    vocab = freqs.most_common()\n",
    "\n",
    "    for i in range(len(vocab)):\n",
    "        bow.append(vocab[i][0])\n",
    "\n",
    "    # Get pairs of elements    \n",
    "    tmp = zip(bow, range(0,len(vocab)-1))\n",
    "    # Make pairs into a dictionary\n",
    "    vocab = dict(tmp)\n",
    "        \n",
    "    # Match token to int\n",
    "    docs_int = list()\n",
    "    \n",
    "    for doc in docs:\n",
    "        docs_int.append(list([vocab.get(x) for x in doc]))\n",
    "        \n",
    "    return docs_int, freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr of tokens:100025\n",
      "Nr of docs:1123\n"
     ]
    }
   ],
   "source": [
    "docs_int, freqs = docs_to_int(corpus=corpus, encoding=encoding, nr_tokens=M, stop = stop)\n",
    "n_docs = len(docs_int)\n",
    "vocab_size = len(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_lda(docs_int, freqs, n_topics):\n",
    "    \n",
    "    n_docs = len(docs_int)\n",
    "    vocab_size = len(freqs)\n",
    "\n",
    "    # Word topic in doc, corresponds to \\Theta\n",
    "    ndz = np.zeros((n_docs,n_topics))\n",
    "\n",
    "    # Word topic count, corresponds to \\phi\n",
    "    nzw = np.zeros((n_topics, vocab_size))\n",
    "\n",
    "    # Counters for documents and topics\n",
    "    nd = np.zeros(n_docs)\n",
    "    nz = np.zeros(n_topics)\n",
    "\n",
    "    # Create dictionary of topics\n",
    "    topics = {}\n",
    "\n",
    "    # iterate over documents \n",
    "    for d in range(n_docs):\n",
    "        for i in range(len(docs_int[d])):\n",
    "                  \n",
    "            w = docs_int[d][i] # Numerical rep of word w in doc d\n",
    "\n",
    "            # Initialise with a random topic\n",
    "            z = np.random.randint(n_topics)\n",
    "            topics[(d,i)] = z\n",
    "\n",
    "            # Increase counters\n",
    "            ndz[d, z] += 1\n",
    "            nzw[z, w] += 1\n",
    "\n",
    "            nd[d] += 1\n",
    "            nz[z] += 1\n",
    "\n",
    "    return topics, ndz, nzw, nd, nz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, ndz, nzw, nd, nz = initialize_lda(docs_int, freqs, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cond_topic_prob(ndz, nzw, nz, nd, w, d, alpha, beta, n_topics, voc_size):\n",
    "    \"\"\"\n",
    "    Conditional probability of topics. \n",
    "    \"\"\"\n",
    "\n",
    "    left = (nzw[:,w] + beta) / (nz + beta * voc_size)\n",
    "    right = (ndz[d,:] + alpha) / (nd[d] + alpha * n_topics)\n",
    "\n",
    "    p_z = left * right\n",
    "    p_z /= np.sum(p_z)\n",
    "    \n",
    "    return p_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_multinomial_beta(alpha, K=None):\n",
    "\n",
    "    if K is None:\n",
    "        # alpha is assumed to be a vector\n",
    "        return np.sum(gammaln(alpha)) - gammaln(np.sum(alpha))\n",
    "    else:\n",
    "        # alpha is assumed to be a scalar\n",
    "        return K * gammaln(alpha) - gammaln(K*alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood(n_topics, voc_size, alpha, beta, nzw, ndz):\n",
    "    likelihood = 0\n",
    "    \n",
    "    for z in range(n_topics):\n",
    "        likelihood += log_multinomial_beta(nzw[z,:] + beta)\n",
    "        likelihood -= log_multinomial_beta(beta, voc_size)\n",
    "        \n",
    "    for d in range(n_docs):\n",
    "        likelihood += log_multinomial_beta(ndz[d,:] + alpha)\n",
    "        likelihood -= log_multinomial_beta(alpha, n_topics)\n",
    "        \n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_Gibbs_Sampler(docs_int, voc_size, n_docs, n_topics, max_iterations, alpha, beta):\n",
    "\n",
    "    start_time = time.time()\n",
    "    topics, ndz, nzw, nd, nz = initialize_lda(docs_int, freqs, n_topics)\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        for d in range(n_docs):\n",
    "             for j in range(len(docs_int[d])):\n",
    "                \n",
    "                w = docs_int[d][j]\n",
    "                z = topics[(d, j)]\n",
    "                ndz[d, z] -= 1\n",
    "                nzw[z, w] -= 1\n",
    "                nd[d] -= 1\n",
    "                nz[z] -= 1\n",
    "\n",
    "                p_z = cond_topic_prob(ndz, nzw, nz, nd, w, d, alpha, beta, n_topics, vocab_size)\n",
    "                z = np.random.multinomial(1, p_z).argmax()\n",
    "\n",
    "                ndz[d,z] += 1\n",
    "                nzw[z,w] += 1\n",
    "                nd[d] += 1\n",
    "                nz[z] += 1\n",
    "                topics[(d, j)] = z\n",
    "\n",
    "        print(\"Iteration\", i)\n",
    "        print(\"Likelihood\", loglikelihood(n_topics, voc_size, alpha, beta, nzw, ndz))\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"Elapsed time: \", elapsed_time)\n",
    "    \n",
    "    return nzw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (20,1,14063) (20,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-387-87d986f9047d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m word_topic_prob_01_10 = LDA_Gibbs_Sampler(docs_int, vocab_size, n_docs, K, \\\n\u001b[0;32m----> 2\u001b[0;31m                                           it, alpha, beta)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-386-114be7540603>\u001b[0m in \u001b[0;36mLDA_Gibbs_Sampler\u001b[0;34m(docs_int, voc_size, n_docs, n_topics, max_iterations, alpha, beta)\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mnz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                 \u001b[0mp_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcond_topic_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnzw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                 \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-381-74c30348495e>\u001b[0m in \u001b[0;36mcond_topic_prob\u001b[0;34m(ndz, nzw, nz, nd, w, d, alpha, beta, n_topics, voc_size)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnzw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnz\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvoc_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mndz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (20,1,14063) (20,) "
     ]
    }
   ],
   "source": [
    "word_topic_prob_01_10 = LDA_Gibbs_Sampler(docs_int, vocab_size, n_docs, K, \\\n",
    "                                          it, alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
