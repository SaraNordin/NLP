{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assignment 2\n",
    "## NLP\n",
    "## Adnan Fazlinovic & Sara Nordin HÃ¤llgren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import math\n",
    "\n",
    "import time\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Import stopwords\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords list\n",
    "\n",
    "stop = list(string.punctuation)\n",
    "for x in stopwords.words('english'):\n",
    "    stop.append(x)\n",
    "\n",
    "'''\n",
    "Add a couple of more stopwords which were observed manually\n",
    "'''\n",
    "for x in ['-PRON-','\\n','...','..',\"'d'\",\"n't\"]:\n",
    "    stop.append(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define corpus\n",
    "corpus = 'books.txt'\n",
    "encoding = 'ISO-8859-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(corpus:str, encoding:str, nr_tokens:int, stop:list) -> list():\n",
    "    \n",
    "    '''\n",
    "    Only consider 100 000 tokens.\n",
    "    Removes strings from stopword-list from corpus.\n",
    "    Removes string with fewer characters than 3.\n",
    "    Keeps lemmatized words.\n",
    "    '''\n",
    "    \n",
    "    token_count = 0\n",
    "    docs = list()\n",
    "    docs_count = 0\n",
    "    freqs = Counter()  \n",
    "    \n",
    "    with open(corpus, encoding = encoding) as f:\n",
    "        \n",
    "        while token_count < nr_tokens:\n",
    "            \n",
    "            doc = f.readline()\n",
    "            docs_count += 1\n",
    "            temp_doc = list()\n",
    "            \n",
    "            # Lemmatizion of tokens\n",
    "            result = nlp(doc.lower())\n",
    "            \n",
    "            for token in result:\n",
    "                if len(token.lemma_) > 2:\n",
    "                    if token.lemma_ not in stop:\n",
    "                        token_count += 1\n",
    "                        freqs[token.lemma_] += 1\n",
    "                        temp_doc.append(token.lemma_)\n",
    "                        \n",
    "            docs.append(temp_doc)\n",
    "\n",
    "    print('Nr of tokens:' + str(token_count))\n",
    "    print('Nr of docs:' + str(docs_count))\n",
    "    \n",
    "    return docs\n",
    "\n",
    "def map_to_int(docs:list()) -> (list(), int, Counter()):\n",
    "    \n",
    "    '''\n",
    "    Create bag of words from cleaned and smaller corpus.\n",
    "    Associate each word in bag with an unique integer,\n",
    "    ranging from 0 (most common word) to length of bag of words.\n",
    "    Map each token in docs to the respective int. Return this list of list of ints.\n",
    "    '''\n",
    "    \n",
    "    freqs = Counter()\n",
    "    for doc in docs:\n",
    "        for token in doc:\n",
    "            freqs[token] += 1\n",
    "    most_common = freqs.most_common()\n",
    "    \n",
    "    token_to_int = []\n",
    "    for i in range(len(most_common)):\n",
    "        token_to_int.append(most_common[i][0])\n",
    "\n",
    "    # Get pairs of elements    \n",
    "    mapping = zip(token_to_int, range(0,len(token_to_int)))\n",
    "    \n",
    "    # Make pairs into a dictionary\n",
    "    vocab = dict(mapping)\n",
    "        \n",
    "    # Match token to int\n",
    "    docs_int = list()\n",
    "    \n",
    "    for doc in docs:\n",
    "        docs_int.append(list([vocab.get(x) for x in doc]))\n",
    "        \n",
    "    return docs_int, len(vocab), vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr of tokens:100010\n",
      "Nr of docs:1246\n"
     ]
    }
   ],
   "source": [
    "docs = clean_corpus(corpus=corpus, encoding=encoding, nr_tokens=100000, stop=stop)\n",
    "docs_mapped, vocab_size, vocab = map_to_int(docs)\n",
    "n_docs = len(docs_mapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lda(docs:list(), n_docs:int, n_topics:int, vocab_size:int) -> (np.matrix, np.matrix, np.matrix):\n",
    "    \n",
    "    # \\Theta\n",
    "    theta = np.zeros((n_docs,n_topics))\n",
    "    for d in range(n_docs):\n",
    "        theta[d] = np.random.dirichlet(alpha * np.ones(n_topics))\n",
    "\n",
    "    # \\phi\n",
    "    phi = np.zeros((n_topics, vocab_size))\n",
    "    for z in range(n_topics):\n",
    "        phi[z] = np.random.dirichlet(beta*np.ones(vocab_size))\n",
    "    \n",
    "    # Word topic assignment\n",
    "    Z_dj = list()\n",
    "    for d in range(n_docs):\n",
    "        Z_dj.append([0]*len(docs_mapped[d]))\n",
    "    \n",
    "    # Init wt by randomly assigning topics to each word in corpus\n",
    "    for d in range(n_docs):\n",
    "        for j in range(len(docs[d])):\n",
    "            Z_dj[d][j] = np.random.randint(n_topics) # Generate random topic\n",
    "        \n",
    "    return theta, phi, Z_dj\n",
    "\n",
    "def update(docs, Z_dj, n_docs, n_topics, vocab_size, theta, phi):\n",
    "    \n",
    "    # Counter for word j in document d\n",
    "    nd = np.zeros((n_docs, n_topics))\n",
    "    mk = np.zeros((n_topics, vocab_size))\n",
    "    \n",
    "    for d in range(n_docs):\n",
    "        for j in range(len(docs[d])):\n",
    "            nd[d][Z_dj[d][j]] += 1\n",
    "            mk[Z_dj[d][j]][docs[d][j]] += 1\n",
    "            \n",
    "    return nd, mk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs(docs:list(), n_docs:int, n_topics:int, vocab:dict(), vocab_size:int, max_iterations:int, alpha:int, beta:int):\n",
    "\n",
    "    start_time = time.time()\n",
    "    theta, phi, Z_dj = init_lda(docs, n_docs=n_docs, n_topics=n_topics, vocab_size=vocab_size)\n",
    "    print('Matrices initialized.')\n",
    "\n",
    "    for it in range(max_iterations):\n",
    "        #print('Iteration ' + str(it))\n",
    "        \n",
    "        nd, mk = update(docs, Z_dj, n_docs, n_topics, vocab_size, theta, phi)\n",
    "        \n",
    "        for d in range(n_docs):\n",
    "            \n",
    "            # Update \\Theta\n",
    "            theta[d] = np.random.dirichlet(alpha + nd[d])\n",
    "            \n",
    "        #print('Theta updated.')\n",
    "        \n",
    "        for k in range(n_topics):\n",
    "            \n",
    "            # Update \\phi\n",
    "            phi[k] = np.random.dirichlet(beta + mk[k])\n",
    "            \n",
    "        #print('phi updated.')\n",
    "        \n",
    "        for d in range(n_docs):\n",
    "            for j in range(len(docs[d])):\n",
    "                \n",
    "                p_dw = np.exp(np.log(theta[d]) + np.log([row[docs[d][j]] for row in phi]))\n",
    "                p_dw /= sum(p_dw)\n",
    "                \n",
    "                Z_dj[d][j] = np.random.multinomial(1, p_dw).argmax()\n",
    "                \n",
    "        if (it+1) % 10 == 0:\n",
    "            print('Iteration ' + str(it+1))\n",
    "                \n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"Elapsed time: \", elapsed_time)\n",
    "    \n",
    "    tops = list()\n",
    "    for k in range(n_topics):\n",
    "        order = np.array(mk[k]).argsort()[::-1][:10]\n",
    "        dt = list()\n",
    "        for x in order:\n",
    "            for k,v in vocab.items():\n",
    "                if x==v:\n",
    "                    dt.append(k)\n",
    "        tops.append(dt)\n",
    "\n",
    "    for i, topic in enumerate(tops):\n",
    "        print('Topic ' + str(i), topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN\n",
    "## Try different settings\n",
    "\n",
    "Since we merged our codes into one document, all simulation results were not plotted and needed to be re-runned. We will re-run the setting we found out to give the best results, which is the one below.\n",
    "\n",
    "$\\alpha = 0.1$ \\\n",
    "$\\beta = 0.1$ \\\n",
    "$K = 30$ \\\n",
    "$iterations = 100$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrices initialized.\n",
      "Iteration 10\n",
      "Iteration 20\n",
      "Iteration 30\n",
      "Iteration 40\n",
      "Iteration 50\n",
      "Iteration 60\n",
      "Iteration 70\n",
      "Iteration 80\n",
      "Iteration 90\n",
      "Iteration 100\n",
      "Elapsed time:  455.75435495376587\n",
      "Topic 0 ['science', 'evidence', 'national', 'future', 'rule', 'scientific', 'theory', 'law', 'bach', 'history']\n",
      "Topic 1 ['book', 'read', 'child', 'get', 'find', 'love', 'life', 'like', 'make', 'one']\n",
      "Topic 2 ['norris', 'fish', 'run', 'government', 'american', 'movie', 'boat', 'ship', 'chuck', 'service']\n",
      "Topic 3 ['book', 'like', 'read', 'one', 'would', 'say', 'get', 'time', 'find', 'make']\n",
      "Topic 4 ['novel', 'character', 'story', 'reader', 'plot', 'jason', 'series', 'fiction', 'erica', 'scene']\n",
      "Topic 5 ['soviet', 'british', 'army', 'mcbride', 'red', 'german', 'war', 'soldier', 'also', 'chinese']\n",
      "Topic 6 ['recipe', 'horse', 'french', 'soul', 'cook', 'ingredient', 'kitchen', 'nut', 'path', 'author']\n",
      "Topic 7 ['war', 'american', 'military', 'world', 'vietnam', 'president', 'conflict', 'america', 'force', 'political']\n",
      "Topic 8 ['food', 'recipe', 'include', 'eat', 'diet', 'freddie', 'cookbook', 'meal', 'tree', 'health']\n",
      "Topic 9 ['power', 'johnson', 'new', 'format', 'commentary', 'javascript', 'poor', 'system', 'financial', 'fee']\n",
      "Topic 10 ['book', 'information', 'well', 'design', 'whole', 'work', 'picture', 'look', 'color', 'fun']\n",
      "Topic 11 ['machine', 'human', 'process', 'von', 'thinking', 'model', 'produce', 'intelligence', 'computer', 'science']\n",
      "Topic 12 ['nothing', 'world', 'thurmond', 'william', 'mae', 'meaning', 'essie', 'adso', 'dillard', 'christian']\n",
      "Topic 13 ['book', 'read', 'one', 'story', 'write', 'time', 'good', 'well', 'work', 'many']\n",
      "Topic 14 ['mystery', 'american', 'leaphorn', 'chee', 'myth', 'hillerman', 'corner', 'killer', 'pray', 'navajo']\n",
      "Topic 15 ['player', 'guide', 'business', 'poor', 'people', 'rich', 'game', 'personal', 'race', 'estate']\n",
      "Topic 16 ['woman', 'man', 'sex', 'body', 'way', 'matter', 'life', 'relationship', 'female', 'lawrence']\n",
      "Topic 17 ['story', 'series', '....', 'king', 'city', 'tale', 'leave', 'river', 'inside', 'find']\n",
      "Topic 18 ['game', 'murder', 'case', 'customer', 'alex', 'angel', 'help', 'offer', 'brain', 'north']\n",
      "Topic 19 ['society', 'human', 'must', 'new', 'power', 'century', 'future', 'gibbon', 'create', 'life']\n",
      "Topic 20 ['asher', 'art', 'edition', 'classic', 'spenser', 'lev', 'syllable', 'quartet', 'five', 'malory']\n",
      "Topic 21 ['news', 'world', 'issue', 'company', 'free', 'problem', 'show', 'west', 'homeless', 'report']\n",
      "Topic 22 ['play', 'prospero', 'world', 'death', 'father', 'son', 'beautiful', 'life', 'shakespeare', 'new']\n",
      "Topic 23 ['wilson', 'creative', 'original', 'nietzsche', 'thinking', 'robicheaux', 'pirsig', 'turner', 'self', 'burke']\n",
      "Topic 24 ['student', 'text', 'problem', 'reference', 'school', 'teacher', 'exercise', 'high', 'suggest', 'study']\n",
      "Topic 25 ['highet', 'artist', 'expert', 'art', 'teacher', 'teaching', 'pattern', 'hug', 'learning', 'tradition']\n",
      "Topic 26 ['author', 'account', 'chapter', 'history', 'war', 'provide', 'modern', 'historical', 'american', 'approach']\n",
      "Topic 27 ['edition', 'journey', 'cat', 'george', 'song', 'translation', 'original', 'brilliant', 'pyramid', 'culture']\n",
      "Topic 28 ['dog', 'sam', 'history', 'show', 'guralnick', 'cooke', 'event', 'church', 'biography', 'truth']\n",
      "Topic 29 ['book', 'one', 'good', 'use', 'great', 'read', 'would', 'give', 'well', 'also']\n"
     ]
    }
   ],
   "source": [
    "gibbs(docs=docs_mapped, n_docs=n_docs, n_topics=30, vocab=vocab, vocab_size=vocab_size, max_iterations=100, alpha=0.1, beta=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find some topics with quite good results. For example, topic 0 is rather science-oriented, with maybe exception of \"bach\". Topic 2 seems to deal with actor Chuck Norris and some of his movies. Topic 5 is rather war-oriented with different countries and armies. Topic 6 is food-related, and 7 is war/military oriented. Topic 8 is again about food, and topic 11 is a bit AI-oriented. Topic 16 is strongly related to men and female and relationships. Topic 21 seems to deal with news, and topic 22 is about plays (theatre). Topic 23 is about philosofers (maybe) and 24 and 25 about education. 26 is history-oriented. Rest is quite unclear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
