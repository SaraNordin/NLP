{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import time\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from numpy import random as rd\n",
    "from scipy.special import gammaln\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## File related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def find_n_lines(filename, encoding, n_tokens):\n",
    "    \"\"\"\n",
    "    We should train on a subset of the corpus, with 100 0000 tokens. Find how many lines this corresponds to.\n",
    "    \"\"\"\n",
    "    \n",
    "    line_counter = 0\n",
    "    token_counter = 0\n",
    "\n",
    "    with open(filename, encoding = encoding) as f:\n",
    "        for line in f:\n",
    "\n",
    "            line_counter += 1\n",
    "            tokens = line.lower().split()\n",
    "\n",
    "            for token in tokens:\n",
    "                token_counter += 1\n",
    "\n",
    "            if token_counter > n_tokens:\n",
    "                break\n",
    "\n",
    "    return line_counter\n",
    "\n",
    "\n",
    "def count_word_frequencies(filename, encoding, n_lines, ignore_list):\n",
    "    \n",
    "    freqs = Counter()\n",
    "    with open(filename, encoding = encoding) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            \n",
    "            tokens = line.lower().split()\n",
    "            for token in tokens:\n",
    "                if token not in ignore_list:\n",
    "                    freqs[token] += 1\n",
    "                \n",
    "            if i == n_lines:\n",
    "                break\n",
    "                \n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def list_of_stopwords():\n",
    "    # Ignore all stopwords in the text!\n",
    "    ignore_words = stopwords.words('english')\n",
    "    also_ignore = [\",\", \".\", '\"', \"(\", \")\", \"-\", \"'\", \"!\", \"?\", \":\", \";\", \"/\", \"n't\", \"'s\", \"'m\"]\n",
    "\n",
    "    for item in also_ignore:\n",
    "        ignore_words.append(item)\n",
    "        \n",
    "    return ignore_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Batch related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_integer_vocabulary(word_freqs, max_voc_size):\n",
    "    \n",
    "    \"\"\" \n",
    "    Create vocabulary where common words are matched to integers. \n",
    "    \"\"\"\n",
    "    \n",
    "    word_list = []\n",
    "\n",
    "    if len(word_freqs.most_common()) > max_voc_size:\n",
    "        vocab = word_freqs.most_common()[0:max_voc_size]\n",
    "\n",
    "    else:\n",
    "        vocab = word_freqs.most_common()\n",
    "\n",
    "    for i in range(len(vocab)):\n",
    "        word_list.append(vocab[i][0])\n",
    "\n",
    "    # Get pairs of elements    \n",
    "    tmp = zip(word_list, range(1,max_voc_size+1))\n",
    "    # Make pairs into a dictionary\n",
    "    vocab = dict(tmp)\n",
    "\n",
    "    # Create default dictionary - returns 0 if an undefined key is called\n",
    "    vocab2 = defaultdict(int)\n",
    "    vocab2.update(vocab)\n",
    "    \n",
    "    return vocab2\n",
    "\n",
    "def find_batch_dimensions(batch_size, filename, ENCODING):\n",
    "    \"\"\"\n",
    "    Find the length of the longest line in each batch.\n",
    "    \"\"\"\n",
    "    \n",
    "    counter = 0      # will end up being the number of lines in the document\n",
    "    len_lines = []   # will contain maximum length of a line in each batch\n",
    "    tmp_lines = []\n",
    "    \n",
    "    with open(filename, encoding=ENCODING) as f:\n",
    "        for line in f:\n",
    "            counter+=1\n",
    "            tokens = line.lower().split()\n",
    "            tmp_lines.append(len(tokens))\n",
    "\n",
    "            if (counter % batch_size == 0):\n",
    "                len_lines.append(max(tmp_lines))\n",
    "                tmp_lines = []\n",
    "                \n",
    "        #This takes care of the last batch if number of lines is not an exact multiple of batch_size\n",
    "        if (counter % batch_size != 0): \n",
    "            len_lines.append(max(tmp_lines)) # if at end of the file\n",
    "            \n",
    "    return counter, len_lines\n",
    "    \n",
    "\n",
    "def create_batches(batch_size, vocabulary, filename, ENCODING):\n",
    "    \"\"\"\n",
    "    Splits the file into batches of a specified size, and transforms common words to integers.\n",
    "    The batches are outputted in a numpy array padded with zeros. Words not in the vocabulary are set to -1.\n",
    "    \"\"\"\n",
    "    \n",
    "    counter, len_lines = find_batch_dimensions(batch_size, filename, ENCODING)\n",
    "    \n",
    "    with open(filename, encoding=ENCODING) as f:\n",
    "        batches=[]\n",
    "        batch_counter=0\n",
    "        line_counter=0\n",
    "\n",
    "        for line in f:\n",
    "            #This creates a temporary array each time we start a new batch\n",
    "            if line_counter % batch_size == 0:\n",
    "                tmp_array=np.zeros(shape=(batch_size,len_lines[batch_counter])) #fill this temporary array\n",
    "\n",
    "            tokens = line.lower().split()\n",
    "            line_as_int = list(map(vocabulary.get, tokens))\n",
    "            line_as_int = [-1 if x is None else x for x in line_as_int] # set None values to -1\n",
    "\n",
    "            tmp_array[line_counter % batch_size,0:(len(line_as_int))]=line_as_int\n",
    "\n",
    "            line_counter+=1 #when we done\n",
    "            if line_counter % batch_size ==0:\n",
    "                batches.append(tmp_array)\n",
    "                batch_counter+=1\n",
    "\n",
    "        # again this takes care of the final batch if number of lines is not multiple of batch_size\n",
    "        if line_counter % batch_size != 0:\n",
    "            tmp_array=tmp_array[0:(line_counter % batch_size),:]\n",
    "            batches.append(tmp_array)\n",
    "        \n",
    "    return(counter, batches)\n",
    "\n",
    "def get_matrix(filename, encoding, n_tokens, ignore_words):\n",
    "    # Find how many lines we need to read to get the desired number of tokens\n",
    "    n_docs = find_n_lines(filename, encoding, n_tokens)\n",
    "\n",
    "    # Count word frequencies in this subset of the file\n",
    "    word_frequencies = count_word_frequencies(filename, encoding, n_docs, ignore_words)\n",
    "\n",
    "    # Create an integer vocabulary. Don't remove any words from the vocabulary.\n",
    "    voc_size = len(word_frequencies)\n",
    "    vocabulary = create_integer_vocabulary(word_frequencies, voc_size)\n",
    "\n",
    "    # Turn the document into batches\n",
    "    lines, batches = create_batches(batch_size=n_docs, vocabulary = vocabulary, filename = filename, ENCODING = encoding)\n",
    "\n",
    "    # Save only the first batch - this is what we'll analyse\n",
    "    matrix = batches[0].astype(int)\n",
    "    return n_docs, matrix, vocabulary, voc_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_everything(n_docs, n_topics, voc_size, int_matrix):\n",
    "    \"\"\"\n",
    "    Initialise some stuff!\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of times that we observe topic z in document d\n",
    "    ndz = np.zeros((n_docs, n_topics))\n",
    "\n",
    "    # Number of times that we observe word w in topic z\n",
    "    nzw = np.zeros((n_topics, voc_size))\n",
    "\n",
    "    # Counters for documents and topics\n",
    "    nd = np.zeros(n_docs)\n",
    "    nz = np.zeros(n_topics)\n",
    "\n",
    "    # Create dictionary of topics\n",
    "    topics = {}\n",
    "\n",
    "    # iterate over documents \n",
    "    for d in range(n_docs):\n",
    "\n",
    "        # i is the index of the word in the document\n",
    "        # w is the numerical representation of the word\n",
    "        for i, w in enumerate(int_matrix[d]):\n",
    "\n",
    "            # Initialise with a random topic\n",
    "            z = rd.randint(n_topics)\n",
    "            topics[(d,i)] = z\n",
    "\n",
    "            # Increase counters\n",
    "            ndz[d, z] += 1\n",
    "            nzw[z, w] += 1\n",
    "\n",
    "            nd[d] += 1\n",
    "            nz[z] += 1\n",
    "\n",
    "    return topics, ndz, nzw, nd, nz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cond_topic_prob(ndz, nzw, nz, nd, w, d, alpha, beta, n_topics):\n",
    "    \"\"\"\n",
    "    Conditional probability of topics. Is this the same formula as in lecture notes?\n",
    "    \"\"\"\n",
    "\n",
    "    left = (nzw[:,w] + beta) / (nz + beta * voc_size)\n",
    "    right = (ndz[d,:] + alpha) / (nd[d] + alpha * n_topics)\n",
    "\n",
    "    p_z = left * right\n",
    "    p_z /= np.sum(p_z)\n",
    "    \n",
    "    return p_z\n",
    "\n",
    "def log_multinomial_beta(alpha, K=None):\n",
    "\n",
    "    if K is None:\n",
    "        # alpha is assumed to be a vector\n",
    "        return np.sum(gammaln(alpha)) - gammaln(np.sum(alpha))\n",
    "    else:\n",
    "        # alpha is assumed to be a scalar\n",
    "        return K * gammaln(alpha) - gammaln(K*alpha)\n",
    "\n",
    "# This should increase as training progresses, show it every few training iterations (?)\n",
    "def loglikelihood(n_topics, voc_size, alpha, beta, nzw, ndz):\n",
    "    likelihood = 0\n",
    "    \n",
    "    for z in range(n_topics):\n",
    "        likelihood += log_multinomial_beta(nzw[z,:] + beta)\n",
    "        likelihood -= log_multinomial_beta(beta, voc_size)\n",
    "        \n",
    "    for d in range(n_docs):\n",
    "        likelihood += log_multinomial_beta(ndz[d,:] + alpha)\n",
    "        likelihood -= log_multinomial_beta(alpha, n_topics)\n",
    "        \n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_Gibbs_Sampler(matrix, voc_size, n_docs, n_topics, max_iterations, alpha, beta):\n",
    "\n",
    "    start_time = time.time()\n",
    "    topics, ndz, nzw, nd, nz = initialise_everything(n_docs, n_topics, voc_size, matrix)\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        for d in range(n_docs):\n",
    "             for j, w in enumerate(matrix[d]):\n",
    "\n",
    "                    z = topics[(d, j)]\n",
    "                    ndz[d, z] -= 1\n",
    "                    nzw[z, w] -= 1\n",
    "                    nd[d] -= 1\n",
    "                    nz[z] -= 1\n",
    "\n",
    "                    p_z = cond_topic_prob(ndz, nzw, nz, nd, w, d, alpha, beta, n_topics)\n",
    "                    z = rd.multinomial(1, p_z).argmax()\n",
    "\n",
    "                    ndz[d,z] += 1\n",
    "                    nzw[z,w] += 1\n",
    "                    nd[d] += 1\n",
    "                    nz[z] += 1\n",
    "                    topics[(d, j)] = z\n",
    "\n",
    "        print(\"Iteration\", i)\n",
    "        print(\"Likelihood\", loglikelihood(n_topics, voc_size, alpha, beta, nzw, ndz))\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"Elapsed time: \", elapsed_time)\n",
    "    \n",
    "    return nzw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_words_by_topic(word_topic_prob, vocabulary, typical_len):\n",
    "    \n",
    "    n_topics = word_topic_prob.shape[0]\n",
    "    typical_words = []\n",
    "\n",
    "    for i in range(n_topics):\n",
    "        arr = word_topic_prob[i,:]\n",
    "        typical_ints = arr.argsort()[-typical_len-2:-2][::-1]   # there's some funny business with the last word in vocab\n",
    "        #print(typical_ints)\n",
    "\n",
    "        for search_int in typical_ints:\n",
    "            if search_int in [0, -1]:\n",
    "                typical_words.append(\"\")\n",
    "            else:\n",
    "                for k, v in vocabulary.items(): \n",
    "                    if v == search_int:\n",
    "                        typical_words.append(k)\n",
    "                        break\n",
    "\n",
    "    # Print the most common words in each topic\n",
    "    typical_words = np.reshape(typical_words, [n_topics, -1])\n",
    "    print(typical_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Write your own code for doing Gibbs sampling for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = \"books.txt\" \n",
    "encoding = \"ISO-8859-1\"\n",
    "n_tokens = 8*10**4\n",
    "\n",
    "ignore_words = list_of_stopwords()\n",
    "\n",
    "n_docs, matrix, vocabulary, voc_size = get_matrix(filename, encoding, n_tokens, ignore_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "typical_len = 10\n",
    "max_iterations = 75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 Topics, $\\alpha = \\beta = 0.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Likelihood -2351002.5387130915\n",
      "Iteration 1\n",
      "Likelihood -2335354.9581859065\n",
      "Iteration 2\n",
      "Likelihood -2323639.282311263\n",
      "Iteration 3\n",
      "Likelihood -2311577.561338965\n",
      "Iteration 4\n",
      "Likelihood -2296861.8465751046\n",
      "Iteration 5\n",
      "Likelihood -2276834.275514169\n",
      "Iteration 6\n",
      "Likelihood -2243067.8784760805\n",
      "Iteration 7\n",
      "Likelihood -2199891.9970693695\n",
      "Iteration 8\n",
      "Likelihood -2156739.2925651036\n",
      "Iteration 9\n",
      "Likelihood -2123050.5709214304\n",
      "Iteration 10\n",
      "Likelihood -2098328.7566538285\n",
      "Iteration 11\n",
      "Likelihood -2079706.214526914\n",
      "Iteration 12\n",
      "Likelihood -2065564.3258698783\n",
      "Iteration 13\n",
      "Likelihood -2052815.0407773168\n",
      "Iteration 14\n",
      "Likelihood -2041726.5628088245\n",
      "Iteration 15\n",
      "Likelihood -2028867.4095321808\n",
      "Iteration 16\n",
      "Likelihood -2017191.9350128223\n",
      "Iteration 17\n",
      "Likelihood -2005201.353564023\n",
      "Iteration 18\n",
      "Likelihood -1994140.9965495174\n",
      "Iteration 19\n",
      "Likelihood -1982265.8389743764\n",
      "Iteration 20\n",
      "Likelihood -1972151.617760897\n",
      "Iteration 21\n",
      "Likelihood -1961846.7944976867\n",
      "Iteration 22\n",
      "Likelihood -1952392.4721891806\n",
      "Iteration 23\n",
      "Likelihood -1944846.7732122233\n",
      "Iteration 24\n",
      "Likelihood -1935353.1113976575\n",
      "Iteration 25\n",
      "Likelihood -1925626.0006306628\n",
      "Iteration 26\n",
      "Likelihood -1916391.7934682453\n",
      "Iteration 27\n",
      "Likelihood -1908160.9043778633\n",
      "Iteration 28\n",
      "Likelihood -1899935.4038577112\n",
      "Iteration 29\n",
      "Likelihood -1891813.246901635\n",
      "Iteration 30\n",
      "Likelihood -1883952.5421948629\n",
      "Iteration 31\n",
      "Likelihood -1875400.224983798\n",
      "Iteration 32\n",
      "Likelihood -1865642.4329339268\n",
      "Iteration 33\n",
      "Likelihood -1856278.541152564\n",
      "Iteration 34\n",
      "Likelihood -1847291.3989991357\n",
      "Iteration 35\n",
      "Likelihood -1839476.8535994103\n",
      "Iteration 36\n",
      "Likelihood -1829911.9761416817\n",
      "Iteration 37\n",
      "Likelihood -1823324.552146303\n",
      "Iteration 38\n",
      "Likelihood -1814509.242381221\n",
      "Iteration 39\n",
      "Likelihood -1807240.6200482773\n",
      "Iteration 40\n",
      "Likelihood -1800257.862002501\n",
      "Iteration 41\n",
      "Likelihood -1792091.122316935\n",
      "Iteration 42\n",
      "Likelihood -1784313.259089396\n",
      "Iteration 43\n",
      "Likelihood -1776460.0360361072\n",
      "Iteration 44\n",
      "Likelihood -1769631.898562565\n",
      "Iteration 45\n",
      "Likelihood -1760317.019874958\n",
      "Iteration 46\n",
      "Likelihood -1755180.3753856407\n",
      "Iteration 47\n",
      "Likelihood -1749576.6849245445\n",
      "Iteration 48\n",
      "Likelihood -1742592.2426720897\n",
      "Iteration 49\n",
      "Likelihood -1735232.9461073051\n",
      "Iteration 50\n",
      "Likelihood -1726859.6143943823\n",
      "Iteration 51\n",
      "Likelihood -1719937.7392624826\n",
      "Iteration 52\n",
      "Likelihood -1712426.6210142719\n",
      "Iteration 53\n",
      "Likelihood -1704059.0160221732\n",
      "Iteration 54\n",
      "Likelihood -1695166.365664298\n",
      "Iteration 55\n",
      "Likelihood -1687976.4787738863\n",
      "Iteration 56\n",
      "Likelihood -1679418.412767039\n",
      "Iteration 57\n",
      "Likelihood -1671956.4309965533\n",
      "Iteration 58\n",
      "Likelihood -1664015.7171239078\n",
      "Iteration 59\n",
      "Likelihood -1656156.592506969\n",
      "Iteration 60\n",
      "Likelihood -1650651.7341271457\n",
      "Iteration 61\n",
      "Likelihood -1643662.7888603758\n",
      "Iteration 62\n",
      "Likelihood -1637755.2279101512\n",
      "Iteration 63\n",
      "Likelihood -1631404.285282863\n",
      "Iteration 64\n",
      "Likelihood -1624769.5442679944\n",
      "Iteration 65\n",
      "Likelihood -1619872.8342499165\n",
      "Iteration 66\n",
      "Likelihood -1615471.4221169576\n",
      "Iteration 67\n",
      "Likelihood -1610779.042299306\n",
      "Iteration 68\n",
      "Likelihood -1602479.3071940173\n",
      "Iteration 69\n",
      "Likelihood -1596595.9361555919\n",
      "Iteration 70\n",
      "Likelihood -1592045.3785948795\n",
      "Iteration 71\n",
      "Likelihood -1587022.1462863332\n",
      "Iteration 72\n",
      "Likelihood -1581470.1369899951\n",
      "Iteration 73\n",
      "Likelihood -1575225.3636989375\n",
      "Iteration 74\n",
      "Likelihood -1568555.8513864744\n",
      "Elapsed time:  2739.564166545868\n"
     ]
    }
   ],
   "source": [
    "n_topics = 10\n",
    "\n",
    "word_topic_prob_01_10 = LDA_Gibbs_Sampler(matrix, voc_size, n_docs, n_topics, \\\n",
    "                                          max_iterations, alpha = 0.1, beta = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['book' 'read' 'would' 'well' 'good' 'know' 'us' 'two' 'think' 'highly']\n",
      " ['read' 'one' 'books' 'people' 'world' 'man' 'works' 'time' 'new'\n",
      "  'women']\n",
      " ['book' 'like' 'reading' 'would' 'new' 'much' 'lot' 'school' 'long'\n",
      "  'ever']\n",
      " ['book' 'books' 'one' 'many' 'history' 'way' 'reading' 'life' 'people'\n",
      "  'work']\n",
      " ['great' 'story' 'like' 'many' 'read' 'world' 'see' 'also' 'novel'\n",
      "  'john']\n",
      " ['one' 'much' 'would' 'also' 'even' 'like' 'sam' 'get' 'time' 'could']\n",
      " ['great' 'reader' 'writing' 'love' 'must' 'different' 'information'\n",
      "  'give' 'patterns' 'could']\n",
      " ['book' 'recommend' 'read' 'anyone' 'great' 'story' 'going'\n",
      "  'interesting' 'information' 'would']\n",
      " ['book' 'one' 'good' 'first' 'really' 'best' 'time' 'find' 'work' 'get']\n",
      " ['life' 'war' 'new' 'family' 'us' '' 'without' 'theory' 'power'\n",
      "  'american']]\n"
     ]
    }
   ],
   "source": [
    "show_words_by_topic(word_topic_prob_01_10, vocabulary, typical_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50 Topics, $\\alpha = \\beta = 0.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Likelihood -3658563.936163828\n",
      "Iteration 1\n",
      "Likelihood -3608550.923346697\n",
      "Iteration 2\n",
      "Likelihood -3558537.080710233\n",
      "Iteration 3\n",
      "Likelihood -3508411.4883670104\n",
      "Iteration 4\n",
      "Likelihood -3449413.682229062\n",
      "Iteration 5\n",
      "Likelihood -3379673.1660486497\n",
      "Iteration 6\n",
      "Likelihood -3301858.0268891277\n",
      "Iteration 7\n",
      "Likelihood -3230342.4180519395\n",
      "Iteration 8\n",
      "Likelihood -3169399.4883624515\n",
      "Iteration 9\n",
      "Likelihood -3119861.807302928\n",
      "Iteration 10\n",
      "Likelihood -3077177.2423457718\n",
      "Iteration 11\n",
      "Likelihood -3035792.7022777093\n",
      "Iteration 12\n",
      "Likelihood -2998331.9405336394\n",
      "Iteration 13\n",
      "Likelihood -2963537.9445924326\n",
      "Iteration 14\n",
      "Likelihood -2929729.842392472\n",
      "Iteration 15\n",
      "Likelihood -2893592.647204069\n",
      "Iteration 16\n",
      "Likelihood -2859870.2313416135\n",
      "Iteration 17\n",
      "Likelihood -2825669.469561156\n",
      "Iteration 18\n",
      "Likelihood -2787576.176527688\n",
      "Iteration 19\n",
      "Likelihood -2750477.565235863\n",
      "Iteration 20\n",
      "Likelihood -2710736.847433333\n",
      "Iteration 21\n",
      "Likelihood -2672623.207860701\n",
      "Iteration 22\n",
      "Likelihood -2631332.23425144\n",
      "Iteration 23\n",
      "Likelihood -2591140.8551768283\n",
      "Iteration 24\n",
      "Likelihood -2546170.545524004\n",
      "Iteration 25\n",
      "Likelihood -2506323.0444725617\n",
      "Iteration 26\n",
      "Likelihood -2461583.32617288\n",
      "Iteration 27\n",
      "Likelihood -2420832.9895652807\n",
      "Iteration 28\n",
      "Likelihood -2378324.2028796324\n",
      "Iteration 29\n",
      "Likelihood -2336240.434196245\n",
      "Iteration 30\n",
      "Likelihood -2297328.1302070436\n",
      "Iteration 31\n",
      "Likelihood -2258799.9716578177\n",
      "Iteration 32\n",
      "Likelihood -2222347.0392818223\n",
      "Iteration 33\n",
      "Likelihood -2183886.062103875\n",
      "Iteration 34\n",
      "Likelihood -2146456.0306069\n",
      "Iteration 35\n",
      "Likelihood -2111927.1178290155\n",
      "Iteration 36\n",
      "Likelihood -2078682.923163468\n",
      "Iteration 37\n",
      "Likelihood -2048748.7065464582\n",
      "Iteration 38\n",
      "Likelihood -2019318.2740013609\n",
      "Iteration 39\n",
      "Likelihood -1991981.9631354662\n",
      "Iteration 40\n",
      "Likelihood -1962048.3421471082\n",
      "Iteration 41\n",
      "Likelihood -1935107.7244747356\n",
      "Iteration 42\n",
      "Likelihood -1907452.8251598494\n",
      "Iteration 43\n",
      "Likelihood -1882947.5428808474\n",
      "Iteration 44\n",
      "Likelihood -1858693.2111342698\n",
      "Iteration 45\n",
      "Likelihood -1838283.713256274\n",
      "Iteration 46\n",
      "Likelihood -1818834.5177367944\n",
      "Iteration 47\n",
      "Likelihood -1799687.6417852526\n",
      "Iteration 48\n",
      "Likelihood -1779021.8538893692\n",
      "Iteration 49\n",
      "Likelihood -1759955.0784453086\n",
      "Iteration 50\n",
      "Likelihood -1744768.1069291646\n",
      "Iteration 51\n",
      "Likelihood -1729329.3590153533\n",
      "Iteration 52\n",
      "Likelihood -1712715.451558336\n",
      "Iteration 53\n",
      "Likelihood -1698737.3899727883\n",
      "Iteration 54\n",
      "Likelihood -1682730.8633009316\n",
      "Iteration 55\n",
      "Likelihood -1670105.633627229\n",
      "Iteration 56\n",
      "Likelihood -1659206.2204886947\n",
      "Iteration 57\n",
      "Likelihood -1645401.911816334\n",
      "Iteration 58\n",
      "Likelihood -1632437.449932132\n",
      "Iteration 59\n",
      "Likelihood -1619804.0517625138\n",
      "Iteration 60\n",
      "Likelihood -1608221.8091357923\n",
      "Iteration 61\n",
      "Likelihood -1598690.3323695129\n",
      "Iteration 62\n",
      "Likelihood -1586528.5770634182\n",
      "Iteration 63\n",
      "Likelihood -1573254.5693706349\n",
      "Iteration 64\n",
      "Likelihood -1561923.9752862232\n",
      "Iteration 65\n",
      "Likelihood -1549367.5536462818\n",
      "Iteration 66\n",
      "Likelihood -1536995.4209562456\n",
      "Iteration 67\n",
      "Likelihood -1527537.036532702\n",
      "Iteration 68\n",
      "Likelihood -1520283.4736684815\n",
      "Iteration 69\n",
      "Likelihood -1510265.783573851\n",
      "Iteration 70\n",
      "Likelihood -1500548.2010935107\n",
      "Iteration 71\n",
      "Likelihood -1488787.365507433\n",
      "Iteration 72\n",
      "Likelihood -1479519.3660199025\n",
      "Iteration 73\n",
      "Likelihood -1471911.4235978427\n",
      "Iteration 74\n",
      "Likelihood -1461795.8017515158\n",
      "Elapsed time:  2944.3656651973724\n"
     ]
    }
   ],
   "source": [
    "n_topics = 50\n",
    "\n",
    "word_topic_prob_01_50 = LDA_Gibbs_Sampler(matrix, voc_size, n_docs, n_topics, \\\n",
    "                                          max_iterations, alpha = 0.1, beta = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['monster' 'born' 'horrific' 'killed' 'fell' 'girls' 'memories' 'amelia'\n",
      "  'telling' 'actual']\n",
      " ['cooke' 'biography' 'events' 'official' 'version' 'wolff' 'known'\n",
      "  'greene' 'car' 'death']\n",
      " ['edition' 'thorough' 'girlfriend' 'nature' 'exactly' 'medieval' 'vast'\n",
      "  'eugenics' 'exhaustive' 'analyzed']\n",
      " ['soul' 'human' 'xxiii' 'later' 'thomas' 'care' 'ladder' 'window'\n",
      "  'monastic' 'grow']\n",
      " ['book' 'well' 'first' 'characters' 'good' 'story' 'read' 'writing'\n",
      "  'enjoyed' 'amazing']\n",
      " ['politics' 'lbj' 'loved' 'mets' 'scientific' 'myron' 'bolitar' 'kathy'\n",
      "  'years' 'career']\n",
      " ['one' 'like' 'would' 'also' 'read' 'many' 'people' 'life' 'much' 'time']\n",
      " ['energy' 'michelle' 'vital' 'mama' 'vampirism' 'age' 'peter' 'battle'\n",
      "  'explosive' 'hearing']\n",
      " ['arguments' 'department' 'grisham' 'rhetorician' 'thousands' 'original'\n",
      "  'creator' 'zen' 'motorcycle' 'relationship']\n",
      " ['reporter' 'reviewer' '...' 'universe' 'fox' 'mop' 'diverting'\n",
      "  'plodding' 'time' 'background']\n",
      " ['reading' 'would' 'without' 'war' 'say' 'society' 'new' 'big' 'best'\n",
      "  'details']\n",
      " ['carroll' 'mr.' 'hannah' 'stage' 'voice' 'truth' 'graves' 'pretence'\n",
      "  'form' 'imitation']\n",
      " ['great' 'ringo' 'one' 'author' 'page' 'past' 'although' 'must' 'bring'\n",
      "  'last']\n",
      " ['giraut' 'wives' 'daughters' 'modern' 'acupuncture' 'rule' 'mrs'\n",
      "  'tradescant' '19th' 'development']\n",
      " ['adso' 'rose' 'word' 'st.' 'modern' 'scientist' 'thus' 'knowledge'\n",
      "  'bernard' 'science']\n",
      " ['service' 'served' 'evening' 'wine' 'euro' 'italian' 'venice' 'roasted'\n",
      "  'green' 'food']\n",
      " ['read' 'great' 'recommend' 'well' 'love' 'good' 'anyone' 'used' 'time'\n",
      "  'information']\n",
      " ['ramon' 'freeway' 'boy' 'named' 'called' 'show' 'cross' 'paths'\n",
      "  'research' 'sections']\n",
      " ['novels' '13' '100' 'offers' 'gypsies' 'dickens' 'unique' 'gypsy'\n",
      "  'tragic' 'plays']\n",
      " ['book' 'life' 'author' 'one' 'read' 'even' 'world' 'books' 'two' 'many']\n",
      " ['mccormicks' 'path' 'mystical' 'french' 'horse' 'classical'\n",
      "  'experience' 'de' 'celtic' 'carlin']\n",
      " ['role' 'year' 'art' 'act' 'experiencing' 'discovers' 'arguments'\n",
      "  'routinely' 'argues' 'claims']\n",
      " ['hunt' 'respect' 'leo' 'thoughts' 'offers' 'estby' 'mother' 'across'\n",
      "  'choices' 'emotions']\n",
      " ['read' 'story' 'could' 'child' 'know' 'time' 'us' 'going' 'gift'\n",
      "  'world']\n",
      " ['lot' 'matheson' 'meets' 'strong' 'mentioned' 'scene' 'repetition'\n",
      "  'edition' 'internally' 'evil']\n",
      " ['problems' 'bible' 'mathematics' 'johnson' 'exercises' 'dr.' 'students'\n",
      "  'use' 'hal' 'broad']\n",
      " ['recipes' 'library' 'authors' 'booklovers' 'soy' 'hope' 'books'\n",
      "  'recipe' 'town' 'side']\n",
      " ['abduction' 'field' 'hallucinations' 'consciousness' 'darwin'\n",
      "  'alternative' 'evolutionary' 'loves' 'conclusion' 'captures']\n",
      " ['style' 'orleans' 'corps' 'needs' 'south' 'american' 'lies' 'river'\n",
      "  'katrina' 'new']\n",
      " ['abandoned' 'corners' 'valley' 'listen' 'christensen' 'codes' 'hear'\n",
      "  'bill' 'vacation' 'trace']\n",
      " ['book' 'think' 'job' \"'ve\" \"'re\" 'find' 'years' 'edition' 'could'\n",
      "  'written']\n",
      " ['corinthians' 'garland' 'commentary' 'educated' 'us$' 'wrote' 'choice'\n",
      "  'surprised' 'gerald' 'printing']\n",
      " ['eugene' 'family' 'mother' 'blues' 'day' 'attempted' 'house' 'handy'\n",
      "  'addiction' 'edmund']\n",
      " ['memorable' 'surface' 'terrorism' 'laden' 'ali' 'terrorist' 'handbook'\n",
      "  'note' 'johnson' 'parallels']\n",
      " ['limit' 'games' 'carre' \"hold'em\" 'opponents' 'days' 'silly' 'le'\n",
      "  'tailor' 'related']\n",
      " ['drivers' 'humans' 'step' 'license' 'id' 'control' 'wisdom' 'act'\n",
      "  'passport' 'helped']\n",
      " ['book' 'see' 'anyone' 'research' 'men' 'every' 'political' 'rome'\n",
      "  'human' 'time']\n",
      " ['dogs' 'dog' 'pack' 'emergence' 'old' 'rise' 'johnson' 'wolf' 'idea'\n",
      "  'southern']\n",
      " ['clark' 'villagers' 'nietzsche' 'kiss' 'anna' 'exposed' 'anzaldua'\n",
      "  'qualities' '68' 'follows']\n",
      " ['history' 'women' 'one' 'way' 'novel' 'see' 'best' 'world' 'others'\n",
      "  'years']\n",
      " ['mole' 'rat' 'lovable' 'willows' 'wind' 'vain' 'along' 'badger'\n",
      "  'grahame' 'vices']\n",
      " ['contrary' 'essential' 'tremendously' 'keep' 'grand' 'puppy' 'jamaica'\n",
      "  'owe' 'flooded' 'jobless']\n",
      " ['heart' 'heard' 'bugs' 'nfl' 'besides' 'guessing' \"cabinet'\" 'hit'\n",
      "  'solutions' 'owes']\n",
      " ['poet' 'computer' 'chanting' 'practice' 'dialogue' 'odd' 'jones'\n",
      "  'douglas' 'workshop' 'turco']\n",
      " ['companies' 'mountains' 'created' 'happy' 'jobs' 'wage' 'development'\n",
      "  'issue' 'former' 'highest']\n",
      " ['book' 'read' 'great' 'would' 'one' 'really' 'well' 'like' 'good'\n",
      "  'real']\n",
      " ['cultural' 'invent' 'ed.]' 'end-of-work-day' 'meant' 'sorts' 'vibe'\n",
      "  'catholic' 'since' 'interactive']\n",
      " ['book' 'make' 'recommend' 'many' 'looking' 'work' 'want' 'however'\n",
      "  'get' 'given']\n",
      " ['series' 'discworld' 'equal' 'granny' 'magic' 'killer' 'notre' 'dame'\n",
      "  'immigrant' 'witches']\n",
      " ['students' 'text' 'alone' 'common' 'design' 'evolution' 'opinion' 'pay'\n",
      "  'darwin' 'show']]\n"
     ]
    }
   ],
   "source": [
    "show_words_by_topic(word_topic_prob_01_50, vocabulary, typical_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['horse' 'soul' 'path' 'eve' 'experience' 'ladder' 'monastic' 'plains'\n",
    "  'carlin' 'mystical'] this topic seems to relate to fantasy or something?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[discworld' 'equal' 'rites' 'magic' 'bernard' 'opinions' 'witches'\n",
    "  'aquinas' 'franciscan' 'become'] Terry Pratchett!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 Topics, $\\alpha = \\beta = 0.01$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Likelihood -2343457.870990367\n",
      "Iteration 1\n",
      "Likelihood -2320749.8681018814\n",
      "Iteration 2\n",
      "Likelihood -2304569.705220574\n",
      "Iteration 3\n",
      "Likelihood -2290597.510808511\n",
      "Iteration 4\n",
      "Likelihood -2276621.381299429\n",
      "Iteration 5\n",
      "Likelihood -2260341.027312455\n",
      "Iteration 6\n",
      "Likelihood -2235869.9136667443\n",
      "Iteration 7\n",
      "Likelihood -2202960.071286286\n",
      "Iteration 8\n",
      "Likelihood -2165050.2412849767\n",
      "Iteration 9\n",
      "Likelihood -2129827.7747944077\n",
      "Iteration 10\n",
      "Likelihood -2101752.4814116503\n",
      "Iteration 11\n",
      "Likelihood -2080085.3136327907\n",
      "Iteration 12\n",
      "Likelihood -2063487.1144998441\n",
      "Iteration 13\n",
      "Likelihood -2045536.251464159\n",
      "Iteration 14\n",
      "Likelihood -2031064.2508593616\n",
      "Iteration 15\n",
      "Likelihood -2016179.4829783826\n",
      "Iteration 16\n",
      "Likelihood -2000026.3275540767\n",
      "Iteration 17\n",
      "Likelihood -1985799.924806189\n",
      "Iteration 18\n",
      "Likelihood -1974548.4329042924\n",
      "Iteration 19\n",
      "Likelihood -1960551.3750523948\n",
      "Iteration 20\n",
      "Likelihood -1949357.0448735512\n",
      "Iteration 21\n",
      "Likelihood -1937563.1301793077\n",
      "Iteration 22\n",
      "Likelihood -1923682.851831316\n",
      "Iteration 23\n",
      "Likelihood -1913015.5144013052\n",
      "Iteration 24\n",
      "Likelihood -1902850.1489342884\n",
      "Iteration 25\n",
      "Likelihood -1890652.9295315593\n",
      "Iteration 26\n",
      "Likelihood -1881812.5760584965\n",
      "Iteration 27\n",
      "Likelihood -1872248.280135396\n",
      "Iteration 28\n",
      "Likelihood -1861734.5649729555\n",
      "Iteration 29\n",
      "Likelihood -1855376.4352269424\n",
      "Iteration 30\n",
      "Likelihood -1847007.591056231\n",
      "Iteration 31\n",
      "Likelihood -1838953.6762262182\n",
      "Iteration 32\n",
      "Likelihood -1828962.859011661\n",
      "Iteration 33\n",
      "Likelihood -1821897.3152444088\n",
      "Iteration 34\n",
      "Likelihood -1814308.8708545014\n",
      "Iteration 35\n",
      "Likelihood -1806408.591205612\n",
      "Iteration 36\n",
      "Likelihood -1798367.2159974314\n",
      "Iteration 37\n",
      "Likelihood -1790829.5185883462\n",
      "Iteration 38\n",
      "Likelihood -1786202.3196843108\n",
      "Iteration 39\n",
      "Likelihood -1780386.9058589505\n",
      "Iteration 40\n",
      "Likelihood -1772239.6836884515\n",
      "Iteration 41\n",
      "Likelihood -1762381.9676228196\n",
      "Iteration 42\n",
      "Likelihood -1755564.2298587367\n",
      "Iteration 43\n",
      "Likelihood -1750728.0541037235\n",
      "Iteration 44\n",
      "Likelihood -1744378.860365603\n",
      "Iteration 45\n",
      "Likelihood -1737437.078058091\n",
      "Iteration 46\n",
      "Likelihood -1733755.551495244\n",
      "Iteration 47\n",
      "Likelihood -1727458.0714545215\n",
      "Iteration 48\n",
      "Likelihood -1721006.3277186544\n",
      "Iteration 49\n",
      "Likelihood -1715122.9817982428\n",
      "Iteration 50\n",
      "Likelihood -1710952.3371547568\n",
      "Iteration 51\n",
      "Likelihood -1706106.5675569442\n",
      "Iteration 52\n",
      "Likelihood -1697957.5722407359\n",
      "Iteration 53\n",
      "Likelihood -1692452.612872429\n",
      "Iteration 54\n",
      "Likelihood -1686534.8308158086\n",
      "Iteration 55\n",
      "Likelihood -1681853.9312886952\n",
      "Iteration 56\n",
      "Likelihood -1676941.2438996534\n",
      "Iteration 57\n",
      "Likelihood -1671963.6910924625\n",
      "Iteration 58\n",
      "Likelihood -1666556.6505406946\n",
      "Iteration 59\n",
      "Likelihood -1661285.2033657678\n",
      "Iteration 60\n",
      "Likelihood -1656386.4206293593\n",
      "Iteration 61\n",
      "Likelihood -1652567.754273746\n",
      "Iteration 62\n",
      "Likelihood -1645563.4299394065\n",
      "Iteration 63\n",
      "Likelihood -1640503.1118176552\n",
      "Iteration 64\n",
      "Likelihood -1635873.6988552802\n",
      "Iteration 65\n",
      "Likelihood -1630479.3025649756\n",
      "Iteration 66\n",
      "Likelihood -1626144.1166818012\n",
      "Iteration 67\n",
      "Likelihood -1623191.4402579635\n",
      "Iteration 68\n",
      "Likelihood -1618651.988864466\n",
      "Iteration 69\n",
      "Likelihood -1612555.7343172184\n",
      "Iteration 70\n",
      "Likelihood -1608668.732877296\n",
      "Iteration 71\n",
      "Likelihood -1602854.501945707\n",
      "Iteration 72\n",
      "Likelihood -1597292.3802901309\n",
      "Iteration 73\n",
      "Likelihood -1590685.7790108905\n",
      "Iteration 74\n",
      "Likelihood -1589277.825193876\n",
      "Elapsed time:  2557.848205804825\n"
     ]
    }
   ],
   "source": [
    "n_topics = 10\n",
    "\n",
    "word_topic_prob_001_10 = LDA_Gibbs_Sampler(matrix, voc_size, n_docs, n_topics, max_iterations, alpha = 0.01, beta = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['book' 'good' 'best' 'author' 'new' 'reader' 'every' 'well' 'like'\n",
      "  'excellent']\n",
      " ['book' 'would' 'much' 'even' 'work' 'history' 'two' 'reading' 'like'\n",
      "  'novel']\n",
      " ['work' 'love' 'first' 'wonderful' 'easy' 'real' 'lot' 'use' 'amazing'\n",
      "  'patterns']\n",
      " ['one' 'book' 'characters' 'time' 'world' 'best' 'like' 'stories'\n",
      "  'could' 'although']\n",
      " ['book' 'good' 'pages' 'interesting' 'woman' 'high' 'recommend' 'long'\n",
      "  'still' 'stories']\n",
      " ['book' 'read' 'great' 'many' 'must' 'recommend' 'better' 'also'\n",
      "  'series' 'students']\n",
      " ['books' 'time' 'readers' 'fiction' 'children' 'reader' 'need' 'says'\n",
      "  'novels' 'american']\n",
      " ['book' 'written' 'information' 'world' 'worth' 'highly' 'novel'\n",
      "  'anyone' 'page' 'complete']\n",
      " ['book' 'life' 'people' 'us' 'story' 'also' 'one' 'read' 'first' 'many']\n",
      " ['book' 'great' 'would' 'reading' 'years' 'way' 'well' 'writing' 'works'\n",
      "  'time']]\n"
     ]
    }
   ],
   "source": [
    "show_words_by_topic(word_topic_prob_001_10, vocabulary, typical_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50 Topics, $\\alpha = \\beta = 0.01$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Likelihood -3575222.7566558477\n",
      "Iteration 1\n",
      "Likelihood -3511858.157047361\n",
      "Iteration 2\n",
      "Likelihood -3453380.977278283\n",
      "Iteration 3\n",
      "Likelihood -3396604.8616934363\n",
      "Iteration 4\n",
      "Likelihood -3330508.159939607\n",
      "Iteration 5\n",
      "Likelihood -3249983.8052112306\n",
      "Iteration 6\n",
      "Likelihood -3174941.4953452684\n",
      "Iteration 7\n",
      "Likelihood -3108042.3533887807\n",
      "Iteration 8\n",
      "Likelihood -3052397.1337047815\n",
      "Iteration 9\n",
      "Likelihood -3003682.1830039476\n",
      "Iteration 10\n",
      "Likelihood -2959216.6975951935\n",
      "Iteration 11\n",
      "Likelihood -2920272.050352145\n",
      "Iteration 12\n",
      "Likelihood -2881800.5491413083\n",
      "Iteration 13\n",
      "Likelihood -2848358.745659724\n",
      "Iteration 14\n",
      "Likelihood -2816436.5785797634\n",
      "Iteration 15\n",
      "Likelihood -2786236.584752912\n",
      "Iteration 16\n",
      "Likelihood -2757118.728225909\n",
      "Iteration 17\n",
      "Likelihood -2726209.2886329475\n",
      "Iteration 18\n",
      "Likelihood -2695776.095584607\n",
      "Iteration 19\n",
      "Likelihood -2666513.283767589\n",
      "Iteration 20\n",
      "Likelihood -2639867.317621984\n",
      "Iteration 21\n",
      "Likelihood -2615079.749422195\n",
      "Iteration 22\n",
      "Likelihood -2591513.5442678286\n",
      "Iteration 23\n",
      "Likelihood -2568032.696406108\n",
      "Iteration 24\n",
      "Likelihood -2547324.989995782\n",
      "Iteration 25\n",
      "Likelihood -2524811.1252395734\n",
      "Iteration 26\n",
      "Likelihood -2505472.6144535495\n",
      "Iteration 27\n",
      "Likelihood -2484702.697172181\n",
      "Iteration 28\n",
      "Likelihood -2465382.5075739287\n",
      "Iteration 29\n",
      "Likelihood -2445211.8929877784\n",
      "Iteration 30\n",
      "Likelihood -2430423.2534541325\n",
      "Iteration 31\n",
      "Likelihood -2410022.2735267957\n",
      "Iteration 32\n",
      "Likelihood -2392751.630579285\n",
      "Iteration 33\n",
      "Likelihood -2375918.4803191586\n",
      "Iteration 34\n",
      "Likelihood -2359286.554103561\n",
      "Iteration 35\n",
      "Likelihood -2342465.3769728467\n",
      "Iteration 36\n",
      "Likelihood -2329205.047772394\n",
      "Iteration 37\n",
      "Likelihood -2314421.3631435214\n",
      "Iteration 38\n",
      "Likelihood -2299782.7761605843\n",
      "Iteration 39\n",
      "Likelihood -2285353.490445708\n",
      "Iteration 40\n",
      "Likelihood -2270530.2172488216\n",
      "Iteration 41\n",
      "Likelihood -2258685.8477638406\n",
      "Iteration 42\n",
      "Likelihood -2245040.459164652\n",
      "Iteration 43\n",
      "Likelihood -2234850.1218590806\n",
      "Iteration 44\n",
      "Likelihood -2220646.516050772\n",
      "Iteration 45\n",
      "Likelihood -2207891.704901442\n",
      "Iteration 46\n",
      "Likelihood -2195845.058847567\n",
      "Iteration 47\n",
      "Likelihood -2183182.2685084534\n",
      "Iteration 48\n",
      "Likelihood -2172172.5708800154\n",
      "Iteration 49\n",
      "Likelihood -2160225.287891804\n",
      "Iteration 50\n",
      "Likelihood -2147360.4192784205\n",
      "Iteration 51\n",
      "Likelihood -2135322.1388939447\n",
      "Iteration 52\n",
      "Likelihood -2124134.529564639\n",
      "Iteration 53\n",
      "Likelihood -2113428.705848001\n",
      "Iteration 54\n",
      "Likelihood -2104655.9810941527\n",
      "Iteration 55\n",
      "Likelihood -2095446.9761939647\n",
      "Iteration 56\n",
      "Likelihood -2087108.7770289255\n",
      "Iteration 57\n",
      "Likelihood -2074896.635397904\n",
      "Iteration 58\n",
      "Likelihood -2063918.2785576207\n",
      "Iteration 59\n",
      "Likelihood -2053715.4571151398\n",
      "Iteration 60\n",
      "Likelihood -2044491.5850788588\n",
      "Iteration 61\n",
      "Likelihood -2034991.4499063606\n",
      "Iteration 62\n",
      "Likelihood -2026173.4258041473\n",
      "Iteration 63\n",
      "Likelihood -2016094.7614729493\n",
      "Iteration 64\n",
      "Likelihood -2003865.0313973096\n",
      "Iteration 65\n",
      "Likelihood -1995014.7718040494\n",
      "Iteration 66\n",
      "Likelihood -1986023.2546985839\n",
      "Iteration 67\n",
      "Likelihood -1979147.1728910392\n",
      "Iteration 68\n",
      "Likelihood -1969635.801285448\n",
      "Iteration 69\n",
      "Likelihood -1960645.6250680787\n",
      "Iteration 70\n",
      "Likelihood -1952495.1445283345\n",
      "Iteration 71\n",
      "Likelihood -1943064.8423143427\n",
      "Iteration 72\n",
      "Likelihood -1935117.8483428485\n",
      "Iteration 73\n",
      "Likelihood -1927861.2471935032\n",
      "Iteration 74\n",
      "Likelihood -1919416.4763603983\n",
      "Elapsed time:  2669.3952128887177\n"
     ]
    }
   ],
   "source": [
    "n_topics = 50\n",
    "\n",
    "word_topic_prob_001_50 = LDA_Gibbs_Sampler(matrix, voc_size, n_docs, n_topics, max_iterations, alpha = 0.01, beta = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['great' 'much' 'way' 'must' 'long' 'series' 'esk' 'put' 'want' 'could']\n",
      " ['story' 'books' 'read' 'first' 'daughter' 'us' 'period' 'however'\n",
      "  'times' 'writer']\n",
      " ['one' 'also' 'give' 'first' 'review' 'knowledge' 'need' 'business'\n",
      "  'serious' 'bring']\n",
      " ['history' 'students' 'c' 'second' 'years' 'journey' 'student' 'ago'\n",
      "  'long' 'fantastic']\n",
      " ['book' 'war' 'author' 'getting' 'later' 'third' 'particularly' 'though'\n",
      "  'example' 'library']\n",
      " ['things' 'helped' 'days' 'practical' '&' 'picture' 'understand' 'liked'\n",
      "  'plan' 'lessons']\n",
      " ['one' 'writing' 'could' 'written' 'women' 'recommended' 'recommend'\n",
      "  'patterns' 'school' 'would']\n",
      " ['excellent' 'highly' 'different' 'anyone' 'tale' 'works' 'kind' 'angel'\n",
      "  'classic' 'good']\n",
      " ['people' 'read' 'really' 'experience' 'rest' 'shows' 'society'\n",
      "  'written' 'myth' 'engineers']\n",
      " ['gibbon' 'work' 'could' 'mother' 'going' 'reading' 'fact' 'time' 'new'\n",
      "  'perhaps']\n",
      " ['children' 'life' 'human' 'freddie' 'experience' 'helga' 'soul' 'care'\n",
      "  'spiritual' 'personal']\n",
      " ['sam' 'also' 'life' 'first' 'much' 'say' 'good' 'well' 'set' 'man']\n",
      " ['book' 'way' 'found' 'life' 'learn' '*' 'may' 'real' 'night' 'made']\n",
      " ['recipes' 'would' 'job' 'say' 'science' \"'ve\" 'looking' 'interested'\n",
      "  'common' 'page']\n",
      " ['great' 'reading' 'best' 'say' 'detail' 'also' 'something' 'works'\n",
      "  'enjoyed' 'writes']\n",
      " ['book' 'read' 'us' 'found' 'ending' 'loved' 'still' 'end' 'enough'\n",
      "  'reader']\n",
      " ['get' 'make' 'like' 'read' 'although' 'years' 'points' 'good' 'highly'\n",
      "  'reading']\n",
      " ['read' 'like' 'one' 'way' 'world' 'know' 'two' \"'ll\" 'well' 'even']\n",
      " ['read' 'know' 'good' 'really' 'interesting' 'person' 'someone' 'says'\n",
      "  'story' 'group']\n",
      " ['book' 'chapter' 'one' 'great' 'find' 'us' 'using' 'home' 'must' 'many']\n",
      " ['gives' 'ca' 'right' 'things' 'long' 'entire' 'information' 'extremely'\n",
      "  'seemed' 'job']\n",
      " ['style' 'sense' 'seen' 'anything' 'look' 'stories' 'see' 'recipe'\n",
      "  'ever' 'content']\n",
      " ['author' 'think' 'time' 'theory' 'see' 'one' 'people' 'comes' 'best'\n",
      "  'ideas']\n",
      " ['darwin' 'would' 'first' 'buy' 'us' 'students' 'information' 'problems'\n",
      "  'want' 'hear']\n",
      " ['novel' 'school' 'many' 'people' 'young' 'writing' 'life' 'rather'\n",
      "  'novels' 'women']\n",
      " ['two' 'best' 'story' 'well' 'characters' 'positive' 'stories' 'today'\n",
      "  'woman' 'three']\n",
      " ['book' 'good' 'would' 'see' 'recommend' 'anyone' 'way' 'order' 'also'\n",
      "  'women']\n",
      " ['book' 'power' 'best' 'new' 'father' 'america' 'like' 'strong' 'feel'\n",
      "  'else']\n",
      " ['find' 'wonderful' 'also' 'best' 'bad' 'think' 'started' 'write' '&'\n",
      "  'liked']\n",
      " ['one' 'also' 'would' 'mr.' 'people' 'many' 'work' 'real' 'better'\n",
      "  'series']\n",
      " ['art' 'recommend' 'work' 'relationship' 'really' 'grisham' 'course'\n",
      "  'got' 'customer' 'town']\n",
      " ['writing' 'love' 'great' 'recommend' 'useful' 'new' 'style' 'format'\n",
      "  'action' 'filled']\n",
      " ['history' 'life' 'play' 'world' 'years' \"o'neill\" 'story' 'mathematics'\n",
      "  'works' 'due']\n",
      " ['book' 'many' 'even' 'find' 'world' 'want' 'case' 'see' 'simple'\n",
      "  'loves']\n",
      " ['think' 'still' 'would' 'even' 'love' 'read' 'takes' 'going' 'change'\n",
      "  'buy']\n",
      " ['time' 'horse' 'even' 'part' 'wanted' 'friend' '1' 'may' 'us'\n",
      "  'mccormicks']\n",
      " ['named' 'old' 'research' 'age' 'studies' 'boy' 'page' 'pearl'\n",
      "  'including' 'man']\n",
      " ['read' 'well' 'people' 'written' 'could' 'problems' 'life' 'story'\n",
      "  'student' 'lot']\n",
      " ['every' 'new' 'good' 'much' 'world' 'books' 'understand' \"'re\" 'making'\n",
      "  'work']\n",
      " ['well' 'written' 'lives' 'anyone' 'steve' 'new' 'first' 'world' 'would'\n",
      "  'like']\n",
      " ['book' 'read' 'never' 'used' 'many' 'reading' 'help' 'beautiful'\n",
      "  'continue' 'earth']\n",
      " ['book' 'read' 'mcginnis' 'good' 'information' 'another' 'question'\n",
      "  'every' 'view' 'infiltration']\n",
      " ['books' 'book' 'would' 'subject' \"'ve\" 'fat' 'right' 'life' 'little'\n",
      "  'found']\n",
      " ['never' 'even' '[...]' 'form' 'woman' 'edition' 'reading' 'instead'\n",
      "  'use' 'language']\n",
      " ['family' 'books' 'though' 'modern' 'william' 'could' 'gaskell' 'police'\n",
      "  'opinion' 'fact']\n",
      " ['books' 'book' 'world' 'one' 'great' 'start' 'year' 'name' 'back'\n",
      "  'reading']\n",
      " ['stories' 'could' 'water' 'series' 'never' 'process' 'provides'\n",
      "  'author' 'influence' 'great']\n",
      " ['read' 'lot' 'reader' 'story' 'easy' 'tell' 'interesting' 'time'\n",
      "  'whole' 'high']\n",
      " ['pages' 'would' 'reading' 'john' 'bob' 'like' 'two' 'really' 'first'\n",
      "  \"'re\"]\n",
      " ['dr.' 'love' 'america' 'good' 'years' 'american' 'never' 'shows'\n",
      "  'fascinating' 'modern']]\n"
     ]
    }
   ],
   "source": [
    "show_words_by_topic(word_topic_prob_001_50, vocabulary, typical_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"nzw_01_10.txt\",\"w\") \n",
    "file.write(str(word_topic_prob_01_10))\n",
    "file.close()\n",
    "\n",
    "file = open(\"nzw_001_10.txt\",\"w\") \n",
    "file.write(str(word_topic_prob_001_10))\n",
    "file.close()\n",
    "\n",
    "file = open(\"nzw_01_50.txt\",\"w\") \n",
    "file.write(str(word_topic_prob_01_50))\n",
    "file.close()\n",
    "\n",
    "file = open(\"nzw_001_50.txt\",\"w\") \n",
    "file.write(str(word_topic_prob_001_50))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_topic_prob_001_50[0, 0]\n",
    "\n",
    "np.savetxt(\"nzw_01_10.csv\", word_topic_prob_01_10, delimiter=\",\")\n",
    "np.savetxt(\"nzw_001_50.csv\", word_topic_prob_001_10, delimiter=\",\")\n",
    "np.savetxt(\"nzw_001_50.csv\", word_topic_prob_01_50, delimiter=\",\")\n",
    "np.savetxt(\"nzw_001_50.csv\", word_topic_prob_001_50, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object oriented in Python - would be less messy to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person:\n",
    "    \n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "    \n",
    "    def greeting(self):\n",
    "        print(\"Hello, my name is \" + self.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = Person(\"Sara\", 25)\n",
    "p1.age\n",
    "\n",
    "p1.greeting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Write your own code for doing Gibbs samling on Bigram LDA\n",
    "\n",
    "as in H. M. Wallach: Topic modeling: beyond bag-of-words. ICML(2006) 977-984. http://dirichlet.net/pdf/wallach06topic.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For corpus use the Amazon book reviews corpus that you also used in Assignment 1. You may have to use only a subset of the documents. A corpus of 100 000 tokens is sufficients size.\n",
    "\n",
    "Run this for different hyperparameters. For LDA you can try α=β=0.1 and α=β=0.01. (A cross-validation search for optimal values will probably be too slow.) Run also for different numbers of topics, e.g. K=10 and K=50.\n",
    "\n",
    "For the bigram model, see to that you use a larger hyperparameter value on the diagonal of the transition matrix over the topics. Since each document in this model has a transition matrix over topics rather than just a probability distribution, the number of topics cannot be as large as for LDA. Try K=5 and K=10."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
