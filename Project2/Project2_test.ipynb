{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import time\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from numpy import random as rd\n",
    "from scipy.special import gammaln\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## File related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def find_n_lines(filename, encoding, n_tokens):\n",
    "    \"\"\"\n",
    "    We should train on a subset of the corpus, with 100 0000 tokens. Find how many lines this corresponds to.\n",
    "    \"\"\"\n",
    "    \n",
    "    line_counter = 0\n",
    "    token_counter = 0\n",
    "\n",
    "    with open(filename, encoding = encoding) as f:\n",
    "        for line in f:\n",
    "\n",
    "            line_counter += 1\n",
    "            tokens = line.lower().split()\n",
    "\n",
    "            for token in tokens:\n",
    "                token_counter += 1\n",
    "\n",
    "            if token_counter > n_tokens:\n",
    "                break\n",
    "\n",
    "    return line_counter\n",
    "\n",
    "\n",
    "def count_word_frequencies(filename, encoding, n_lines, ignore_list):\n",
    "    \n",
    "    freqs = Counter()\n",
    "    with open(filename, encoding = encoding) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            \n",
    "            tokens = line.lower().split()\n",
    "            for token in tokens:\n",
    "                if token not in ignore_list:\n",
    "                    freqs[token] += 1\n",
    "                \n",
    "            if i == n_lines:\n",
    "                break\n",
    "                \n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def list_of_stopwords():\n",
    "    # Ignore all stopwords in the text!\n",
    "    ignore_words = stopwords.words('english')\n",
    "    also_ignore = [\",\", \".\", '\"', \"(\", \")\", \"-\", \"'\", \"!\", \"?\", \":\", \";\", \"/\", \"n't\", \"'s\", \"'m\"]\n",
    "\n",
    "    for item in also_ignore:\n",
    "        ignore_words.append(item)\n",
    "        \n",
    "    return ignore_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Batch related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_integer_vocabulary(word_freqs, max_voc_size):\n",
    "    \n",
    "    \"\"\" \n",
    "    Create vocabulary where common words are matched to integers. \n",
    "    \"\"\"\n",
    "    \n",
    "    word_list = []\n",
    "\n",
    "    if len(word_freqs.most_common()) > max_voc_size:\n",
    "        vocab = word_freqs.most_common()[0:max_voc_size]\n",
    "\n",
    "    else:\n",
    "        vocab = word_freqs.most_common()\n",
    "\n",
    "    for i in range(len(vocab)):\n",
    "        word_list.append(vocab[i][0])\n",
    "\n",
    "    # Get pairs of elements    \n",
    "    tmp = zip(word_list, range(1,max_voc_size+1))\n",
    "    # Make pairs into a dictionary\n",
    "    vocab = dict(tmp)\n",
    "\n",
    "    # Create default dictionary - returns 0 if an undefined key is called\n",
    "    vocab2 = defaultdict(int)\n",
    "    vocab2.update(vocab)\n",
    "    \n",
    "    return vocab2\n",
    "\n",
    "def find_batch_dimensions(batch_size, filename, ENCODING):\n",
    "    \"\"\"\n",
    "    Find the length of the longest line in each batch.\n",
    "    \"\"\"\n",
    "    \n",
    "    counter = 0      # will end up being the number of lines in the document\n",
    "    len_lines = []   # will contain maximum length of a line in each batch\n",
    "    tmp_lines = []\n",
    "    \n",
    "    with open(filename, encoding=ENCODING) as f:\n",
    "        for line in f:\n",
    "            counter+=1\n",
    "            tokens = line.lower().split()\n",
    "            tmp_lines.append(len(tokens))\n",
    "\n",
    "            if (counter % batch_size == 0):\n",
    "                len_lines.append(max(tmp_lines))\n",
    "                tmp_lines = []\n",
    "                \n",
    "        #This takes care of the last batch if number of lines is not an exact multiple of batch_size\n",
    "        if (counter % batch_size != 0): \n",
    "            len_lines.append(max(tmp_lines)) # if at end of the file\n",
    "            \n",
    "    return counter, len_lines\n",
    "    \n",
    "\n",
    "def create_batches(batch_size, vocabulary, filename, ENCODING):\n",
    "    \"\"\"\n",
    "    Splits the file into batches of a specified size, and transforms common words to integers.\n",
    "    The batches are outputted in a numpy array padded with zeros. Words not in the vocabulary are set to -1.\n",
    "    \"\"\"\n",
    "    \n",
    "    counter, len_lines = find_batch_dimensions(batch_size, filename, ENCODING)\n",
    "    \n",
    "    with open(filename, encoding=ENCODING) as f:\n",
    "        batches=[]\n",
    "        batch_counter=0\n",
    "        line_counter=0\n",
    "\n",
    "        for line in f:\n",
    "            #This creates a temporary array each time we start a new batch\n",
    "            if line_counter % batch_size == 0:\n",
    "                tmp_array=np.zeros(shape=(batch_size,len_lines[batch_counter])) #fill this temporary array\n",
    "\n",
    "            tokens = line.lower().split()\n",
    "            line_as_int = list(map(vocabulary.get, tokens))\n",
    "            line_as_int = [-1 if x is None else x for x in line_as_int] # set None values to -1\n",
    "\n",
    "            tmp_array[line_counter % batch_size,0:(len(line_as_int))]=line_as_int\n",
    "\n",
    "            line_counter+=1 #when we done\n",
    "            if line_counter % batch_size ==0:\n",
    "                batches.append(tmp_array)\n",
    "                batch_counter+=1\n",
    "\n",
    "        # again this takes care of the final batch if number of lines is not multiple of batch_size\n",
    "        if line_counter % batch_size != 0:\n",
    "            tmp_array=tmp_array[0:(line_counter % batch_size),:]\n",
    "            batches.append(tmp_array)\n",
    "        \n",
    "    return(counter, batches)\n",
    "\n",
    "def get_matrix(filename, encoding, n_tokens, ignore_words):\n",
    "    # Find how many lines we need to read to get the desired number of tokens\n",
    "    n_docs = find_n_lines(filename, encoding, n_tokens)\n",
    "\n",
    "    # Count word frequencies in this subset of the file\n",
    "    word_frequencies = count_word_frequencies(filename, encoding, n_docs, ignore_words)\n",
    "\n",
    "    # Create an integer vocabulary. Don't remove any words from the vocabulary.\n",
    "    voc_size = len(word_frequencies)\n",
    "    vocabulary = create_integer_vocabulary(word_frequencies, voc_size)\n",
    "\n",
    "    # Turn the document into batches\n",
    "    lines, batches = create_batches(batch_size=n_docs, vocabulary = vocabulary, filename = filename, ENCODING = encoding)\n",
    "\n",
    "    # Save only the first batch - this is what we'll analyse\n",
    "    matrix = batches[0].astype(int)\n",
    "    return n_docs, matrix, vocabulary, voc_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## LDA Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def initialise_everything(n_docs, n_topics, voc_size, int_matrix):\n",
    "    \"\"\"\n",
    "    Initialise the things we need for LDA.\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of times that we observe topic z in document d\n",
    "    ndz = np.zeros((n_docs, n_topics))\n",
    "\n",
    "    # Number of times that we observe word w in topic z\n",
    "    nzw = np.zeros((n_topics, voc_size))\n",
    "\n",
    "    # Counters for documents and topics\n",
    "    nd = np.zeros(n_docs)\n",
    "    nz = np.zeros(n_topics)\n",
    "\n",
    "    # Create dictionary of topics\n",
    "    topics = {}\n",
    "\n",
    "    # iterate over documents \n",
    "    for d in range(n_docs):\n",
    "\n",
    "        # i is the index of the word in the document\n",
    "        # w is the numerical representation of the word\n",
    "        for i, w in enumerate(int_matrix[d]):\n",
    "\n",
    "            # Initialise with a random topic\n",
    "            z = rd.randint(n_topics)\n",
    "            topics[(d,i)] = z\n",
    "\n",
    "            # Increase counters\n",
    "            ndz[d, z] += 1\n",
    "            nzw[z, w] += 1\n",
    "\n",
    "            nd[d] += 1\n",
    "            nz[z] += 1\n",
    "\n",
    "    return topics, ndz, nzw, nd, nz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cond_topic_prob(ndz, nzw, nz, nd, w, d, alpha, beta, n_topics):\n",
    "    \"\"\"\n",
    "    Conditional probability of topics. \n",
    "    \"\"\"\n",
    "\n",
    "    left = (nzw[:,w] + beta) / (nz + beta * voc_size)\n",
    "    right = (ndz[d,:] + alpha) / (nd[d] + alpha * n_topics)\n",
    "\n",
    "    p_z = left * right\n",
    "    p_z /= np.sum(p_z)\n",
    "    \n",
    "    return p_z\n",
    "\n",
    "def log_multinomial_beta(alpha, K=None):\n",
    "\n",
    "    if K is None:\n",
    "        # alpha is assumed to be a vector\n",
    "        return np.sum(gammaln(alpha)) - gammaln(np.sum(alpha))\n",
    "    else:\n",
    "        # alpha is assumed to be a scalar\n",
    "        return K * gammaln(alpha) - gammaln(K*alpha)\n",
    "\n",
    "# This should increase as training progresses, show it every few training iterations (?)\n",
    "def loglikelihood(n_topics, voc_size, alpha, beta, nzw, ndz):\n",
    "    likelihood = 0\n",
    "    \n",
    "    for z in range(n_topics):\n",
    "        likelihood += log_multinomial_beta(nzw[z,:] + beta)\n",
    "        likelihood -= log_multinomial_beta(beta, voc_size)\n",
    "        \n",
    "    for d in range(n_docs):\n",
    "        likelihood += log_multinomial_beta(ndz[d,:] + alpha)\n",
    "        likelihood -= log_multinomial_beta(alpha, n_topics)\n",
    "        \n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def LDA_Gibbs_Sampler(matrix, voc_size, n_docs, n_topics, max_iterations, alpha, beta):\n",
    "\n",
    "    start_time = time.time()\n",
    "    topics, ndz, nzw, nd, nz = initialise_everything(n_docs, n_topics, voc_size, matrix)\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        for d in range(n_docs):\n",
    "             for j, w in enumerate(matrix[d]):\n",
    "\n",
    "                z = topics[(d, j)]\n",
    "                ndz[d, z] -= 1\n",
    "                nzw[z, w] -= 1\n",
    "                nd[d] -= 1\n",
    "                nz[z] -= 1\n",
    "\n",
    "                p_z = cond_topic_prob(ndz, nzw, nz, nd, w, d, alpha, beta, n_topics)\n",
    "                z = rd.multinomial(1, p_z).argmax()\n",
    "\n",
    "                ndz[d,z] += 1\n",
    "                nzw[z,w] += 1\n",
    "                nd[d] += 1\n",
    "                nz[z] += 1\n",
    "                topics[(d, j)] = z\n",
    "\n",
    "        print(\"Iteration\", i)\n",
    "        print(\"Likelihood\", loglikelihood(n_topics, voc_size, alpha, beta, nzw, ndz))\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"Elapsed time: \", elapsed_time)\n",
    "    \n",
    "    return nzw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show_words_by_topic(word_topic_prob, vocabulary, typical_len):\n",
    "    \n",
    "    n_topics = word_topic_prob.shape[0]\n",
    "    typical_words = []\n",
    "\n",
    "    for i in range(n_topics):\n",
    "        arr = word_topic_prob[i,:]\n",
    "        typical_ints = arr.argsort()[-typical_len-2:-2][::-1]   # there's some funny business with the last word in vocab\n",
    "        #print(typical_ints)\n",
    "\n",
    "        for search_int in typical_ints:\n",
    "            if search_int in [0, -1]:\n",
    "                typical_words.append(\"\")\n",
    "            else:\n",
    "                for k, v in vocabulary.items(): \n",
    "                    if v == search_int:\n",
    "                        typical_words.append(k)\n",
    "                        break\n",
    "\n",
    "    # Print the most common words in each topic\n",
    "    typical_words = np.reshape(typical_words, [n_topics, -1])\n",
    "    print(typical_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 1: Write your own code for doing Gibbs sampling for LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The initial results from this algorithm were just nonsense (all topics mostly consisted of stopwords and grammatical symbols). This is why stopwords and grammatical symbols have been ignored in the vocabulary. \n",
    "\n",
    "The data matrix uses zero padding, which due to an oversight was not taken into account in this implementation. Thus there is an issue with the conditional probabilites; the number of words ends up being the same for each document and also, the counts for different topics is biased. However these issues appear to not have affected the end result too much, as reasonable looking results are obtained. \n",
    "\n",
    "Collapsed Gibbs sampling for LDA has been implemented, and the algorithm is tested on a subset of the Amazon book review corpus consisting of 80 000 tokens. The algorithm runs for 75 iterations and the log-likelihood is outputted at the end of each iteration. We tried using 10 and 50 number of topics, both with $\\alpha = \\beta = 0.1$ and $\\alpha = \\beta = 0.01$. Finally, the 10 most common words for each topic are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = \"books.txt\" \n",
    "encoding = \"ISO-8859-1\"\n",
    "n_tokens = 8*10**4\n",
    "\n",
    "ignore_words = list_of_stopwords()\n",
    "\n",
    "n_docs, matrix, vocabulary, voc_size = get_matrix(filename, encoding, n_tokens, ignore_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "typical_len = 10\n",
    "max_iterations = 75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 10 Topics, $\\alpha = \\beta = 0.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Likelihood -2351002.5387130915\n",
      "Iteration 1\n",
      "Likelihood -2335354.9581859065\n",
      "Iteration 2\n",
      "Likelihood -2323639.282311263\n",
      "Iteration 3\n",
      "Likelihood -2311577.561338965\n",
      "Iteration 4\n",
      "Likelihood -2296861.8465751046\n",
      "Iteration 5\n",
      "Likelihood -2276834.275514169\n",
      "Iteration 6\n",
      "Likelihood -2243067.8784760805\n",
      "Iteration 7\n",
      "Likelihood -2199891.9970693695\n",
      "Iteration 8\n",
      "Likelihood -2156739.2925651036\n",
      "Iteration 9\n",
      "Likelihood -2123050.5709214304\n",
      "Iteration 10\n",
      "Likelihood -2098328.7566538285\n",
      "Iteration 11\n",
      "Likelihood -2079706.214526914\n",
      "Iteration 12\n",
      "Likelihood -2065564.3258698783\n",
      "Iteration 13\n",
      "Likelihood -2052815.0407773168\n",
      "Iteration 14\n",
      "Likelihood -2041726.5628088245\n",
      "Iteration 15\n",
      "Likelihood -2028867.4095321808\n",
      "Iteration 16\n",
      "Likelihood -2017191.9350128223\n",
      "Iteration 17\n",
      "Likelihood -2005201.353564023\n",
      "Iteration 18\n",
      "Likelihood -1994140.9965495174\n",
      "Iteration 19\n",
      "Likelihood -1982265.8389743764\n",
      "Iteration 20\n",
      "Likelihood -1972151.617760897\n",
      "Iteration 21\n",
      "Likelihood -1961846.7944976867\n",
      "Iteration 22\n",
      "Likelihood -1952392.4721891806\n",
      "Iteration 23\n",
      "Likelihood -1944846.7732122233\n",
      "Iteration 24\n",
      "Likelihood -1935353.1113976575\n",
      "Iteration 25\n",
      "Likelihood -1925626.0006306628\n",
      "Iteration 26\n",
      "Likelihood -1916391.7934682453\n",
      "Iteration 27\n",
      "Likelihood -1908160.9043778633\n",
      "Iteration 28\n",
      "Likelihood -1899935.4038577112\n",
      "Iteration 29\n",
      "Likelihood -1891813.246901635\n",
      "Iteration 30\n",
      "Likelihood -1883952.5421948629\n",
      "Iteration 31\n",
      "Likelihood -1875400.224983798\n",
      "Iteration 32\n",
      "Likelihood -1865642.4329339268\n",
      "Iteration 33\n",
      "Likelihood -1856278.541152564\n",
      "Iteration 34\n",
      "Likelihood -1847291.3989991357\n",
      "Iteration 35\n",
      "Likelihood -1839476.8535994103\n",
      "Iteration 36\n",
      "Likelihood -1829911.9761416817\n",
      "Iteration 37\n",
      "Likelihood -1823324.552146303\n",
      "Iteration 38\n",
      "Likelihood -1814509.242381221\n",
      "Iteration 39\n",
      "Likelihood -1807240.6200482773\n",
      "Iteration 40\n",
      "Likelihood -1800257.862002501\n",
      "Iteration 41\n",
      "Likelihood -1792091.122316935\n",
      "Iteration 42\n",
      "Likelihood -1784313.259089396\n",
      "Iteration 43\n",
      "Likelihood -1776460.0360361072\n",
      "Iteration 44\n",
      "Likelihood -1769631.898562565\n",
      "Iteration 45\n",
      "Likelihood -1760317.019874958\n",
      "Iteration 46\n",
      "Likelihood -1755180.3753856407\n",
      "Iteration 47\n",
      "Likelihood -1749576.6849245445\n",
      "Iteration 48\n",
      "Likelihood -1742592.2426720897\n",
      "Iteration 49\n",
      "Likelihood -1735232.9461073051\n",
      "Iteration 50\n",
      "Likelihood -1726859.6143943823\n",
      "Iteration 51\n",
      "Likelihood -1719937.7392624826\n",
      "Iteration 52\n",
      "Likelihood -1712426.6210142719\n",
      "Iteration 53\n",
      "Likelihood -1704059.0160221732\n",
      "Iteration 54\n",
      "Likelihood -1695166.365664298\n",
      "Iteration 55\n",
      "Likelihood -1687976.4787738863\n",
      "Iteration 56\n",
      "Likelihood -1679418.412767039\n",
      "Iteration 57\n",
      "Likelihood -1671956.4309965533\n",
      "Iteration 58\n",
      "Likelihood -1664015.7171239078\n",
      "Iteration 59\n",
      "Likelihood -1656156.592506969\n",
      "Iteration 60\n",
      "Likelihood -1650651.7341271457\n",
      "Iteration 61\n",
      "Likelihood -1643662.7888603758\n",
      "Iteration 62\n",
      "Likelihood -1637755.2279101512\n",
      "Iteration 63\n",
      "Likelihood -1631404.285282863\n",
      "Iteration 64\n",
      "Likelihood -1624769.5442679944\n",
      "Iteration 65\n",
      "Likelihood -1619872.8342499165\n",
      "Iteration 66\n",
      "Likelihood -1615471.4221169576\n",
      "Iteration 67\n",
      "Likelihood -1610779.042299306\n",
      "Iteration 68\n",
      "Likelihood -1602479.3071940173\n",
      "Iteration 69\n",
      "Likelihood -1596595.9361555919\n",
      "Iteration 70\n",
      "Likelihood -1592045.3785948795\n",
      "Iteration 71\n",
      "Likelihood -1587022.1462863332\n",
      "Iteration 72\n",
      "Likelihood -1581470.1369899951\n",
      "Iteration 73\n",
      "Likelihood -1575225.3636989375\n",
      "Iteration 74\n",
      "Likelihood -1568555.8513864744\n",
      "Elapsed time:  2739.564166545868\n"
     ]
    }
   ],
   "source": [
    "n_topics = 10\n",
    "\n",
    "word_topic_prob_01_10 = LDA_Gibbs_Sampler(matrix, voc_size, n_docs, n_topics, \\\n",
    "                                          max_iterations, alpha = 0.1, beta = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['book' 'read' 'would' 'well' 'good' 'know' 'us' 'two' 'think' 'highly']\n",
      " ['read' 'one' 'books' 'people' 'world' 'man' 'works' 'time' 'new'\n",
      "  'women']\n",
      " ['book' 'like' 'reading' 'would' 'new' 'much' 'lot' 'school' 'long'\n",
      "  'ever']\n",
      " ['book' 'books' 'one' 'many' 'history' 'way' 'reading' 'life' 'people'\n",
      "  'work']\n",
      " ['great' 'story' 'like' 'many' 'read' 'world' 'see' 'also' 'novel'\n",
      "  'john']\n",
      " ['one' 'much' 'would' 'also' 'even' 'like' 'sam' 'get' 'time' 'could']\n",
      " ['great' 'reader' 'writing' 'love' 'must' 'different' 'information'\n",
      "  'give' 'patterns' 'could']\n",
      " ['book' 'recommend' 'read' 'anyone' 'great' 'story' 'going'\n",
      "  'interesting' 'information' 'would']\n",
      " ['book' 'one' 'good' 'first' 'really' 'best' 'time' 'find' 'work' 'get']\n",
      " ['life' 'war' 'new' 'family' 'us' '' 'without' 'theory' 'power'\n",
      "  'american']]\n"
     ]
    }
   ],
   "source": [
    "show_words_by_topic(word_topic_prob_01_10, vocabulary, typical_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Of the above topics, three appear to make sense:\n",
    "\n",
    "- __\"People\" topic__: ['read' 'one' 'books' 'people' 'world' 'man' 'works' 'time' 'new'\n",
    "  'women']\n",
    "  \n",
    "- __\"Positive\" topic__:  ['book' 'recommend' 'read' 'anyone' 'great' 'story' 'going'\n",
    "  'interesting' 'information' 'would']\n",
    "  \n",
    "- __\"Serious\" topic__: ['life' 'war' 'new' 'family' 'us' '' 'without' 'theory' 'power'\n",
    "  'american']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 50 Topics, $\\alpha = \\beta = 0.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Likelihood -3658563.936163828\n",
      "Iteration 1\n",
      "Likelihood -3608550.923346697\n",
      "Iteration 2\n",
      "Likelihood -3558537.080710233\n",
      "Iteration 3\n",
      "Likelihood -3508411.4883670104\n",
      "Iteration 4\n",
      "Likelihood -3449413.682229062\n",
      "Iteration 5\n",
      "Likelihood -3379673.1660486497\n",
      "Iteration 6\n",
      "Likelihood -3301858.0268891277\n",
      "Iteration 7\n",
      "Likelihood -3230342.4180519395\n",
      "Iteration 8\n",
      "Likelihood -3169399.4883624515\n",
      "Iteration 9\n",
      "Likelihood -3119861.807302928\n",
      "Iteration 10\n",
      "Likelihood -3077177.2423457718\n",
      "Iteration 11\n",
      "Likelihood -3035792.7022777093\n",
      "Iteration 12\n",
      "Likelihood -2998331.9405336394\n",
      "Iteration 13\n",
      "Likelihood -2963537.9445924326\n",
      "Iteration 14\n",
      "Likelihood -2929729.842392472\n",
      "Iteration 15\n",
      "Likelihood -2893592.647204069\n",
      "Iteration 16\n",
      "Likelihood -2859870.2313416135\n",
      "Iteration 17\n",
      "Likelihood -2825669.469561156\n",
      "Iteration 18\n",
      "Likelihood -2787576.176527688\n",
      "Iteration 19\n",
      "Likelihood -2750477.565235863\n",
      "Iteration 20\n",
      "Likelihood -2710736.847433333\n",
      "Iteration 21\n",
      "Likelihood -2672623.207860701\n",
      "Iteration 22\n",
      "Likelihood -2631332.23425144\n",
      "Iteration 23\n",
      "Likelihood -2591140.8551768283\n",
      "Iteration 24\n",
      "Likelihood -2546170.545524004\n",
      "Iteration 25\n",
      "Likelihood -2506323.0444725617\n",
      "Iteration 26\n",
      "Likelihood -2461583.32617288\n",
      "Iteration 27\n",
      "Likelihood -2420832.9895652807\n",
      "Iteration 28\n",
      "Likelihood -2378324.2028796324\n",
      "Iteration 29\n",
      "Likelihood -2336240.434196245\n",
      "Iteration 30\n",
      "Likelihood -2297328.1302070436\n",
      "Iteration 31\n",
      "Likelihood -2258799.9716578177\n",
      "Iteration 32\n",
      "Likelihood -2222347.0392818223\n",
      "Iteration 33\n",
      "Likelihood -2183886.062103875\n",
      "Iteration 34\n",
      "Likelihood -2146456.0306069\n",
      "Iteration 35\n",
      "Likelihood -2111927.1178290155\n",
      "Iteration 36\n",
      "Likelihood -2078682.923163468\n",
      "Iteration 37\n",
      "Likelihood -2048748.7065464582\n",
      "Iteration 38\n",
      "Likelihood -2019318.2740013609\n",
      "Iteration 39\n",
      "Likelihood -1991981.9631354662\n",
      "Iteration 40\n",
      "Likelihood -1962048.3421471082\n",
      "Iteration 41\n",
      "Likelihood -1935107.7244747356\n",
      "Iteration 42\n",
      "Likelihood -1907452.8251598494\n",
      "Iteration 43\n",
      "Likelihood -1882947.5428808474\n",
      "Iteration 44\n",
      "Likelihood -1858693.2111342698\n",
      "Iteration 45\n",
      "Likelihood -1838283.713256274\n",
      "Iteration 46\n",
      "Likelihood -1818834.5177367944\n",
      "Iteration 47\n",
      "Likelihood -1799687.6417852526\n",
      "Iteration 48\n",
      "Likelihood -1779021.8538893692\n",
      "Iteration 49\n",
      "Likelihood -1759955.0784453086\n",
      "Iteration 50\n",
      "Likelihood -1744768.1069291646\n",
      "Iteration 51\n",
      "Likelihood -1729329.3590153533\n",
      "Iteration 52\n",
      "Likelihood -1712715.451558336\n",
      "Iteration 53\n",
      "Likelihood -1698737.3899727883\n",
      "Iteration 54\n",
      "Likelihood -1682730.8633009316\n",
      "Iteration 55\n",
      "Likelihood -1670105.633627229\n",
      "Iteration 56\n",
      "Likelihood -1659206.2204886947\n",
      "Iteration 57\n",
      "Likelihood -1645401.911816334\n",
      "Iteration 58\n",
      "Likelihood -1632437.449932132\n",
      "Iteration 59\n",
      "Likelihood -1619804.0517625138\n",
      "Iteration 60\n",
      "Likelihood -1608221.8091357923\n",
      "Iteration 61\n",
      "Likelihood -1598690.3323695129\n",
      "Iteration 62\n",
      "Likelihood -1586528.5770634182\n",
      "Iteration 63\n",
      "Likelihood -1573254.5693706349\n",
      "Iteration 64\n",
      "Likelihood -1561923.9752862232\n",
      "Iteration 65\n",
      "Likelihood -1549367.5536462818\n",
      "Iteration 66\n",
      "Likelihood -1536995.4209562456\n",
      "Iteration 67\n",
      "Likelihood -1527537.036532702\n",
      "Iteration 68\n",
      "Likelihood -1520283.4736684815\n",
      "Iteration 69\n",
      "Likelihood -1510265.783573851\n",
      "Iteration 70\n",
      "Likelihood -1500548.2010935107\n",
      "Iteration 71\n",
      "Likelihood -1488787.365507433\n",
      "Iteration 72\n",
      "Likelihood -1479519.3660199025\n",
      "Iteration 73\n",
      "Likelihood -1471911.4235978427\n",
      "Iteration 74\n",
      "Likelihood -1461795.8017515158\n",
      "Elapsed time:  2944.3656651973724\n"
     ]
    }
   ],
   "source": [
    "n_topics = 50\n",
    "\n",
    "word_topic_prob_01_50 = LDA_Gibbs_Sampler(matrix, voc_size, n_docs, n_topics, \\\n",
    "                                          max_iterations, alpha = 0.1, beta = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['monster' 'born' 'horrific' 'killed' 'fell' 'girls' 'memories' 'amelia'\n",
      "  'telling' 'actual']\n",
      " ['cooke' 'biography' 'events' 'official' 'version' 'wolff' 'known'\n",
      "  'greene' 'car' 'death']\n",
      " ['edition' 'thorough' 'girlfriend' 'nature' 'exactly' 'medieval' 'vast'\n",
      "  'eugenics' 'exhaustive' 'analyzed']\n",
      " ['soul' 'human' 'xxiii' 'later' 'thomas' 'care' 'ladder' 'window'\n",
      "  'monastic' 'grow']\n",
      " ['book' 'well' 'first' 'characters' 'good' 'story' 'read' 'writing'\n",
      "  'enjoyed' 'amazing']\n",
      " ['politics' 'lbj' 'loved' 'mets' 'scientific' 'myron' 'bolitar' 'kathy'\n",
      "  'years' 'career']\n",
      " ['one' 'like' 'would' 'also' 'read' 'many' 'people' 'life' 'much' 'time']\n",
      " ['energy' 'michelle' 'vital' 'mama' 'vampirism' 'age' 'peter' 'battle'\n",
      "  'explosive' 'hearing']\n",
      " ['arguments' 'department' 'grisham' 'rhetorician' 'thousands' 'original'\n",
      "  'creator' 'zen' 'motorcycle' 'relationship']\n",
      " ['reporter' 'reviewer' '...' 'universe' 'fox' 'mop' 'diverting'\n",
      "  'plodding' 'time' 'background']\n",
      " ['reading' 'would' 'without' 'war' 'say' 'society' 'new' 'big' 'best'\n",
      "  'details']\n",
      " ['carroll' 'mr.' 'hannah' 'stage' 'voice' 'truth' 'graves' 'pretence'\n",
      "  'form' 'imitation']\n",
      " ['great' 'ringo' 'one' 'author' 'page' 'past' 'although' 'must' 'bring'\n",
      "  'last']\n",
      " ['giraut' 'wives' 'daughters' 'modern' 'acupuncture' 'rule' 'mrs'\n",
      "  'tradescant' '19th' 'development']\n",
      " ['adso' 'rose' 'word' 'st.' 'modern' 'scientist' 'thus' 'knowledge'\n",
      "  'bernard' 'science']\n",
      " ['service' 'served' 'evening' 'wine' 'euro' 'italian' 'venice' 'roasted'\n",
      "  'green' 'food']\n",
      " ['read' 'great' 'recommend' 'well' 'love' 'good' 'anyone' 'used' 'time'\n",
      "  'information']\n",
      " ['ramon' 'freeway' 'boy' 'named' 'called' 'show' 'cross' 'paths'\n",
      "  'research' 'sections']\n",
      " ['novels' '13' '100' 'offers' 'gypsies' 'dickens' 'unique' 'gypsy'\n",
      "  'tragic' 'plays']\n",
      " ['book' 'life' 'author' 'one' 'read' 'even' 'world' 'books' 'two' 'many']\n",
      " ['mccormicks' 'path' 'mystical' 'french' 'horse' 'classical'\n",
      "  'experience' 'de' 'celtic' 'carlin']\n",
      " ['role' 'year' 'art' 'act' 'experiencing' 'discovers' 'arguments'\n",
      "  'routinely' 'argues' 'claims']\n",
      " ['hunt' 'respect' 'leo' 'thoughts' 'offers' 'estby' 'mother' 'across'\n",
      "  'choices' 'emotions']\n",
      " ['read' 'story' 'could' 'child' 'know' 'time' 'us' 'going' 'gift'\n",
      "  'world']\n",
      " ['lot' 'matheson' 'meets' 'strong' 'mentioned' 'scene' 'repetition'\n",
      "  'edition' 'internally' 'evil']\n",
      " ['problems' 'bible' 'mathematics' 'johnson' 'exercises' 'dr.' 'students'\n",
      "  'use' 'hal' 'broad']\n",
      " ['recipes' 'library' 'authors' 'booklovers' 'soy' 'hope' 'books'\n",
      "  'recipe' 'town' 'side']\n",
      " ['abduction' 'field' 'hallucinations' 'consciousness' 'darwin'\n",
      "  'alternative' 'evolutionary' 'loves' 'conclusion' 'captures']\n",
      " ['style' 'orleans' 'corps' 'needs' 'south' 'american' 'lies' 'river'\n",
      "  'katrina' 'new']\n",
      " ['abandoned' 'corners' 'valley' 'listen' 'christensen' 'codes' 'hear'\n",
      "  'bill' 'vacation' 'trace']\n",
      " ['book' 'think' 'job' \"'ve\" \"'re\" 'find' 'years' 'edition' 'could'\n",
      "  'written']\n",
      " ['corinthians' 'garland' 'commentary' 'educated' 'us$' 'wrote' 'choice'\n",
      "  'surprised' 'gerald' 'printing']\n",
      " ['eugene' 'family' 'mother' 'blues' 'day' 'attempted' 'house' 'handy'\n",
      "  'addiction' 'edmund']\n",
      " ['memorable' 'surface' 'terrorism' 'laden' 'ali' 'terrorist' 'handbook'\n",
      "  'note' 'johnson' 'parallels']\n",
      " ['limit' 'games' 'carre' \"hold'em\" 'opponents' 'days' 'silly' 'le'\n",
      "  'tailor' 'related']\n",
      " ['drivers' 'humans' 'step' 'license' 'id' 'control' 'wisdom' 'act'\n",
      "  'passport' 'helped']\n",
      " ['book' 'see' 'anyone' 'research' 'men' 'every' 'political' 'rome'\n",
      "  'human' 'time']\n",
      " ['dogs' 'dog' 'pack' 'emergence' 'old' 'rise' 'johnson' 'wolf' 'idea'\n",
      "  'southern']\n",
      " ['clark' 'villagers' 'nietzsche' 'kiss' 'anna' 'exposed' 'anzaldua'\n",
      "  'qualities' '68' 'follows']\n",
      " ['history' 'women' 'one' 'way' 'novel' 'see' 'best' 'world' 'others'\n",
      "  'years']\n",
      " ['mole' 'rat' 'lovable' 'willows' 'wind' 'vain' 'along' 'badger'\n",
      "  'grahame' 'vices']\n",
      " ['contrary' 'essential' 'tremendously' 'keep' 'grand' 'puppy' 'jamaica'\n",
      "  'owe' 'flooded' 'jobless']\n",
      " ['heart' 'heard' 'bugs' 'nfl' 'besides' 'guessing' \"cabinet'\" 'hit'\n",
      "  'solutions' 'owes']\n",
      " ['poet' 'computer' 'chanting' 'practice' 'dialogue' 'odd' 'jones'\n",
      "  'douglas' 'workshop' 'turco']\n",
      " ['companies' 'mountains' 'created' 'happy' 'jobs' 'wage' 'development'\n",
      "  'issue' 'former' 'highest']\n",
      " ['book' 'read' 'great' 'would' 'one' 'really' 'well' 'like' 'good'\n",
      "  'real']\n",
      " ['cultural' 'invent' 'ed.]' 'end-of-work-day' 'meant' 'sorts' 'vibe'\n",
      "  'catholic' 'since' 'interactive']\n",
      " ['book' 'make' 'recommend' 'many' 'looking' 'work' 'want' 'however'\n",
      "  'get' 'given']\n",
      " ['series' 'discworld' 'equal' 'granny' 'magic' 'killer' 'notre' 'dame'\n",
      "  'immigrant' 'witches']\n",
      " ['students' 'text' 'alone' 'common' 'design' 'evolution' 'opinion' 'pay'\n",
      "  'darwin' 'show']]\n"
     ]
    }
   ],
   "source": [
    "show_words_by_topic(word_topic_prob_01_50, vocabulary, typical_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Of the above, 12 topics appear to make sense:\n",
    "\n",
    "- __\"Horror\" topic__: ['monster' 'born' 'horrific' 'killed' 'fell' 'girls' 'memories' 'amelia'\n",
    "  'telling' 'actual']\n",
    "  \n",
    "- __\"Positive\" topic__: ['book' 'well' 'first' 'characters' 'good' 'story' 'read' 'writing'\n",
    "  'enjoyed' 'amazing']\n",
    "  \n",
    "- __\"Myron Bolitar\" topic__: ['politics' 'lbj' 'loved' 'mets' 'scientific' 'myron' 'bolitar' 'kathy'\n",
    "  'years' 'career'] - LBJ = Lyndon B Johnson, 36th president of the USA\n",
    "  \n",
    " \n",
    "- __\"Science\" topic__: ['adso' 'rose' 'word' 'st.' 'modern' 'scientist' 'thus' 'knowledge'\n",
    "  'bernard' 'science']\n",
    "  \n",
    "- __\"Italian restaurant\" topic__: ['service' 'served' 'evening' 'wine' 'euro' 'italian' 'venice' 'roasted'\n",
    "  'green' 'food']\n",
    "  \n",
    "- __\"Alien abduction\" topic__: ['abduction' 'field' 'hallucinations' 'consciousness' 'darwin'\n",
    "  'alternative' 'evolutionary' 'loves' 'conclusion' 'captures']\n",
    " \n",
    " \n",
    "- __\"Hurricane Katrina\" topic__: ['style' 'orleans' 'corps' 'needs' 'south' 'american' 'lies' 'river'\n",
    "  'katrina' 'new']\n",
    "  \n",
    "- __\"Terrorism\" topic__:  ['memorable' 'surface' 'terrorism' 'laden' 'ali' 'terrorist' 'handbook'\n",
    "  'note' 'johnson' 'parallels']\n",
    "  \n",
    "- __\"ID check\" topic__:  ['drivers' 'humans' 'step' 'license' 'id' 'control' 'wisdom' 'act'\n",
    "  'passport' 'helped']\n",
    "  \n",
    "  \n",
    "- __\"Companies\" topic__:  ['companies' 'mountains' 'created' 'happy' 'jobs' 'wage' 'development'\n",
    "  'issue' 'former' 'highest']\n",
    " \n",
    "- __\"Terry Pratchett__\" topic: ['series' 'discworld' 'equal' 'granny' 'magic' 'killer' 'notre' 'dame'\n",
    "  'immigrant' 'witches']\n",
    "  \n",
    "- __\"Evolution\" topic__: ['students' 'text' 'alone' 'common' 'design' 'evolution' 'opinion' 'pay'\n",
    "  'darwin' 'show']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 10 Topics, $\\alpha = \\beta = 0.01$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Likelihood -2343457.870990367\n",
      "Iteration 1\n",
      "Likelihood -2320749.8681018814\n",
      "Iteration 2\n",
      "Likelihood -2304569.705220574\n",
      "Iteration 3\n",
      "Likelihood -2290597.510808511\n",
      "Iteration 4\n",
      "Likelihood -2276621.381299429\n",
      "Iteration 5\n",
      "Likelihood -2260341.027312455\n",
      "Iteration 6\n",
      "Likelihood -2235869.9136667443\n",
      "Iteration 7\n",
      "Likelihood -2202960.071286286\n",
      "Iteration 8\n",
      "Likelihood -2165050.2412849767\n",
      "Iteration 9\n",
      "Likelihood -2129827.7747944077\n",
      "Iteration 10\n",
      "Likelihood -2101752.4814116503\n",
      "Iteration 11\n",
      "Likelihood -2080085.3136327907\n",
      "Iteration 12\n",
      "Likelihood -2063487.1144998441\n",
      "Iteration 13\n",
      "Likelihood -2045536.251464159\n",
      "Iteration 14\n",
      "Likelihood -2031064.2508593616\n",
      "Iteration 15\n",
      "Likelihood -2016179.4829783826\n",
      "Iteration 16\n",
      "Likelihood -2000026.3275540767\n",
      "Iteration 17\n",
      "Likelihood -1985799.924806189\n",
      "Iteration 18\n",
      "Likelihood -1974548.4329042924\n",
      "Iteration 19\n",
      "Likelihood -1960551.3750523948\n",
      "Iteration 20\n",
      "Likelihood -1949357.0448735512\n",
      "Iteration 21\n",
      "Likelihood -1937563.1301793077\n",
      "Iteration 22\n",
      "Likelihood -1923682.851831316\n",
      "Iteration 23\n",
      "Likelihood -1913015.5144013052\n",
      "Iteration 24\n",
      "Likelihood -1902850.1489342884\n",
      "Iteration 25\n",
      "Likelihood -1890652.9295315593\n",
      "Iteration 26\n",
      "Likelihood -1881812.5760584965\n",
      "Iteration 27\n",
      "Likelihood -1872248.280135396\n",
      "Iteration 28\n",
      "Likelihood -1861734.5649729555\n",
      "Iteration 29\n",
      "Likelihood -1855376.4352269424\n",
      "Iteration 30\n",
      "Likelihood -1847007.591056231\n",
      "Iteration 31\n",
      "Likelihood -1838953.6762262182\n",
      "Iteration 32\n",
      "Likelihood -1828962.859011661\n",
      "Iteration 33\n",
      "Likelihood -1821897.3152444088\n",
      "Iteration 34\n",
      "Likelihood -1814308.8708545014\n",
      "Iteration 35\n",
      "Likelihood -1806408.591205612\n",
      "Iteration 36\n",
      "Likelihood -1798367.2159974314\n",
      "Iteration 37\n",
      "Likelihood -1790829.5185883462\n",
      "Iteration 38\n",
      "Likelihood -1786202.3196843108\n",
      "Iteration 39\n",
      "Likelihood -1780386.9058589505\n",
      "Iteration 40\n",
      "Likelihood -1772239.6836884515\n",
      "Iteration 41\n",
      "Likelihood -1762381.9676228196\n",
      "Iteration 42\n",
      "Likelihood -1755564.2298587367\n",
      "Iteration 43\n",
      "Likelihood -1750728.0541037235\n",
      "Iteration 44\n",
      "Likelihood -1744378.860365603\n",
      "Iteration 45\n",
      "Likelihood -1737437.078058091\n",
      "Iteration 46\n",
      "Likelihood -1733755.551495244\n",
      "Iteration 47\n",
      "Likelihood -1727458.0714545215\n",
      "Iteration 48\n",
      "Likelihood -1721006.3277186544\n",
      "Iteration 49\n",
      "Likelihood -1715122.9817982428\n",
      "Iteration 50\n",
      "Likelihood -1710952.3371547568\n",
      "Iteration 51\n",
      "Likelihood -1706106.5675569442\n",
      "Iteration 52\n",
      "Likelihood -1697957.5722407359\n",
      "Iteration 53\n",
      "Likelihood -1692452.612872429\n",
      "Iteration 54\n",
      "Likelihood -1686534.8308158086\n",
      "Iteration 55\n",
      "Likelihood -1681853.9312886952\n",
      "Iteration 56\n",
      "Likelihood -1676941.2438996534\n",
      "Iteration 57\n",
      "Likelihood -1671963.6910924625\n",
      "Iteration 58\n",
      "Likelihood -1666556.6505406946\n",
      "Iteration 59\n",
      "Likelihood -1661285.2033657678\n",
      "Iteration 60\n",
      "Likelihood -1656386.4206293593\n",
      "Iteration 61\n",
      "Likelihood -1652567.754273746\n",
      "Iteration 62\n",
      "Likelihood -1645563.4299394065\n",
      "Iteration 63\n",
      "Likelihood -1640503.1118176552\n",
      "Iteration 64\n",
      "Likelihood -1635873.6988552802\n",
      "Iteration 65\n",
      "Likelihood -1630479.3025649756\n",
      "Iteration 66\n",
      "Likelihood -1626144.1166818012\n",
      "Iteration 67\n",
      "Likelihood -1623191.4402579635\n",
      "Iteration 68\n",
      "Likelihood -1618651.988864466\n",
      "Iteration 69\n",
      "Likelihood -1612555.7343172184\n",
      "Iteration 70\n",
      "Likelihood -1608668.732877296\n",
      "Iteration 71\n",
      "Likelihood -1602854.501945707\n",
      "Iteration 72\n",
      "Likelihood -1597292.3802901309\n",
      "Iteration 73\n",
      "Likelihood -1590685.7790108905\n",
      "Iteration 74\n",
      "Likelihood -1589277.825193876\n",
      "Elapsed time:  2557.848205804825\n"
     ]
    }
   ],
   "source": [
    "n_topics = 10\n",
    "\n",
    "word_topic_prob_001_10 = LDA_Gibbs_Sampler(matrix, voc_size, n_docs, n_topics, max_iterations, alpha = 0.01, beta = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['book' 'good' 'best' 'author' 'new' 'reader' 'every' 'well' 'like'\n",
      "  'excellent']\n",
      " ['book' 'would' 'much' 'even' 'work' 'history' 'two' 'reading' 'like'\n",
      "  'novel']\n",
      " ['work' 'love' 'first' 'wonderful' 'easy' 'real' 'lot' 'use' 'amazing'\n",
      "  'patterns']\n",
      " ['one' 'book' 'characters' 'time' 'world' 'best' 'like' 'stories'\n",
      "  'could' 'although']\n",
      " ['book' 'good' 'pages' 'interesting' 'woman' 'high' 'recommend' 'long'\n",
      "  'still' 'stories']\n",
      " ['book' 'read' 'great' 'many' 'must' 'recommend' 'better' 'also'\n",
      "  'series' 'students']\n",
      " ['books' 'time' 'readers' 'fiction' 'children' 'reader' 'need' 'says'\n",
      "  'novels' 'american']\n",
      " ['book' 'written' 'information' 'world' 'worth' 'highly' 'novel'\n",
      "  'anyone' 'page' 'complete']\n",
      " ['book' 'life' 'people' 'us' 'story' 'also' 'one' 'read' 'first' 'many']\n",
      " ['book' 'great' 'would' 'reading' 'years' 'way' 'well' 'writing' 'works'\n",
      "  'time']]\n"
     ]
    }
   ],
   "source": [
    "show_words_by_topic(word_topic_prob_001_10, vocabulary, typical_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Of the above topics, four appear to make sense:\n",
    "\n",
    "- __\"Positive\" topic__: ['book' 'good' 'best' 'author' 'new' 'reader' 'every' 'well' 'like'\n",
    "  'excellent']\n",
    "  \n",
    "- __\"Positive\" topic 2__: ['work' 'love' 'first' 'wonderful' 'easy' 'real' 'lot' 'use' 'amazing'\n",
    "  'patterns']\n",
    "  \n",
    "- __\"Recommending other books in series\" topic__:  ['book' 'read' 'great' 'many' 'must' 'recommend' 'better' 'also'\n",
    "  'series' 'students']\n",
    "  \n",
    "- __\"Children's fiction\" topic__: ['books' 'time' 'readers' 'fiction' 'children' 'reader' 'need' 'says'\n",
    "  'novels' 'american']\n",
    "\n",
    "Notice: two similar topics. Also, there are no negative topics! Maybe that is from the corpus, or maybe because we don't consider negations. \n",
    "\n",
    "Compare the loglikelihood between models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 50 Topics, $\\alpha = \\beta = 0.01$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Likelihood -3575222.7566558477\n",
      "Iteration 1\n",
      "Likelihood -3511858.157047361\n",
      "Iteration 2\n",
      "Likelihood -3453380.977278283\n",
      "Iteration 3\n",
      "Likelihood -3396604.8616934363\n",
      "Iteration 4\n",
      "Likelihood -3330508.159939607\n",
      "Iteration 5\n",
      "Likelihood -3249983.8052112306\n",
      "Iteration 6\n",
      "Likelihood -3174941.4953452684\n",
      "Iteration 7\n",
      "Likelihood -3108042.3533887807\n",
      "Iteration 8\n",
      "Likelihood -3052397.1337047815\n",
      "Iteration 9\n",
      "Likelihood -3003682.1830039476\n",
      "Iteration 10\n",
      "Likelihood -2959216.6975951935\n",
      "Iteration 11\n",
      "Likelihood -2920272.050352145\n",
      "Iteration 12\n",
      "Likelihood -2881800.5491413083\n",
      "Iteration 13\n",
      "Likelihood -2848358.745659724\n",
      "Iteration 14\n",
      "Likelihood -2816436.5785797634\n",
      "Iteration 15\n",
      "Likelihood -2786236.584752912\n",
      "Iteration 16\n",
      "Likelihood -2757118.728225909\n",
      "Iteration 17\n",
      "Likelihood -2726209.2886329475\n",
      "Iteration 18\n",
      "Likelihood -2695776.095584607\n",
      "Iteration 19\n",
      "Likelihood -2666513.283767589\n",
      "Iteration 20\n",
      "Likelihood -2639867.317621984\n",
      "Iteration 21\n",
      "Likelihood -2615079.749422195\n",
      "Iteration 22\n",
      "Likelihood -2591513.5442678286\n",
      "Iteration 23\n",
      "Likelihood -2568032.696406108\n",
      "Iteration 24\n",
      "Likelihood -2547324.989995782\n",
      "Iteration 25\n",
      "Likelihood -2524811.1252395734\n",
      "Iteration 26\n",
      "Likelihood -2505472.6144535495\n",
      "Iteration 27\n",
      "Likelihood -2484702.697172181\n",
      "Iteration 28\n",
      "Likelihood -2465382.5075739287\n",
      "Iteration 29\n",
      "Likelihood -2445211.8929877784\n",
      "Iteration 30\n",
      "Likelihood -2430423.2534541325\n",
      "Iteration 31\n",
      "Likelihood -2410022.2735267957\n",
      "Iteration 32\n",
      "Likelihood -2392751.630579285\n",
      "Iteration 33\n",
      "Likelihood -2375918.4803191586\n",
      "Iteration 34\n",
      "Likelihood -2359286.554103561\n",
      "Iteration 35\n",
      "Likelihood -2342465.3769728467\n",
      "Iteration 36\n",
      "Likelihood -2329205.047772394\n",
      "Iteration 37\n",
      "Likelihood -2314421.3631435214\n",
      "Iteration 38\n",
      "Likelihood -2299782.7761605843\n",
      "Iteration 39\n",
      "Likelihood -2285353.490445708\n",
      "Iteration 40\n",
      "Likelihood -2270530.2172488216\n",
      "Iteration 41\n",
      "Likelihood -2258685.8477638406\n",
      "Iteration 42\n",
      "Likelihood -2245040.459164652\n",
      "Iteration 43\n",
      "Likelihood -2234850.1218590806\n",
      "Iteration 44\n",
      "Likelihood -2220646.516050772\n",
      "Iteration 45\n",
      "Likelihood -2207891.704901442\n",
      "Iteration 46\n",
      "Likelihood -2195845.058847567\n",
      "Iteration 47\n",
      "Likelihood -2183182.2685084534\n",
      "Iteration 48\n",
      "Likelihood -2172172.5708800154\n",
      "Iteration 49\n",
      "Likelihood -2160225.287891804\n",
      "Iteration 50\n",
      "Likelihood -2147360.4192784205\n",
      "Iteration 51\n",
      "Likelihood -2135322.1388939447\n",
      "Iteration 52\n",
      "Likelihood -2124134.529564639\n",
      "Iteration 53\n",
      "Likelihood -2113428.705848001\n",
      "Iteration 54\n",
      "Likelihood -2104655.9810941527\n",
      "Iteration 55\n",
      "Likelihood -2095446.9761939647\n",
      "Iteration 56\n",
      "Likelihood -2087108.7770289255\n",
      "Iteration 57\n",
      "Likelihood -2074896.635397904\n",
      "Iteration 58\n",
      "Likelihood -2063918.2785576207\n",
      "Iteration 59\n",
      "Likelihood -2053715.4571151398\n",
      "Iteration 60\n",
      "Likelihood -2044491.5850788588\n",
      "Iteration 61\n",
      "Likelihood -2034991.4499063606\n",
      "Iteration 62\n",
      "Likelihood -2026173.4258041473\n",
      "Iteration 63\n",
      "Likelihood -2016094.7614729493\n",
      "Iteration 64\n",
      "Likelihood -2003865.0313973096\n",
      "Iteration 65\n",
      "Likelihood -1995014.7718040494\n",
      "Iteration 66\n",
      "Likelihood -1986023.2546985839\n",
      "Iteration 67\n",
      "Likelihood -1979147.1728910392\n",
      "Iteration 68\n",
      "Likelihood -1969635.801285448\n",
      "Iteration 69\n",
      "Likelihood -1960645.6250680787\n",
      "Iteration 70\n",
      "Likelihood -1952495.1445283345\n",
      "Iteration 71\n",
      "Likelihood -1943064.8423143427\n",
      "Iteration 72\n",
      "Likelihood -1935117.8483428485\n",
      "Iteration 73\n",
      "Likelihood -1927861.2471935032\n",
      "Iteration 74\n",
      "Likelihood -1919416.4763603983\n",
      "Elapsed time:  2669.3952128887177\n"
     ]
    }
   ],
   "source": [
    "n_topics = 50\n",
    "\n",
    "word_topic_prob_001_50 = LDA_Gibbs_Sampler(matrix, voc_size, n_docs, n_topics, max_iterations, alpha = 0.01, beta = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['great' 'much' 'way' 'must' 'long' 'series' 'esk' 'put' 'want' 'could']\n",
      " ['story' 'books' 'read' 'first' 'daughter' 'us' 'period' 'however'\n",
      "  'times' 'writer']\n",
      " ['one' 'also' 'give' 'first' 'review' 'knowledge' 'need' 'business'\n",
      "  'serious' 'bring']\n",
      " ['history' 'students' 'c' 'second' 'years' 'journey' 'student' 'ago'\n",
      "  'long' 'fantastic']\n",
      " ['book' 'war' 'author' 'getting' 'later' 'third' 'particularly' 'though'\n",
      "  'example' 'library']\n",
      " ['things' 'helped' 'days' 'practical' '&' 'picture' 'understand' 'liked'\n",
      "  'plan' 'lessons']\n",
      " ['one' 'writing' 'could' 'written' 'women' 'recommended' 'recommend'\n",
      "  'patterns' 'school' 'would']\n",
      " ['excellent' 'highly' 'different' 'anyone' 'tale' 'works' 'kind' 'angel'\n",
      "  'classic' 'good']\n",
      " ['people' 'read' 'really' 'experience' 'rest' 'shows' 'society'\n",
      "  'written' 'myth' 'engineers']\n",
      " ['gibbon' 'work' 'could' 'mother' 'going' 'reading' 'fact' 'time' 'new'\n",
      "  'perhaps']\n",
      " ['children' 'life' 'human' 'freddie' 'experience' 'helga' 'soul' 'care'\n",
      "  'spiritual' 'personal']\n",
      " ['sam' 'also' 'life' 'first' 'much' 'say' 'good' 'well' 'set' 'man']\n",
      " ['book' 'way' 'found' 'life' 'learn' '*' 'may' 'real' 'night' 'made']\n",
      " ['recipes' 'would' 'job' 'say' 'science' \"'ve\" 'looking' 'interested'\n",
      "  'common' 'page']\n",
      " ['great' 'reading' 'best' 'say' 'detail' 'also' 'something' 'works'\n",
      "  'enjoyed' 'writes']\n",
      " ['book' 'read' 'us' 'found' 'ending' 'loved' 'still' 'end' 'enough'\n",
      "  'reader']\n",
      " ['get' 'make' 'like' 'read' 'although' 'years' 'points' 'good' 'highly'\n",
      "  'reading']\n",
      " ['read' 'like' 'one' 'way' 'world' 'know' 'two' \"'ll\" 'well' 'even']\n",
      " ['read' 'know' 'good' 'really' 'interesting' 'person' 'someone' 'says'\n",
      "  'story' 'group']\n",
      " ['book' 'chapter' 'one' 'great' 'find' 'us' 'using' 'home' 'must' 'many']\n",
      " ['gives' 'ca' 'right' 'things' 'long' 'entire' 'information' 'extremely'\n",
      "  'seemed' 'job']\n",
      " ['style' 'sense' 'seen' 'anything' 'look' 'stories' 'see' 'recipe'\n",
      "  'ever' 'content']\n",
      " ['author' 'think' 'time' 'theory' 'see' 'one' 'people' 'comes' 'best'\n",
      "  'ideas']\n",
      " ['darwin' 'would' 'first' 'buy' 'us' 'students' 'information' 'problems'\n",
      "  'want' 'hear']\n",
      " ['novel' 'school' 'many' 'people' 'young' 'writing' 'life' 'rather'\n",
      "  'novels' 'women']\n",
      " ['two' 'best' 'story' 'well' 'characters' 'positive' 'stories' 'today'\n",
      "  'woman' 'three']\n",
      " ['book' 'good' 'would' 'see' 'recommend' 'anyone' 'way' 'order' 'also'\n",
      "  'women']\n",
      " ['book' 'power' 'best' 'new' 'father' 'america' 'like' 'strong' 'feel'\n",
      "  'else']\n",
      " ['find' 'wonderful' 'also' 'best' 'bad' 'think' 'started' 'write' '&'\n",
      "  'liked']\n",
      " ['one' 'also' 'would' 'mr.' 'people' 'many' 'work' 'real' 'better'\n",
      "  'series']\n",
      " ['art' 'recommend' 'work' 'relationship' 'really' 'grisham' 'course'\n",
      "  'got' 'customer' 'town']\n",
      " ['writing' 'love' 'great' 'recommend' 'useful' 'new' 'style' 'format'\n",
      "  'action' 'filled']\n",
      " ['history' 'life' 'play' 'world' 'years' \"o'neill\" 'story' 'mathematics'\n",
      "  'works' 'due']\n",
      " ['book' 'many' 'even' 'find' 'world' 'want' 'case' 'see' 'simple'\n",
      "  'loves']\n",
      " ['think' 'still' 'would' 'even' 'love' 'read' 'takes' 'going' 'change'\n",
      "  'buy']\n",
      " ['time' 'horse' 'even' 'part' 'wanted' 'friend' '1' 'may' 'us'\n",
      "  'mccormicks']\n",
      " ['named' 'old' 'research' 'age' 'studies' 'boy' 'page' 'pearl'\n",
      "  'including' 'man']\n",
      " ['read' 'well' 'people' 'written' 'could' 'problems' 'life' 'story'\n",
      "  'student' 'lot']\n",
      " ['every' 'new' 'good' 'much' 'world' 'books' 'understand' \"'re\" 'making'\n",
      "  'work']\n",
      " ['well' 'written' 'lives' 'anyone' 'steve' 'new' 'first' 'world' 'would'\n",
      "  'like']\n",
      " ['book' 'read' 'never' 'used' 'many' 'reading' 'help' 'beautiful'\n",
      "  'continue' 'earth']\n",
      " ['book' 'read' 'mcginnis' 'good' 'information' 'another' 'question'\n",
      "  'every' 'view' 'infiltration']\n",
      " ['books' 'book' 'would' 'subject' \"'ve\" 'fat' 'right' 'life' 'little'\n",
      "  'found']\n",
      " ['never' 'even' '[...]' 'form' 'woman' 'edition' 'reading' 'instead'\n",
      "  'use' 'language']\n",
      " ['family' 'books' 'though' 'modern' 'william' 'could' 'gaskell' 'police'\n",
      "  'opinion' 'fact']\n",
      " ['books' 'book' 'world' 'one' 'great' 'start' 'year' 'name' 'back'\n",
      "  'reading']\n",
      " ['stories' 'could' 'water' 'series' 'never' 'process' 'provides'\n",
      "  'author' 'influence' 'great']\n",
      " ['read' 'lot' 'reader' 'story' 'easy' 'tell' 'interesting' 'time'\n",
      "  'whole' 'high']\n",
      " ['pages' 'would' 'reading' 'john' 'bob' 'like' 'two' 'really' 'first'\n",
      "  \"'re\"]\n",
      " ['dr.' 'love' 'america' 'good' 'years' 'american' 'never' 'shows'\n",
      "  'fascinating' 'modern']]\n"
     ]
    }
   ],
   "source": [
    "show_words_by_topic(word_topic_prob_001_50, vocabulary, typical_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- __\"Lesson planning\" topic__: ['things' 'helped' 'days' 'practical' '&' 'picture' 'understand' 'liked'\n",
    "  'plan' 'lessons']\n",
    "  \n",
    "- __\"Positive\" topic__:  ['great' 'reading' 'best' 'say' 'detail' 'also' 'something' 'works'\n",
    "  'enjoyed' 'writes']\n",
    "\n",
    "- __\"Age\" topic__:  ['named' 'old' 'research' 'age' 'studies' 'boy' 'page' 'pearl'\n",
    "  'including' 'man']\n",
    "\n",
    "the words \"book\" and \"read\" are very common here - seems like this one didn't learn as much. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Save results as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"nzw_01_10.csv\", word_topic_prob_01_10, delimiter=\",\")\n",
    "np.savetxt(\"nzw_01_50.csv\", word_topic_prob_01_50, delimiter=\",\")\n",
    "np.savetxt(\"nzw_001_10.csv\", word_topic_prob_001_10, delimiter=\",\")\n",
    "np.savetxt(\"nzw_001_50.csv\", word_topic_prob_001_50, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Write your own code for doing Gibbs samling on Bigram LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def count_bigram_topics(filename, encoding, n_lines, vocabulary, ignore_list, topics):\n",
    "        \n",
    "    string_dict = defaultdict(Counter)\n",
    "    int_dict = defaultdict(Counter)\n",
    "    \n",
    "    with open(filename, encoding = encoding) as f:\n",
    "        for d, line in enumerate(f):\n",
    "            \n",
    "            if d == n_lines:\n",
    "                break\n",
    "            \n",
    "            tokens = line.lower().split()\n",
    "  \n",
    "            for i, (t1, t2) in enumerate(zip(tokens, tokens[1:])):\n",
    "                \n",
    "                if t2 not in ignore_list:\n",
    "                    z = topics[(d, i+1)]\n",
    "\n",
    "                    string_dict[(t1, t2)][z] += 1\n",
    "\n",
    "                    int1 = vocabulary[t1]\n",
    "                    int2 = vocabulary[t2]\n",
    "                    int_dict[(int1, int2)][z] += 1\n",
    "            \n",
    "    return string_dict, int_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cond_prob_bigram(d, ndz, nzw, nz, nd, w, w_old, alpha, beta, n_topics, bigrams_num):\n",
    "    p_z = np.zeros(n_topics)\n",
    "\n",
    "    for k in range(n_topics):\n",
    "        left = ( bigrams_num[(w_old, w)][k] + beta * np.sum(nzw[:,w]) )/( nzw[k, w_old + 1] + beta )\n",
    "        right = ( ndz[d,k] + alpha * nz[k] )/( nd[d] + alpha )\n",
    "\n",
    "        p_z[k] = left*right\n",
    "\n",
    "    p_z /= sum(p_z)\n",
    "    return p_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def initialise_for_bigram(matrix, n_docs, n_topics, voc_size):\n",
    "    \n",
    "    ndz = np.zeros((n_docs, n_topics))\n",
    "    nzw = np.zeros((n_topics, voc_size))\n",
    "    nd = np.zeros(n_docs)\n",
    "    nz = np.zeros(n_topics)\n",
    "    topics = {}\n",
    "\n",
    "    for d, line in enumerate(matrix):\n",
    "        for i, w in enumerate(line):\n",
    "\n",
    "            if w not in [-1, 0]:\n",
    "\n",
    "                # Initialise with a random topic\n",
    "                z = rd.randint(n_topics)\n",
    "                topics[(d,i)] = z\n",
    "                ndz[d, z] += 1\n",
    "                nzw[z, w] += 1\n",
    "                nd[d] += 1\n",
    "                nz[z] += 1\n",
    "                \n",
    "    return ndz, nzw, nd, nz, topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def run_bigram_LDA(ndz, nzw, nd, nz, topics, matrix, max_iterations, alpha, beta, n_topics, bigrams_num):\n",
    "\n",
    "    something_went_wrong = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        for d, line in enumerate(matrix):\n",
    "            for j, w in enumerate(line):\n",
    "                if w not in [-1, 0]:\n",
    "\n",
    "                    z = topics[(d, j)]\n",
    "                    ndz[d, z] -= 1\n",
    "                    nzw[z, w] -= 1\n",
    "                    nd[d] -= 1\n",
    "                    nz[z] -= 1\n",
    "\n",
    "                    # preceding word in the bigram   \n",
    "                    w_prev = matrix[d, j-1]\n",
    "\n",
    "                    # If the word only appears once in corpus\n",
    "                    if np.sum(nzw[:,w]) == 0:\n",
    "                        z = rd.randint(n_topics)      \n",
    "\n",
    "                    elif bigrams_num[(w_prev, w)][z] == 0:\n",
    "\n",
    "                        something_went_wrong += 1\n",
    "                        z = rd.randint(n_topics)      \n",
    "\n",
    "                    else:\n",
    "\n",
    "                        bigrams_num[(w_prev, w)][z] -= 1\n",
    "                        p_z = cond_prob_bigram(d, ndz, nzw, nz, nd, w, w_prev, alpha, beta, n_topics, bigrams_num)\n",
    "                        z = rd.multinomial(1, p_z).argmax()\n",
    "\n",
    "                    ndz[d,z] += 1\n",
    "                    nzw[z,w] += 1\n",
    "                    nd[d] += 1\n",
    "                    nz[z] += 1\n",
    "                    topics[(d, j)] = z\n",
    "\n",
    "                    bigrams_num[(w_prev, w)][z] += 1\n",
    "\n",
    "        print(\"Iteration\", i)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"Elapsed time: \", elapsed_time)\n",
    "\n",
    "    return nzw, something_went_wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it took me two days to read this book because once i started i could n't put it down . mr.magnus leads us through various exercises to show us how we too can have an astral projection . the exercises are fun and interesting . he also adds entries from his journal that makes the reader feel that they too can achieve the ability to enter the astral realm . this book is easy to read unlike many other books on the subject which make the reader feel bogged down with specificities which soon become boring . a beautifully written , enthusiastic book about a subject that is surrounded in mystery \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(filename, encoding = encoding) as f:\n",
    "    for line in f:\n",
    "        print(line)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step is exactly the same as in part 1\n",
    "\n",
    "filename = \"books.txt\" \n",
    "encoding = \"ISO-8859-1\"\n",
    "n_tokens = 5*10**5\n",
    "\n",
    "ignore_words = list_of_stopwords()\n",
    "n_docs, matrix, vocabulary, voc_size = get_matrix(filename, encoding, n_tokens, ignore_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iterations = 100\n",
    "beta = 0.1\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N_topics = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "Iteration 13\n",
      "Iteration 14\n",
      "Iteration 15\n",
      "Iteration 16\n",
      "Iteration 17\n",
      "Iteration 18\n",
      "Iteration 19\n",
      "Iteration 20\n",
      "Iteration 21\n",
      "Iteration 22\n",
      "Iteration 23\n",
      "Iteration 24\n",
      "Iteration 25\n",
      "Iteration 26\n",
      "Iteration 27\n",
      "Iteration 28\n",
      "Iteration 29\n",
      "Iteration 30\n",
      "Iteration 31\n",
      "Iteration 32\n",
      "Iteration 33\n",
      "Iteration 34\n",
      "Iteration 35\n",
      "Iteration 36\n",
      "Iteration 37\n",
      "Iteration 38\n",
      "Iteration 39\n",
      "Iteration 40\n",
      "Iteration 41\n",
      "Iteration 42\n",
      "Iteration 43\n",
      "Iteration 44\n",
      "Iteration 45\n",
      "Iteration 46\n",
      "Iteration 47\n",
      "Iteration 48\n",
      "Iteration 49\n",
      "Iteration 50\n",
      "Iteration 51\n",
      "Iteration 52\n",
      "Iteration 53\n",
      "Iteration 54\n",
      "Iteration 55\n",
      "Iteration 56\n",
      "Iteration 57\n",
      "Iteration 58\n",
      "Iteration 59\n",
      "Iteration 60\n",
      "Iteration 61\n",
      "Iteration 62\n",
      "Iteration 63\n",
      "Iteration 64\n",
      "Iteration 65\n",
      "Iteration 66\n",
      "Iteration 67\n",
      "Iteration 68\n",
      "Iteration 69\n",
      "Iteration 70\n",
      "Iteration 71\n",
      "Iteration 72\n",
      "Iteration 73\n",
      "Iteration 74\n",
      "Iteration 75\n",
      "Iteration 76\n",
      "Iteration 77\n",
      "Iteration 78\n",
      "Iteration 79\n",
      "Iteration 80\n",
      "Iteration 81\n",
      "Iteration 82\n",
      "Iteration 83\n",
      "Iteration 84\n",
      "Iteration 85\n",
      "Iteration 86\n",
      "Iteration 87\n",
      "Iteration 88\n",
      "Iteration 89\n",
      "Iteration 90\n",
      "Iteration 91\n",
      "Iteration 92\n",
      "Iteration 93\n",
      "Iteration 94\n",
      "Iteration 95\n",
      "Iteration 96\n",
      "Iteration 97\n",
      "Iteration 98\n",
      "Iteration 99\n",
      "Elapsed time:  2616.172966480255\n"
     ]
    }
   ],
   "source": [
    "n_topics = 5\n",
    "\n",
    "ndz, nzw, nd, nz, topics = initialise_for_bigram(matrix, n_docs, n_topics, voc_size)\n",
    "bigrams, bigrams_num = count_bigram_topics(filename, encoding, n_docs, vocabulary, ignore_words, topics)\n",
    "probs_5, error_counter = run_bigram_LDA(ndz, nzw, nd, nz, topics, matrix, max_iterations, alpha, beta, n_topics, bigrams_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['like' 'one' 'books' 'reading' 'would' 'people' 'time' 'story'\n",
      "  'recommend' 'us']\n",
      " ['read' 'like' 'story' 'would' 'books' 'years' 'time' 'reading' 'life'\n",
      "  'people']\n",
      " ['read' 'great' 'would' 'good' 'many' 'first' 'well' 'also' 'story'\n",
      "  'like']\n",
      " ['like' 'one' 'books' 'would' 'time' 'years' 'reading' 'life' 'written'\n",
      "  'story']\n",
      " ['like' 'one' 'reading' 'recommend' 'books' 'time' 'would' 'story'\n",
      "  'history' 'life']]\n"
     ]
    }
   ],
   "source": [
    "show_words_by_topic(probs_5, vocabulary, typical_len = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of these make sense?\n",
    "\n",
    "[['like' 'one' 'books' 'reading' 'would' 'people' 'time' 'story'\n",
    "  'recommend' 'us']\n",
    "  \n",
    " ['read' 'like' 'story' 'would' 'books' 'years' 'time' 'reading' 'life'\n",
    "  'people']\n",
    "  \n",
    " ['read' 'great' 'would' 'good' 'many' 'first' 'well' 'also' 'story'\n",
    "  'like']\n",
    "  \n",
    " ['like' 'one' 'books' 'would' 'time' 'years' 'reading' 'life' 'written'\n",
    "  'story']\n",
    "  \n",
    " ['like' 'one' 'reading' 'recommend' 'books' 'time' 'would' 'story'\n",
    "  'history' 'life']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "Iteration 13\n",
      "Iteration 14\n",
      "Iteration 15\n",
      "Iteration 16\n",
      "Iteration 17\n",
      "Iteration 18\n",
      "Iteration 19\n",
      "Iteration 20\n",
      "Iteration 21\n",
      "Iteration 22\n",
      "Iteration 23\n",
      "Iteration 24\n",
      "Iteration 25\n",
      "Iteration 26\n",
      "Iteration 27\n",
      "Iteration 28\n",
      "Iteration 29\n",
      "Iteration 30\n",
      "Iteration 31\n",
      "Iteration 32\n",
      "Iteration 33\n",
      "Iteration 34\n",
      "Iteration 35\n",
      "Iteration 36\n",
      "Iteration 37\n",
      "Iteration 38\n",
      "Iteration 39\n",
      "Iteration 40\n",
      "Iteration 41\n",
      "Iteration 42\n",
      "Iteration 43\n",
      "Iteration 44\n",
      "Iteration 45\n",
      "Iteration 46\n",
      "Iteration 47\n",
      "Iteration 48\n",
      "Iteration 49\n",
      "Iteration 50\n",
      "Iteration 51\n",
      "Iteration 52\n",
      "Iteration 53\n",
      "Iteration 54\n",
      "Iteration 55\n",
      "Iteration 56\n",
      "Iteration 57\n",
      "Iteration 58\n",
      "Iteration 59\n",
      "Iteration 60\n",
      "Iteration 61\n",
      "Iteration 62\n",
      "Iteration 63\n",
      "Iteration 64\n",
      "Iteration 65\n",
      "Iteration 66\n",
      "Iteration 67\n",
      "Iteration 68\n",
      "Iteration 69\n",
      "Iteration 70\n",
      "Iteration 71\n",
      "Iteration 72\n",
      "Iteration 73\n",
      "Iteration 74\n",
      "Iteration 75\n",
      "Iteration 76\n",
      "Iteration 77\n",
      "Iteration 78\n",
      "Iteration 79\n",
      "Iteration 80\n",
      "Iteration 81\n",
      "Iteration 82\n",
      "Iteration 83\n",
      "Iteration 84\n",
      "Iteration 85\n",
      "Iteration 86\n",
      "Iteration 87\n",
      "Iteration 88\n",
      "Iteration 89\n",
      "Iteration 90\n",
      "Iteration 91\n",
      "Iteration 92\n",
      "Iteration 93\n",
      "Iteration 94\n",
      "Iteration 95\n",
      "Iteration 96\n",
      "Iteration 97\n",
      "Iteration 98\n",
      "Iteration 99\n",
      "Elapsed time:  3688.843549013138\n"
     ]
    }
   ],
   "source": [
    "n_topics = 10\n",
    "\n",
    "ndz, nzw, nd, nz, topics = initialise_for_bigram(matrix, n_docs, n_topics, voc_size)\n",
    "bigrams, bigrams_num = count_bigram_topics(filename, encoding, n_docs, vocabulary, ignore_words, topics)\n",
    "probs_10, error_counter = run_bigram_LDA(ndz, nzw, nd, nz, topics, matrix, max_iterations, alpha, beta, n_topics, bigrams_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['like' 'would' 'one' 'years' 'reading' 'books' 'time' 'us' 'story'\n",
      "  'people']\n",
      " ['read' 'one' 'books' 'would' 'time' 'years' 'story' 'reading' 'new'\n",
      "  'us']\n",
      " ['like' 'one' 'reading' 'books' 'time' 'story' 'us' 'would' 'years'\n",
      "  'life']\n",
      " ['read' 'century' 'one' 'books' 'people' 'reading' 'time' 'would' 'life'\n",
      "  'story']\n",
      " ['like' 'books' 'one' 'reading' 'would' 'story' 'life' 'years' 'time'\n",
      "  'us']\n",
      " ['read' 'great' 'good' 'would' 'many' 'also' 'well' 'story' 'first'\n",
      "  'much']\n",
      " ['one' 'like' 'recommend' 'books' 'years' 'would' 'reading' 'people'\n",
      "  'life' 'us']\n",
      " ['like' 'one' 'books' 'recommend' 'reading' 'would' 'time' 'life' 'work'\n",
      "  'people']\n",
      " ['one' 'like' 'would' 'reading' 'years' 'books' 'story' 'time' 'people'\n",
      "  'history']\n",
      " ['like' 'one' 'would' 'books' 'reading' 'story' 'people' 'time' 'years'\n",
      "  'way']]\n"
     ]
    }
   ],
   "source": [
    "show_words_by_topic(probs_10, vocabulary, typical_len = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of these really make sense?\n",
    "\n",
    "[['like' 'would' 'one' 'years' 'reading' 'books' 'time' 'us' 'story'\n",
    "  'people']\n",
    "  \n",
    " ['read' 'one' 'books' 'would' 'time' 'years' 'story' 'reading' 'new'\n",
    "  'us']\n",
    "  \n",
    " ['like' 'one' 'reading' 'books' 'time' 'story' 'us' 'would' 'years'\n",
    "  'life']\n",
    "  \n",
    " ['read' 'century' 'one' 'books' 'people' 'reading' 'time' 'would' 'life'\n",
    "  'story']\n",
    "  \n",
    " ['like' 'books' 'one' 'reading' 'would' 'story' 'life' 'years' 'time'\n",
    "  'us']\n",
    "  \n",
    " ['read' 'great' 'good' 'would' 'many' 'also' 'well' 'story' 'first'\n",
    "  'much']\n",
    "  \n",
    " ['one' 'like' 'recommend' 'books' 'years' 'would' 'reading' 'people'\n",
    "  'life' 'us']\n",
    "  \n",
    " ['like' 'one' 'books' 'recommend' 'reading' 'would' 'time' 'life' 'work'\n",
    "  'people']\n",
    "  \n",
    " ['one' 'like' 'would' 'reading' 'years' 'books' 'story' 'time' 'people'\n",
    "  'history']\n",
    "  \n",
    " ['like' 'one' 'would' 'books' 'reading' 'story' 'people' 'time' 'years'\n",
    "  'way']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
