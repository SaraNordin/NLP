{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Write your own code for doing Gibbs sampling for LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Write your own code for doing Gibbs samling on Bigram LDA\n",
    "\n",
    "as in H. M. Wallach: Topic modeling: beyond bag-of-words. ICML(2006) 977-984. http://dirichlet.net/pdf/wallach06topic.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For corpus use the Amazon book reviews corpus that you also used in Assignment 1. You may have to use only a subset of the documents. A corpus of 100 000 tokens is sufficients size.\n",
    "\n",
    "Run this for different hyperparameters. For LDA you can try α=β=0.1 and α=β=0.01. (A cross-validation search for optimal values will probably be too slow.) Run also for different numbers of topics, e.g. K=10 and K=50.\n",
    "\n",
    "For the bigram model, see to that you use a larger hyperparameter value on the diagonal of the transition matrix over the topics. Since each document in this model has a transition matrix over topics rather than just a probability distribution, the number of topics cannot be as large as for LDA. Try K=5 and K=10.\n",
    "\n",
    "Assess model performance by perplexity and in tables over the ten top words for the topics that seem to make the most sense.\n",
    "\n",
    "As mentioned in the instructions, you should submit a report and code (source files, notebooks, or Colab links)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "582\n"
     ]
    }
   ],
   "source": [
    "filename = \"books.txt\" \n",
    "encoding = \"ISO-8859-1\"\n",
    "\n",
    "line_counter = 0\n",
    "token_counter = 0\n",
    "\n",
    "with open(filename, encoding = encoding) as f:\n",
    "    for line in f:\n",
    "\n",
    "        line_counter += 1\n",
    "        tokens = line.lower().split()\n",
    "\n",
    "        for token in tokens:\n",
    "            token_counter += 1\n",
    "            \n",
    "        if token_counter > 100000:\n",
    "            break\n",
    "\n",
    "print(line_counter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
