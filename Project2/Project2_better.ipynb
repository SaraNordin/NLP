{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import time\n",
    "from scipy.special import gammaln\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Import stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def docs_to_int(corpus:str, encoding:str, nr_tokens:int, stop:list) -> (list(), Counter()):\n",
    "    \n",
    "    token_count = 0\n",
    "    docs = list()\n",
    "    docs_count = 0\n",
    "    \n",
    "    # Find unique words (removal of stopwords and punctuations and other stuff.)\n",
    "    freqs = Counter()   \n",
    "    \n",
    "    with open(corpus, encoding = encoding) as f:\n",
    "        \n",
    "        while token_count < nr_tokens:\n",
    "            \n",
    "            doc = f.readline()\n",
    "            docs_count += 1\n",
    "            temp_doc = list()\n",
    "            \n",
    "            # Lemmatizion of tokens\n",
    "            result = nlp(doc.lower(), disable=['tagger', 'parser', 'ner'])\n",
    "            \n",
    "            for token in result:\n",
    "                if len(token.lemma_) > 2:\n",
    "                    if token.lemma_ not in stop:\n",
    "                        token_count += 1\n",
    "                        freqs[token.lemma_] += 1\n",
    "                        temp_doc.append(token.lemma_)\n",
    "                        \n",
    "            docs.append(temp_doc)\n",
    "\n",
    "    print('Nr of tokens:' + str(token_count))\n",
    "    print('Nr of docs:' + str(docs_count))\n",
    "    \n",
    "    # Create bag of words\n",
    "    bow = []\n",
    "    vocab = freqs.most_common()\n",
    "\n",
    "    for i in range(len(vocab)):\n",
    "        bow.append(vocab[i][0])\n",
    "\n",
    "    # Get pairs of elements    \n",
    "    tmp = zip(bow, range(0,len(vocab)-1))\n",
    "    # Make pairs into a dictionary\n",
    "    vocab = dict(tmp)\n",
    "        \n",
    "    # Match token to int\n",
    "    docs_int = list()\n",
    "    \n",
    "    for doc in docs:\n",
    "        docs_int.append(list([vocab.get(x) for x in doc]))\n",
    "        \n",
    "    return docs_int, freqs, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#stop = list(string.punctuation)\n",
    "stop = list()\n",
    "\n",
    "for x in stopwords.words('english'):\n",
    "    stop.append(x)\n",
    "\n",
    "'''\n",
    "Add a couple of more stopwords which were observed manually\n",
    "'''\n",
    "for x in ['-PRON-','\\n','...','..',\"'d'\",\"n't\"]:\n",
    "    stop.append(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "\n",
    "corpus = 'books.txt'\n",
    "encoding = 'ISO-8859-1'\n",
    "M = 10**5    # Corpus size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr of tokens:100030\n",
      "Nr of docs:1237\n"
     ]
    }
   ],
   "source": [
    "docs_int, freqs, vocab = docs_to_int(corpus=corpus, encoding=encoding, nr_tokens=M, stop = stop)\n",
    "vocab_size = len(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1236, 99)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_loc = []\n",
    "\n",
    "for d, line in enumerate(docs_int):\n",
    "\n",
    "    for j, w in enumerate(line):\n",
    "        if w == None:\n",
    "            nan_loc.append((d, j))\n",
    "        \n",
    "nan_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 's amazing how despite all the tragedies and wars , big business elitists are able to cash in on the damage while religious fundamentalists never get caught , much less held accountable . the idiots who show their hate of this book are from terrorist nations that have a knack of socializing poverty and terrorism while at the same time privatizing wealth . despite all the big talk about winning the so-called war on terrorism , the ugly truth is wars have not taught us anything . if it were n't for big business funding hitler , hitler would have had a harder time killing the jews . sadly though , even after world war ii ended , the big business elites that funded and continue to fund dictatorships like hitler , stalin , and the modern ones are not only not held accountable but often end up walking away as \" heroes \" . if we 're really going to win the war on terrorism and / or poverty , we 're going to have to stop supporting big business elite and stop allowing our uber-corrupt politicians from exploiting peoples fears on terrorism even while maximizing poverty \n",
      "\n",
      "0 amaze\n",
      "1 despite\n",
      "2 tragedy\n",
      "3 war\n",
      "4 big\n",
      "5 business\n",
      "6 elitist\n",
      "7 able\n",
      "8 cash\n",
      "9 damage\n",
      "10 religious\n",
      "11 fundamentalist\n",
      "12 never\n",
      "13 get\n",
      "14 catch\n",
      "15 much\n",
      "16 little\n",
      "17 hold\n",
      "18 accountable\n",
      "19 idiot\n",
      "20 show\n",
      "21 hate\n",
      "22 book\n",
      "23 terrorist\n",
      "24 nation\n",
      "25 knack\n",
      "26 socialize\n",
      "27 poverty\n",
      "28 terrorism\n",
      "29 time\n",
      "30 privatize\n",
      "31 wealth\n",
      "32 despite\n",
      "33 big\n",
      "34 talk\n",
      "35 win\n",
      "36 call\n",
      "37 war\n",
      "38 terrorism\n",
      "39 ugly\n",
      "40 truth\n",
      "41 war\n",
      "42 teach\n",
      "43 anything\n",
      "44 big\n",
      "45 business\n",
      "46 fund\n",
      "47 hitler\n",
      "48 hitler\n",
      "49 would\n",
      "50 hard\n",
      "51 time\n",
      "52 kill\n",
      "53 jews\n",
      "54 sadly\n",
      "55 though\n",
      "56 even\n",
      "57 world\n",
      "58 war\n",
      "59 end\n",
      "60 big\n",
      "61 business\n",
      "62 elite\n",
      "63 fund\n",
      "64 continue\n",
      "65 fund\n",
      "66 dictatorship\n",
      "67 like\n",
      "68 hitler\n",
      "69 stalin\n",
      "70 modern\n",
      "71 one\n",
      "72 hold\n",
      "73 accountable\n",
      "74 often\n",
      "75 end\n",
      "76 walk\n",
      "77 away\n",
      "78 hero\n",
      "79 really\n",
      "80 win\n",
      "81 war\n",
      "82 terrorism\n",
      "83 poverty\n",
      "84 stop\n",
      "85 support\n",
      "86 big\n",
      "87 business\n",
      "88 elite\n",
      "89 stop\n",
      "90 allow\n",
      "91 uber\n",
      "92 corrupt\n",
      "93 politician\n",
      "94 exploit\n",
      "95 people\n",
      "96 fear\n",
      "97 terrorism\n",
      "98 even\n",
      "99 maximize\n",
      "100 poverty\n"
     ]
    }
   ],
   "source": [
    "temp_doc = []\n",
    "\n",
    "with open(corpus, encoding = encoding) as f:\n",
    "    for line_idx, line in enumerate(f):\n",
    "        if line_idx == d:\n",
    "            print(line)\n",
    "            \n",
    "            result = nlp(line.lower(), disable=['tagger', 'parser', 'ner'])\n",
    "            \n",
    "            for token in result:\n",
    "                if len(token.lemma_) > 2:\n",
    "                    if token.lemma_ not in stop:\n",
    "                        \n",
    "                        temp_doc.append(token.lemma_)\n",
    "                             \n",
    "            for idx, token in enumerate(temp_doc):\n",
    "                print(idx, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick fix - sort this out later\n",
    "\n",
    "docs_int[nan_loc[0][0]][nan_loc[0][1]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('book', 2795),\n",
       " ('much', 1163),\n",
       " ('read', 1097),\n",
       " ('one', 834),\n",
       " ('good', 736),\n",
       " ('like', 540),\n",
       " ('would', 524),\n",
       " ('write', 522),\n",
       " ('make', 488),\n",
       " ('story', 476)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs.most_common()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17, 3, 0, 2121, 255, 6919, 1912, 21, 5, 116, 394, 6920, 3824, 2740]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_int[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4869"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[\"astral\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA STUFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def initialize_lda(docs_int, freqs, n_topics):\n",
    "    \n",
    "    n_docs = len(docs_int)\n",
    "    vocab_size = len(freqs)\n",
    "\n",
    "    # Word topic in doc, corresponds to \\Theta\n",
    "    ndz = np.zeros((n_docs,n_topics))\n",
    "\n",
    "    # Word topic count, corresponds to \\phi\n",
    "    nzw = np.zeros((n_topics, vocab_size))\n",
    "\n",
    "    # Counters for documents and topics\n",
    "    nd = np.zeros(n_docs)\n",
    "    nz = np.zeros(n_topics)\n",
    "\n",
    "    # Create dictionary of topics\n",
    "    topics = {}\n",
    "\n",
    "    # iterate over documents \n",
    "    for d, line in enumerate(docs_int):\n",
    "        for i, w in enumerate(line):\n",
    "                  \n",
    "            # w = docs_int[d][i] # Numerical rep of word w in doc d\n",
    "\n",
    "            # Initialise with a random topic\n",
    "            z = np.random.randint(n_topics)\n",
    "            topics[(d,i)] = z\n",
    "\n",
    "            # Increase counters\n",
    "            ndz[d, z] += 1\n",
    "            nzw[z, w] += 1\n",
    "\n",
    "            nd[d] += 1\n",
    "            nz[z] += 1\n",
    "\n",
    "    return topics, ndz, nzw, nd, nz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cond_topic_prob(ndz, nzw, nz, nd, w, d, alpha, beta, n_topics, voc_size):\n",
    "    \"\"\"\n",
    "    Conditional probability of topics. \n",
    "    \"\"\"\n",
    "\n",
    "    left = (nzw[:,w] + beta) / (nz + beta * voc_size)\n",
    "    right = (ndz[d,:] + alpha) / (nd[d] + alpha * n_topics)\n",
    "\n",
    "    p_z = left * right\n",
    "    p_z /= np.sum(p_z)\n",
    "    \n",
    "    return p_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def log_multinomial_beta(alpha, K=None):\n",
    "\n",
    "    if K is None:\n",
    "        # alpha is assumed to be a vector\n",
    "        return np.sum(gammaln(alpha)) - gammaln(np.sum(alpha))\n",
    "    else:\n",
    "        # alpha is assumed to be a scalar\n",
    "        return K * gammaln(alpha) - gammaln(K*alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def loglikelihood(n_topics, voc_size, alpha, beta, nzw, ndz):\n",
    "    likelihood = 0\n",
    "    \n",
    "    n_docs = ndz.shape[0]\n",
    "    \n",
    "    for z in range(n_topics):\n",
    "        likelihood += log_multinomial_beta(nzw[z,:] + beta)\n",
    "        likelihood -= log_multinomial_beta(beta, voc_size)\n",
    "        \n",
    "    for d in range(n_docs):\n",
    "        likelihood += log_multinomial_beta(ndz[d,:] + alpha)\n",
    "        likelihood -= log_multinomial_beta(alpha, n_topics)\n",
    "        \n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def LDA_Gibbs_Sampler(docs_int, voc_size, n_topics, max_iterations, alpha, beta, freqs):\n",
    "\n",
    "    start_time = time.time()\n",
    "    topics, ndz, nzw, nd, nz = initialize_lda(docs_int, freqs, n_topics)\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        for d, line in enumerate(docs_int):\n",
    "             for j, w in enumerate(line):\n",
    "                \n",
    "                z = topics[(d, j)]\n",
    "                ndz[d, z] -= 1\n",
    "                nzw[z, w] -= 1\n",
    "                nd[d] -= 1\n",
    "                nz[z] -= 1\n",
    "            \n",
    "                p_z = cond_topic_prob(ndz, nzw, nz, nd, w, d, alpha, beta, n_topics, vocab_size)\n",
    "                z = np.random.multinomial(1, p_z).argmax() # maybe don't use argmax, use it as a probability\n",
    "                \n",
    "                ndz[d,z] += 1\n",
    "                nzw[z,w] += 1\n",
    "                nd[d] += 1\n",
    "                nz[z] += 1\n",
    "                topics[(d, j)] = z\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\"Iteration\", i)\n",
    "            print(\"Likelihood\", loglikelihood(n_topics, voc_size, alpha, beta, nzw, ndz))\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"Elapsed time: \", elapsed_time)\n",
    "    \n",
    "    return nzw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show_words_by_topic(word_topic_prob, vocabulary, typical_len):\n",
    "    \n",
    "    n_topics = word_topic_prob.shape[0]\n",
    "    typical_words = []\n",
    "\n",
    "    for i in range(n_topics):\n",
    "        arr = word_topic_prob[i,:]\n",
    "        typical_ints = arr.argsort()[-typical_len-2:-2][::-1]   # there's some funny business with the last word in vocab\n",
    "        #print(typical_ints)\n",
    "\n",
    "        for search_int in typical_ints:\n",
    "            if search_int in [0, -1]:\n",
    "                typical_words.append(\"\")\n",
    "            else:\n",
    "                for k, v in vocabulary.items(): \n",
    "                    if v == search_int:\n",
    "                        typical_words.append(k)\n",
    "                        break\n",
    "\n",
    "    # Print the most common words in each topic\n",
    "    typical_words = np.reshape(typical_words, [n_topics, -1])\n",
    "    print(typical_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1  # Hyperparameter\n",
    "beta = 0.1   # Hyperparameter\n",
    "K = 10       # Nr of topics\n",
    "it = 100      # Nr of iterations\n",
    "\n",
    "topics, ndz, nzw, nd, nz = initialize_lda(docs_int, freqs, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Likelihood -1032872.9336507256\n",
      "Iteration 10\n",
      "Likelihood -922503.9558998622\n",
      "Iteration 20\n",
      "Likelihood -901413.2203634626\n",
      "Iteration 30\n",
      "Likelihood -890514.91921006\n",
      "Iteration 40\n",
      "Likelihood -883245.3888710137\n",
      "Elapsed time:  273.2125232219696\n",
      "[['much' 'good' 'like' 'get' 'write' 'one' 'would' 'make' 'time' 'great']\n",
      " ['much' 'like' 'one' 'good' 'character' 'would' 'get' 'story' 'little'\n",
      "  'say']\n",
      " ['author' 'church' 'name' 'historical' 'spiritual' 'word' 'view' 'human'\n",
      "  'doe' 'science']\n",
      " ['man' 'old' 'world' 'time' 'life' 'like' 'year' 'family' 'young' 'play']\n",
      " ['novel' 'read' 'much' 'reader' 'one' 'life' 'character' 'write' 'work'\n",
      "  'live']\n",
      " ['take' 'government' 'prospero' 'much' 'race' 'force' 'james' 'live'\n",
      "  'give' 'success']\n",
      " ['read' 'much' 'use' 'find' 'one' 'great' 'work' 'think' 'write' 'many']\n",
      " ['food' 'news' 'smith' 'show' 'new' 'get' 'eat' 'buy' 'find' 'include']\n",
      " ['american' 'history' 'man' 'write' 'account' 'america' 'military' 'one'\n",
      "  'power' 'human']\n",
      " ['much' 'one' 'still' 'consider' 'love' 'point' 'know' 'horse' 'reader'\n",
      "  'modern']]\n"
     ]
    }
   ],
   "source": [
    "K = 10\n",
    "alpha = 0.1\n",
    "beta = alpha\n",
    "\n",
    "word_topic_prob_01_10, log_lik_01_10 = LDA_Gibbs_Sampler(docs_int, vocab_size, K, it, alpha, beta, freqs)\n",
    "show_words_by_topic(word_topic_prob_01_10, vocab, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Likelihood -1181398.7504743172\n",
      "Iteration 10\n",
      "Likelihood -993165.9162597334\n",
      "Iteration 20\n",
      "Likelihood -949611.1432695037\n",
      "Iteration 30\n",
      "Likelihood -928661.2743395902\n",
      "Iteration 40\n",
      "Likelihood -918175.8819436037\n",
      "Iteration 50\n",
      "Likelihood -911789.5153736168\n",
      "Iteration 60\n",
      "Likelihood -906753.2238608436\n",
      "Iteration 70\n",
      "Likelihood -902399.8762865227\n",
      "Iteration 80\n",
      "Likelihood -898857.2788436102\n",
      "Iteration 90\n",
      "Likelihood -896018.964296845\n",
      "Elapsed time:  580.6504461765289\n",
      "[['collection' 'baseball' 'selection' 'hal' 'bach' 'musical' 'rock'\n",
      "  'player' 'christians' 'hitter']\n",
      " ['vedic' 'religious' 'iskcon' 'coelho' 'veronika' 'god' 'beauty'\n",
      "  'plague' 'technology' 'patient']\n",
      " ['law' 'britain' 'find' 'webb' 'nine' 'well' 'pendel' 'laymon' 'john'\n",
      "  'thirty']\n",
      " ['news' 'america' 'country' 'government' 'political' 'medium' 'power'\n",
      "  'military' 'state' 'show']\n",
      " ['subject' 'eastern' 'snake' 'battle' 'yates' 'politic' 'map' 'benefit'\n",
      "  'cultural' 'cop']\n",
      " ['biography' 'guralnick' 'cooke' 'cancer' 'version' 'push' 'official'\n",
      "  'chuck' 'believe' 'whitman']\n",
      " ['potty' 'irene' 'build' 'bud' 'weave' 'temple' 'stretch' 'replace'\n",
      "  'weaver' 'rush']\n",
      " ['human' 'brain' 'science' 'intelligence' 'darwin' 'evolution' 'von'\n",
      "  'behavior' 'scientific' 'model']\n",
      " ['steve' 'rachel' 'syllable' 'queene' 'malory' 'edition' '....' 'line'\n",
      "  'faerie' 'journalist']\n",
      " ['probability' 'clean' 'keynes' 'logical' 'general' 'statistical'\n",
      "  'wireless' 'sarney' 'theory' 'sanchia']\n",
      " ['gaskell' 'estate' 'card' 'rome' 'millionaire' 'success' 'eker' 'deck'\n",
      "  'tend' 'livy']\n",
      " ['shakespeare' 'miranda' 'caliban' 'ariel' 'king' 'anime' 'city'\n",
      "  'gentle' 'ferdinand' 'craft']\n",
      " ['condition' 'childhood' 'prose' 'lightning' 'van' 'display' 'margaret'\n",
      "  'close' 'victorian' 'whenever']\n",
      " ['history' 'text' 'philosophy' 'teacher' 'great' 'century' 'art' 'teach'\n",
      "  'present' 'highet']\n",
      " ['society' 'mother' 'mcbride' 'religious' 'black' 'global' 'live'\n",
      "  'child' 'leader' 'raise']\n",
      " ['arab' 'vaite' 'mature' 'maintain' 'glain' 'israel' 'cressida'\n",
      "  'natural' 'edible' 'leilani']\n",
      " ['ramanujan' 'carroll' 'ebay' 'truth' 'stage' 'tennis' 'hannah' 'whose'\n",
      "  'miss' 'chamber']\n",
      " ['kipling' 'dory' 'boat' 'meaker' 'always' 'scare' 'weekend'\n",
      "  'professional' 'finger' 'prey']\n",
      " ['johnson' 'condition' 'exist' 'slave' 'beyond' 'tone' 'evolve' 'treat'\n",
      "  'power' 'musharraf']\n",
      " ['mitchell' 'dave' 'racket' 'business' 'hell' 'contact' 'anybody'\n",
      "  'christian' 'professional' 'anywhere']\n",
      " ['' 'friend' 'old' 'help' 'play' 'young' 'name' 'family' 'kid' 'year']\n",
      " ['world' 'name' 'mean' 'lev' 'become' 'nothing' 'jewish' 'literature'\n",
      "  'william' 'adso']\n",
      " ['sterling' 'rut' 'libby' 'marie' '6th' 'aid' 'seedy' 'model' 'tape'\n",
      "  'within']\n",
      " ['helga' 'anne' 'battle' 'hunt' 'pirsig' 'mcpherson' 'original' 'fairly'\n",
      "  'xxiii' 'across']\n",
      " ['hancock' 'consciousness' 'pool' 'landscape' 'license' 'painting'\n",
      "  'supernatural' 'carol' 'phenomenon' 'content']\n",
      " ['stephen' 'train' 'hunt' 'animal' 'fox' 'mystery' 'force' 'base'\n",
      "  'alpha' 'companion']\n",
      " ['business' 'good' 'world' 'provide' 'reference' 'example' 'problem'\n",
      "  'exercise' 'self' 'real']\n",
      " ['society' 'love' 'nature' 'british' 'custom' 'thompson' 'man' 'theory'\n",
      "  'though' 'resentment']\n",
      " ['man' 'female' 'orgasm' 'body' 'lawrence' 'clearly' 'sexual' 'male'\n",
      "  'press' 'constance']\n",
      " ['story' 'one' 'much' 'good' 'character' 'novel' 'time' 'like' 'life'\n",
      "  'write']\n",
      " ['george' 'army' 'red' 'vietnam' 'abdulin' 'german' 'soldier' 'hitler'\n",
      "  'battle' 'combat']\n",
      " [\"o'neill\" 'thurmond' 'player' 'quilt' 'mae' 'return' 'space' 'piece'\n",
      "  'block' 'essie']\n",
      " ['richard' 'adams' 'release' 'gently' 'ring' 'detective' 'handbook'\n",
      "  'exam' 'dialect' 'computer']\n",
      " ['toad' 'social' 'proust' 'richardson' 'angeles' 'bruce' 'puppy'\n",
      "  'poorly' 'automobile' 'industry']\n",
      " ['reader' 'james' 'black' 'valyan' 'robicheaux' 'carl' 'white' 'burke'\n",
      "  'wake' 'niki']\n",
      " ['pushkin' 'whether' 'silverdeath' 'frank' 'immigrant' 'hansen'\n",
      "  'medical' 'destroy' 'psychology' 'chinese']\n",
      " ['birth' 'omit' 'eve' 'lemony' 'poem' 'eliot' 'mine' 'cooky' 'nicely'\n",
      "  'field']\n",
      " ['fermina' 'roz' 'cholera' 'thursday' 'ariza' 'daza' 'black' 'rise'\n",
      "  'marquez' 'lily']\n",
      " ['literature' 'japanese' 'disappoint' 'coast' 'funny' 'wait' 'chinese'\n",
      "  'account' 'heaven' 'attorney']\n",
      " ['fee' 'vampire' 'discworld' 'must' 'wizard' 'hogfather' 'belief'\n",
      "  'equal' 'death' 'dictionary']\n",
      " ['mcdonald' 'chee' 'leaphorn' 'navajo' 'hillerman' 'tie' 'inform'\n",
      "  'trail' 'issue' 'ashe']\n",
      " ['doug' 'poet' 'hall' 'magic' 'goblin' 'reason' 'sit' 'somehow'\n",
      "  'discussion' 'makenna']\n",
      " ['beaumont' 'peter' 'berry' 'david' 'arrive' 'mathematics' 'fix' 'jance'\n",
      "  'several' 'geoffrey']\n",
      " ['cell' 'kissinger' 'argue' 'science' 'today' 'white' 'project'\n",
      "  'isaacson' 'term' 'seller']\n",
      " ['mental' 'ackerman' 'glad' 'thomas' 'ape' 'bonobo' 'fiction' 'smiley'\n",
      "  'leonard' 'con']\n",
      " ['hug' 'homeless' 'child' 'autism' 'bukowski' 'encounter' 'due' 'taoism'\n",
      "  'monkey' 'lion']\n",
      " ['read' 'one' 'good' 'would' 'write' 'like' 'find' 'make' 'get' 'many']\n",
      " ['cook' 'eat' 'diet' 'meal' 'include' 'cookbook' 'healthy' 'weight'\n",
      "  'cut' 'health']\n",
      " ['alex' 'species' 'san' 'mexico' 'hike' 'guide' 'spirit' 'north' 'bird'\n",
      "  'street']\n",
      " ['soul' 'spirituality' 'god' 'map' 'prayer' 'size' 'mccormicks' 'fit'\n",
      "  'also' 'show']]\n"
     ]
    }
   ],
   "source": [
    "K = 50\n",
    "alpha = 0.1\n",
    "beta = alpha\n",
    "\n",
    "word_topic_prob_01_50, log_lik_01_50 = LDA_Gibbs_Sampler(docs_int, vocab_size, K, it, alpha, beta, freqs)\n",
    "show_words_by_topic(word_topic_prob_01_50, vocab, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Likelihood -1068254.9497986077\n",
      "Iteration 10\n",
      "Likelihood -935168.9056323167\n",
      "Iteration 20\n",
      "Likelihood -920033.2654612741\n",
      "Iteration 30\n",
      "Likelihood -912652.8357973326\n",
      "Iteration 40\n",
      "Likelihood -908021.0522482244\n",
      "Elapsed time:  281.6496891975403\n",
      "[['like' 'would' 'character' 'one' 'think' 'much' 'get' 'story' 'good'\n",
      "  'make']\n",
      " ['read' 'one' 'good' 'get' 'little' 'great' 'love' 'story' 'child'\n",
      "  'work']\n",
      " ['one' 'man' '' 'may' 'american' 'problem' 'make' 'well' 'would' 'use']\n",
      " ['read' 'people' 'time' 'would' 'find' 'good' 'make' 'think' 'idea'\n",
      "  'many']\n",
      " ['history' 'food' 'issue' 'write' 'interest' 'much' 'come' 'want' 'also'\n",
      "  'look']\n",
      " ['read' 'write' 'good' 'one' 'great' 'think' 'find' 'make' 'character'\n",
      "  'well']\n",
      " ['love' 'become' 'human' 'work' 'family' 'even' 'write' 'life' 'mean'\n",
      "  'year']\n",
      " ['people' 'novel' 'good' 'say' 'world' 'give' 'create' 'person' 'part'\n",
      "  'much']\n",
      " ['one' 'story' 'much' 'would' 'way' 'two' 'life' 'news' 'people' 'call']\n",
      " ['good' 'much' 'one' 'like' 'find' 'know' 'make' 'feel' 'would' 'doe']]\n"
     ]
    }
   ],
   "source": [
    "K = 10\n",
    "alpha = 0.01\n",
    "beta = alpha\n",
    "\n",
    "word_topic_prob_001_10 = LDA_Gibbs_Sampler(docs_int, vocab_size, K, it, alpha, beta, freqs)\n",
    "show_words_by_topic(word_topic_prob_001_10, vocab, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Likelihood -1189673.8378284348\n",
      "Iteration 10\n",
      "Likelihood -952077.5114507346\n",
      "Iteration 20\n",
      "Likelihood -929264.4139531485\n",
      "Iteration 30\n",
      "Likelihood -919213.3685814547\n",
      "Iteration 40\n",
      "Likelihood -912071.2629240489\n",
      "Elapsed time:  289.19020915031433\n",
      "[['get' 'one' 'life' 'help' 'new' 'feel' 'read' 'end' 'thing' 'give']\n",
      " ['much' 'one' 'problem' 'use' 'help' 'work' 'good' 'need' 'look'\n",
      "  'reader']\n",
      " ['use' 'little' 'give' 'different' 'event' 'would' 'history' 'probably'\n",
      "  'may' 'find']\n",
      " ['use' 'guide' 'world' 'gibbon' 'life' 'novel' 'find' 'guralnick' 'two'\n",
      "  'biography']\n",
      " ['art' 'great' 'way' 'take' 'actually' 'life' 'artist' 'year' 'love'\n",
      "  'learn']\n",
      " ['read' 'find' 'would' 'really' 'say' 'author' 'story' 'point' 'make'\n",
      "  'know']\n",
      " ['think' 'people' 'god' 'game' 'belief' 'well' 'end' 'still' 'one'\n",
      "  'follow']\n",
      " ['know' 'doe' 'world' 'one' 'get' 'come' 'tell' 'live' 'say' 'year']\n",
      " ['country' 'world' 'year' 'power' 'fight' 'show' 'name' 'political'\n",
      "  'military' 'many']\n",
      " ['write' 'character' 'old' 'part' 'make' 'good' 'like' 'end' 'long'\n",
      "  'child']\n",
      " ['story' 'interest' 'historical' 'norris' 'fiction' 'human' 'point'\n",
      "  'hunt' 'narrative' 'theme']\n",
      " ['like' 'make' 'character' 'would' 'get' 'doe' 'good' 'thing' 'try'\n",
      "  'say']\n",
      " ['people' 'must' 'rise' 'many' 'live' 'power' 'family' 'year' 'order'\n",
      "  'man']\n",
      " ['one' 'also' 'little' 'read' 'great' 'another' 'lot' 'really' 'man'\n",
      "  'use']\n",
      " ['food' 'use' 'cook' 'meal' 'garden' 'easy' 'serve' 'herb' 'two'\n",
      "  'rather']\n",
      " ['find' 'work' 'read' 'idea' 'get' 'want' 'way' 'think' 'understand'\n",
      "  'give']\n",
      " ['one' 'write' 'well' 'love' 'good' 'think' 'get' 'time' 'find' 'great']\n",
      " ['good' 'much' 'well' 'also' 'many' 'know' 'make' 'learn' 'people'\n",
      "  'would']\n",
      " ['read' 'like' 'think' 'good' 'believe' 'life' 'doe' 'understand' 'man'\n",
      "  'time']\n",
      " ['become' 'freddie' 'wine' 'act' 'well' 'behavior' 'whelan' 'spiritual'\n",
      "  'limit' 'find']\n",
      " ['boy' 'force' 'show' 'try' 'hope' 'man' 'fantasy' 'ask' 'get' 'learn']\n",
      " ['' 'news' 'use' 'way' 'many' 'read' 'think' 'author' 'time' 'medium']\n",
      " ['show' 'james' 'people' 'long' 'use' 'day' 'author' 'piece' 'century'\n",
      "  'make']\n",
      " ['world' 'good' 'another' 'like' 'work' 'present' 'modern' 'every'\n",
      "  'smith' 'ancient']\n",
      " ['make' 'interest' 'novel' 'may' 'god' 'late' 'need' 'tell' 'write'\n",
      "  'try']\n",
      " ['great' 'interest' 'read' 'way' 'author' 'story' 'new' 'much' 'mind'\n",
      "  'job']\n",
      " ['story' 'much' 'like' 'great' 'journey' 'work' 'really' 'give' 'old'\n",
      "  'enough']\n",
      " ['write' 'much' 'good' 'great' 'love' 'page' 'one' 'like' 'many'\n",
      "  'review']\n",
      " ['see' 'prospero' 'cross' 'want' 'american' 'church' 'word' 'become'\n",
      "  'start' 'detail']\n",
      " ['text' 'much' 'one' 'ever' 'like' 'say' 'exercise' 'student' 'may'\n",
      "  'system']\n",
      " ['work' 'give' 'one' 'good' 'would' 'pattern' 'love' 'easy' 'make'\n",
      "  'anyone']\n",
      " ['jason' 'reader' 'read' 'well' 'little' 'even' 'change' 'think' 'need'\n",
      "  'work']\n",
      " ['world' 'great' 'way' 'business' 'risk' 'life' 'avoid' 'one'\n",
      "  'government' 'decision']\n",
      " ['idea' 'film' 'miss' 'process' 'spirituality' 'reason' 'almost' 'work'\n",
      "  'race' 'thousand']\n",
      " ['man' 'become' 'order' 'self' 'parent' 'society' 'take' 'south'\n",
      "  'personal' 'age']\n",
      " ['write' 'little' 'age' 'read' 'doe' 'life' 'many' 'need' 'great' 'must']\n",
      " ['novel' 'even' 'many' 'good' 'time' 'leave' 'great' 'people' 'work'\n",
      "  'look']\n",
      " ['live' 'true' 'young' 'tell' 'life' 'another' 'way' 'child' 'asher'\n",
      "  'mystery']\n",
      " ['like' 'much' 'good' 'write' 'would' 'life' 'get' 'feel' 'little'\n",
      "  'make']\n",
      " ['would' 'much' 'like' 'find' 'take' 'bob' 'little' 'well' 'cover'\n",
      "  'worth']\n",
      " ['would' 'back' 'day' 'feel' 'theory' 'find' 'new' 'poor' 'love'\n",
      "  'include']\n",
      " ['would' 'recommend' 'use' 'also' 'many' 'organization' 'social'\n",
      "  'education' 'study' 'example']\n",
      " ['much' 'job' 'love' 'grow' 'difficult' 'well' 'live' 'emotion' 'upon'\n",
      "  'relationship']\n",
      " ['child' 'family' 'much' 'way' 'people' 'know' 'come' 'would' 'erica'\n",
      "  'one']\n",
      " ['much' 'story' 'interest' 'time' 'anyone' 'come' 'well' 'mcbride'\n",
      "  'even' 'water']\n",
      " ['french' 'early' 'plant' 'long' 'truth' 'understand' 'late' 'year'\n",
      "  'language' 'many']\n",
      " ['give' 'like' 'one' 'great' 'love' 'recommend' 'anyone' 'write' 'think'\n",
      "  'even']\n",
      " ['like' 'read' 'think' 'time' 'good' 'would' 'many' '' 'get' 'come']\n",
      " ['author' 'come' 'doe' 'time' 'century' 'history' 'funny' 'translation'\n",
      "  'religious' 'far']\n",
      " ['much' 'page' 'one' 'read' 'love' 'library' 'find' 'keep' 'amaze'\n",
      "  'even']]\n"
     ]
    }
   ],
   "source": [
    "K = 50\n",
    "alpha = 0.01\n",
    "beta = alpha\n",
    "\n",
    "word_topic_prob_001_50 = LDA_Gibbs_Sampler(docs_int, vocab_size, K, it, alpha, beta, freqs)\n",
    "show_words_by_topic(word_topic_prob_001_50, vocab, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
