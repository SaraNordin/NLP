{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assignment 2\n",
    "## NLP\n",
    "## Test adnan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import math\n",
    "\n",
    "import time\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Import stopwords\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords list\n",
    "\n",
    "stop = list(string.punctuation)\n",
    "for x in stopwords.words('english'):\n",
    "    stop.append(x)\n",
    "\n",
    "'''\n",
    "Add a couple of more stopwords which were observed manually\n",
    "'''\n",
    "for x in ['-PRON-','\\n','...','..',\"'d'\",\"n't\"]:\n",
    "    stop.append(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "\n",
    "corpus = 'books.txt'\n",
    "encoding = 'ISO-8859-1'\n",
    "\n",
    "alpha = 0.1 # Hyperparameter\n",
    "beta = 0.1 # Hyperparameter\n",
    "K = 30 # Nr of topics\n",
    "M = 100000 # Corpus size\n",
    "it = 100 # Nr of iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(corpus:str, encoding:str, nr_tokens:int, stop:list) -> list():\n",
    "    \n",
    "    '''\n",
    "    Only consider 100 000 tokens.\n",
    "    Removes strings from stopword-list from corpus.\n",
    "    Removes string with fewer characters than 3.\n",
    "    Keeps lemmatized words.\n",
    "    '''\n",
    "    \n",
    "    token_count = 0\n",
    "    docs = list()\n",
    "    docs_count = 0\n",
    "    freqs = Counter()  \n",
    "    \n",
    "    with open(corpus, encoding = encoding) as f:\n",
    "        \n",
    "        while token_count < nr_tokens:\n",
    "            \n",
    "            doc = f.readline()\n",
    "            docs_count += 1\n",
    "            temp_doc = list()\n",
    "            \n",
    "            # Lemmatizion of tokens\n",
    "            result = nlp(doc.lower())\n",
    "            \n",
    "            for token in result:\n",
    "                if len(token.lemma_) > 2:\n",
    "                    if token.lemma_ not in stop:\n",
    "                        token_count += 1\n",
    "                        freqs[token.lemma_] += 1\n",
    "                        temp_doc.append(token.lemma_)\n",
    "                        \n",
    "            docs.append(temp_doc)\n",
    "\n",
    "    print('Nr of tokens:' + str(token_count))\n",
    "    print('Nr of docs:' + str(docs_count))\n",
    "    \n",
    "    return docs\n",
    "\n",
    "def map_to_int(docs:list()) -> (list(), int, Counter()):\n",
    "    \n",
    "    '''\n",
    "    Create bag of words from cleaned and smaller corpus.\n",
    "    Associate each word in bag with an unique integer,\n",
    "    ranging from 0 (most common word) to length of bag of words.\n",
    "    Map each token in docs to the respective int. Return this list of list of ints.\n",
    "    '''\n",
    "    \n",
    "    freqs = Counter()\n",
    "    for doc in docs:\n",
    "        for token in doc:\n",
    "            freqs[token] += 1\n",
    "    most_common = freqs.most_common()\n",
    "    \n",
    "    token_to_int = []\n",
    "    for i in range(len(most_common)):\n",
    "        token_to_int.append(most_common[i][0])\n",
    "\n",
    "    # Get pairs of elements    \n",
    "    mapping = zip(token_to_int, range(0,len(token_to_int)))\n",
    "    \n",
    "    # Make pairs into a dictionary\n",
    "    vocab = dict(mapping)\n",
    "        \n",
    "    # Match token to int\n",
    "    docs_int = list()\n",
    "    \n",
    "    for doc in docs:\n",
    "        docs_int.append(list([vocab.get(x) for x in doc]))\n",
    "        \n",
    "    return docs_int, len(vocab), vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr of tokens:100010\n",
      "Nr of docs:1246\n"
     ]
    }
   ],
   "source": [
    "docs = clean_corpus(corpus=corpus, encoding=encoding, nr_tokens=M, stop=stop)\n",
    "docs_mapped, vocab_size, vocab = map_to_int(docs)\n",
    "n_docs = len(docs_mapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lda(docs:list(), n_docs:int, n_topics:int, vocab_size:int) -> (np.matrix, np.matrix, np.matrix):\n",
    "    \n",
    "    # \\Theta\n",
    "    theta = np.zeros((n_docs,n_topics))\n",
    "    for d in range(n_docs):\n",
    "        theta[d] = np.random.dirichlet(alpha * np.ones(n_topics))\n",
    "\n",
    "    # \\phi\n",
    "    phi = np.zeros((n_topics, vocab_size))\n",
    "    for z in range(n_topics):\n",
    "        phi[z] = np.random.dirichlet(beta*np.ones(vocab_size))\n",
    "    \n",
    "    # Word topic assignment\n",
    "    Z_dj = list()\n",
    "    for d in range(n_docs):\n",
    "        Z_dj.append([0]*len(docs_mapped[d]))\n",
    "    \n",
    "    # Init wt by randomly assigning topics to each word in corpus\n",
    "    for d in range(n_docs):\n",
    "        for j in range(len(docs[d])):\n",
    "            Z_dj[d][j] = np.random.randint(n_topics) # Generate random topic\n",
    "        \n",
    "    return theta, phi, Z_dj\n",
    "\n",
    "def update(docs, Z_dj, n_docs, n_topics, vocab_size, theta, phi):\n",
    "    \n",
    "    # Counter for word j in document d\n",
    "    nd = np.zeros((n_docs, n_topics))\n",
    "    mk = np.zeros((n_topics, vocab_size))\n",
    "    \n",
    "    for d in range(n_docs):\n",
    "        for j in range(len(docs[d])):\n",
    "            nd[d][Z_dj[d][j]] += 1\n",
    "            mk[Z_dj[d][j]][docs[d][j]] += 1\n",
    "            \n",
    "    return nd, mk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, phi, Z_dj = init_lda(docs_mapped, n_docs, K, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs(docs:list(), n_docs:int, n_topics:int, vocab_size:int, max_iterations:int, alpha:int, beta:int):\n",
    "\n",
    "    start_time = time.time()\n",
    "    theta, phi, Z_dj = init_lda(docs, n_docs=n_docs, n_topics=n_topics, vocab_size=vocab_size)\n",
    "    print('Matrices initialized.')\n",
    "\n",
    "    for it in range(max_iterations):\n",
    "        #print('Iteration ' + str(it))\n",
    "        \n",
    "        nd, mk = update(docs, Z_dj, n_docs, n_topics, vocab_size, theta, phi)\n",
    "        \n",
    "        for d in range(n_docs):\n",
    "            \n",
    "            # Update \\Theta\n",
    "            theta[d] = np.random.dirichlet(alpha + nd[d])\n",
    "            \n",
    "        #print('Theta updated.')\n",
    "        \n",
    "        for k in range(n_topics):\n",
    "            \n",
    "            # Update \\phi\n",
    "            phi[k] = np.random.dirichlet(beta + mk[k])\n",
    "            \n",
    "        #print('phi updated.')\n",
    "        \n",
    "        for d in range(n_docs):\n",
    "            for j in range(len(docs[d])):\n",
    "                \n",
    "                p_dw = np.exp(np.log(theta[d]) + np.log([row[docs[d][j]] for row in phi]))\n",
    "                p_dw /= sum(p_dw)\n",
    "                \n",
    "                Z_dj[d][j] = np.random.multinomial(1, p_dw).argmax()\n",
    "                \n",
    "        if (it+1) % 10 == 0:\n",
    "            print('Iteration ' + str(it+1))\n",
    "                \n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"Elapsed time: \", elapsed_time)\n",
    "    \n",
    "    return mk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrices initialized.\n",
      "Iteration 10\n",
      "Iteration 20\n",
      "Iteration 30\n",
      "Iteration 40\n",
      "Iteration 50\n",
      "Iteration 60\n",
      "Iteration 70\n",
      "Iteration 80\n",
      "Iteration 90\n",
      "Iteration 100\n",
      "Elapsed time:  438.7033472061157\n"
     ]
    }
   ],
   "source": [
    "mk = gibbs(docs_mapped, n_docs, K, vocab_size, it, alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "tops = list()\n",
    "for k in range(K):\n",
    "    order = np.array(mk[k]).argsort()[::-1][:10]\n",
    "    dt = list()\n",
    "    for x in order:\n",
    "        for k,v in vocab.items():\n",
    "            if x==v:\n",
    "                dt.append(k)\n",
    "    tops.append(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['war',\n",
       "  'military',\n",
       "  'soviet',\n",
       "  'president',\n",
       "  'army',\n",
       "  'red',\n",
       "  'vietnam',\n",
       "  'u.s',\n",
       "  'american',\n",
       "  'force'],\n",
       " ['series',\n",
       "  'esk',\n",
       "  'transformation',\n",
       "  'century',\n",
       "  'musil',\n",
       "  'discworld',\n",
       "  'magic',\n",
       "  'eve',\n",
       "  'dean',\n",
       "  'various'],\n",
       " ['book',\n",
       "  'read',\n",
       "  'recommend',\n",
       "  'story',\n",
       "  'game',\n",
       "  'love',\n",
       "  'star',\n",
       "  'highly',\n",
       "  'also',\n",
       "  'really'],\n",
       " ['book',\n",
       "  'one',\n",
       "  'good',\n",
       "  'use',\n",
       "  'read',\n",
       "  'many',\n",
       "  'find',\n",
       "  'also',\n",
       "  'make',\n",
       "  'well'],\n",
       " ['....',\n",
       "  'john',\n",
       "  'card',\n",
       "  'real',\n",
       "  'rich',\n",
       "  'financial',\n",
       "  'dirk',\n",
       "  'company',\n",
       "  'estate',\n",
       "  'poor'],\n",
       " ['news',\n",
       "  'human',\n",
       "  'people',\n",
       "  'drug',\n",
       "  'number',\n",
       "  'state',\n",
       "  'cancer',\n",
       "  'society',\n",
       "  'politic',\n",
       "  'report'],\n",
       " ['plant',\n",
       "  'page',\n",
       "  'guide',\n",
       "  'north',\n",
       "  'title',\n",
       "  'common',\n",
       "  'picture',\n",
       "  'javascript',\n",
       "  'design',\n",
       "  'color'],\n",
       " ['human',\n",
       "  'horse',\n",
       "  'machine',\n",
       "  'behavior',\n",
       "  'whelan',\n",
       "  'brain',\n",
       "  'model',\n",
       "  'von',\n",
       "  'intelligence',\n",
       "  'computer'],\n",
       " ['theory',\n",
       "  'evidence',\n",
       "  'science',\n",
       "  'study',\n",
       "  'scientific',\n",
       "  'claim',\n",
       "  'system',\n",
       "  'elbow',\n",
       "  'order',\n",
       "  'approach'],\n",
       " ['cat',\n",
       "  'translation',\n",
       "  'original',\n",
       "  'funny',\n",
       "  'ever',\n",
       "  'nietzsche',\n",
       "  'totally',\n",
       "  'dialect',\n",
       "  'iran',\n",
       "  'academic'],\n",
       " ['book',\n",
       "  'one',\n",
       "  'story',\n",
       "  'read',\n",
       "  'novel',\n",
       "  'character',\n",
       "  'good',\n",
       "  'make',\n",
       "  'reader',\n",
       "  'love'],\n",
       " ['highet',\n",
       "  'oil',\n",
       "  'vampire',\n",
       "  'henry',\n",
       "  'water',\n",
       "  'harper',\n",
       "  'house',\n",
       "  'energy',\n",
       "  'lily',\n",
       "  'ghost'],\n",
       " ['alex',\n",
       "  'helga',\n",
       "  'spenser',\n",
       "  'syllable',\n",
       "  'man',\n",
       "  'pirsig',\n",
       "  'line',\n",
       "  'appeal',\n",
       "  'government',\n",
       "  'queene'],\n",
       " ['british',\n",
       "  'anne',\n",
       "  'mention',\n",
       "  'cultural',\n",
       "  'game',\n",
       "  'thompson',\n",
       "  'hint',\n",
       "  'hal',\n",
       "  'creepy',\n",
       "  'lodge'],\n",
       " ['woman',\n",
       "  'man',\n",
       "  'smith',\n",
       "  'sex',\n",
       "  'body',\n",
       "  'nature',\n",
       "  'female',\n",
       "  'another',\n",
       "  'pleasure',\n",
       "  'orgasm'],\n",
       " ['issue',\n",
       "  'french',\n",
       "  'society',\n",
       "  'west',\n",
       "  'school',\n",
       "  'daughter',\n",
       "  'parent',\n",
       "  'order',\n",
       "  'reveal',\n",
       "  'religious'],\n",
       " ['love',\n",
       "  'year',\n",
       "  'soul',\n",
       "  'old',\n",
       "  'friend',\n",
       "  'food',\n",
       "  'spiritual',\n",
       "  'life',\n",
       "  'chinese',\n",
       "  'wonderful'],\n",
       " ['music',\n",
       "  'fish',\n",
       "  'pattern',\n",
       "  'slave',\n",
       "  'land',\n",
       "  'boat',\n",
       "  'nightjohn',\n",
       "  'pearl',\n",
       "  'bach',\n",
       "  'lung'],\n",
       " ['recipe',\n",
       "  'diet',\n",
       "  'school',\n",
       "  'eat',\n",
       "  'food',\n",
       "  'asher',\n",
       "  'cook',\n",
       "  'freddie',\n",
       "  'cookbook',\n",
       "  'meal'],\n",
       " ['jason',\n",
       "  'erica',\n",
       "  'people',\n",
       "  'james',\n",
       "  'reader',\n",
       "  'novel',\n",
       "  'valyan',\n",
       "  'business',\n",
       "  'kissinger',\n",
       "  'black'],\n",
       " ['bob',\n",
       "  'leaphorn',\n",
       "  'name',\n",
       "  'chee',\n",
       "  'greek',\n",
       "  'william',\n",
       "  'doug',\n",
       "  'hillerman',\n",
       "  'navajo',\n",
       "  'mitchell'],\n",
       " ['book',\n",
       "  'read',\n",
       "  'like',\n",
       "  'get',\n",
       "  'one',\n",
       "  'good',\n",
       "  'well',\n",
       "  'great',\n",
       "  'find',\n",
       "  'write'],\n",
       " ['history',\n",
       "  'god',\n",
       "  'human',\n",
       "  'social',\n",
       "  'account',\n",
       "  'world',\n",
       "  'pray',\n",
       "  'detail',\n",
       "  'race',\n",
       "  'interested'],\n",
       " ['sam',\n",
       "  'norris',\n",
       "  'organization',\n",
       "  'biography',\n",
       "  'guralnick',\n",
       "  'cooke',\n",
       "  'provide',\n",
       "  'event',\n",
       "  'chuck',\n",
       "  'world'],\n",
       " ['america',\n",
       "  'show',\n",
       "  'american',\n",
       "  'mcginnis',\n",
       "  'mcdonald',\n",
       "  'medium',\n",
       "  'customer',\n",
       "  'new',\n",
       "  'call',\n",
       "  'united'],\n",
       " ['book',\n",
       "  'work',\n",
       "  'great',\n",
       "  'edition',\n",
       "  'dog',\n",
       "  'another',\n",
       "  'use',\n",
       "  'one',\n",
       "  'well',\n",
       "  'write'],\n",
       " ['book',\n",
       "  'would',\n",
       "  'one',\n",
       "  'author',\n",
       "  'time',\n",
       "  'read',\n",
       "  'make',\n",
       "  'even',\n",
       "  'find',\n",
       "  'also'],\n",
       " ['child',\n",
       "  'garden',\n",
       "  'herb',\n",
       "  'hug',\n",
       "  'toad',\n",
       "  'mental',\n",
       "  'dillard',\n",
       "  'steinbeck',\n",
       "  'truly',\n",
       "  'feeling'],\n",
       " ['life',\n",
       "  'young',\n",
       "  'story',\n",
       "  'man',\n",
       "  'child',\n",
       "  'family',\n",
       "  'people',\n",
       "  'world',\n",
       "  'father',\n",
       "  'time'],\n",
       " ['student',\n",
       "  'book',\n",
       "  'work',\n",
       "  'teacher',\n",
       "  'great',\n",
       "  'use',\n",
       "  'child',\n",
       "  'year',\n",
       "  'understand',\n",
       "  'good']]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
