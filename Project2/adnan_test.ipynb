{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assignment 2\n",
    "## NLP\n",
    "## Test adnan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import math\n",
    "\n",
    "import time\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Import stopwords\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords list\n",
    "\n",
    "stop = list(string.punctuation)\n",
    "for x in stopwords.words('english'):\n",
    "    stop.append(x)\n",
    "\n",
    "'''\n",
    "Add a couple of more stopwords which were observed manually\n",
    "'''\n",
    "for x in ['-PRON-','\\n','...','..',\"'d'\",\"n't\"]:\n",
    "    stop.append(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "\n",
    "corpus = 'books.txt'\n",
    "encoding = 'ISO-8859-1'\n",
    "\n",
    "alpha = 0.1 # Hyperparameter\n",
    "beta = 0.1 # Hyperparameter\n",
    "K = 30 # Nr of topics\n",
    "M = 100000 # Corpus size\n",
    "it = 100 # Nr of iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(corpus:str, encoding:str, nr_tokens:int, stop:list) -> list():\n",
    "    \n",
    "    '''\n",
    "    Only consider 100 000 tokens.\n",
    "    Removes strings from stopword-list from corpus.\n",
    "    Removes string with fewer characters than 3.\n",
    "    Keeps lemmatized words.\n",
    "    '''\n",
    "    \n",
    "    token_count = 0\n",
    "    docs = list()\n",
    "    docs_count = 0\n",
    "    freqs = Counter()  \n",
    "    \n",
    "    with open(corpus, encoding = encoding) as f:\n",
    "        \n",
    "        while token_count < nr_tokens:\n",
    "            \n",
    "            doc = f.readline()\n",
    "            docs_count += 1\n",
    "            temp_doc = list()\n",
    "            \n",
    "            # Lemmatizion of tokens\n",
    "            result = nlp(doc.lower())\n",
    "            \n",
    "            for token in result:\n",
    "                if len(token.lemma_) > 2:\n",
    "                    if token.lemma_ not in stop:\n",
    "                        token_count += 1\n",
    "                        freqs[token.lemma_] += 1\n",
    "                        temp_doc.append(token.lemma_)\n",
    "                        \n",
    "            docs.append(temp_doc)\n",
    "\n",
    "    print('Nr of tokens:' + str(token_count))\n",
    "    print('Nr of docs:' + str(docs_count))\n",
    "    \n",
    "    return docs\n",
    "\n",
    "def map_to_int(docs:list()) -> (list(), int, Counter()):\n",
    "    \n",
    "    '''\n",
    "    Create bag of words from cleaned and smaller corpus.\n",
    "    Associate each word in bag with an unique integer,\n",
    "    ranging from 0 (most common word) to length of bag of words.\n",
    "    Map each token in docs to the respective int. Return this list of list of ints.\n",
    "    '''\n",
    "    \n",
    "    freqs = Counter()\n",
    "    for doc in docs:\n",
    "        for token in doc:\n",
    "            freqs[token] += 1\n",
    "    most_common = freqs.most_common()\n",
    "    \n",
    "    token_to_int = []\n",
    "    for i in range(len(most_common)):\n",
    "        token_to_int.append(most_common[i][0])\n",
    "\n",
    "    # Get pairs of elements    \n",
    "    mapping = zip(token_to_int, range(0,len(token_to_int)))\n",
    "    \n",
    "    # Make pairs into a dictionary\n",
    "    vocab = dict(mapping)\n",
    "        \n",
    "    # Match token to int\n",
    "    docs_int = list()\n",
    "    \n",
    "    for doc in docs:\n",
    "        docs_int.append(list([vocab.get(x) for x in doc]))\n",
    "        \n",
    "    return docs_int, len(vocab), vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr of tokens:100010\n",
      "Nr of docs:1246\n"
     ]
    }
   ],
   "source": [
    "docs = clean_corpus(corpus=corpus, encoding=encoding, nr_tokens=M, stop=stop)\n",
    "docs_mapped, vocab_size, vocab = map_to_int(docs)\n",
    "n_docs = len(docs_mapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lda(docs:list(), n_docs:int, n_topics:int, vocab_size:int) -> (np.matrix, np.matrix, np.matrix):\n",
    "    \n",
    "    # \\Theta\n",
    "    theta = np.zeros((n_docs,n_topics))\n",
    "    for d in range(n_docs):\n",
    "        theta[d] = np.random.dirichlet(alpha * np.ones(n_topics))\n",
    "\n",
    "    # \\phi\n",
    "    phi = np.zeros((n_topics, vocab_size))\n",
    "    for z in range(n_topics):\n",
    "        phi[z] = np.random.dirichlet(beta*np.ones(vocab_size))\n",
    "    \n",
    "    # Word topic assignment\n",
    "    Z_dj = list()\n",
    "    for d in range(n_docs):\n",
    "        Z_dj.append([0]*len(docs_mapped[d]))\n",
    "    \n",
    "    # Init wt by randomly assigning topics to each word in corpus\n",
    "    for d in range(n_docs):\n",
    "        for j in range(len(docs[d])):\n",
    "            Z_dj[d][j] = np.random.randint(n_topics) # Generate random topic\n",
    "        \n",
    "    return theta, phi, Z_dj\n",
    "\n",
    "def update(docs, Z_dj, n_docs, n_topics, vocab_size, theta, phi):\n",
    "    \n",
    "    # Counter for word j in document d\n",
    "    nd = np.zeros((n_docs, n_topics))\n",
    "    mk = np.zeros((n_topics, vocab_size))\n",
    "    \n",
    "    for d in range(n_docs):\n",
    "        for j in range(len(docs[d])):\n",
    "            nd[d][Z_dj[d][j]] += 1\n",
    "            mk[Z_dj[d][j]][docs[d][j]] += 1\n",
    "            \n",
    "    return nd, mk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, phi, Z_dj = init_lda(docs_mapped, n_docs, K, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs(docs:list(), n_docs:int, n_topics:int, vocab_size:int, max_iterations:int, alpha:int, beta:int):\n",
    "\n",
    "    start_time = time.time()\n",
    "    theta, phi, Z_dj = init_lda(docs, n_docs=n_docs, n_topics=n_topics, vocab_size=vocab_size)\n",
    "    print('Matrices initialized.')\n",
    "\n",
    "    for it in range(max_iterations):\n",
    "        #print('Iteration ' + str(it))\n",
    "        \n",
    "        nd, mk = update(docs, Z_dj, n_docs, n_topics, vocab_size, theta, phi)\n",
    "        \n",
    "        for d in range(n_docs):\n",
    "            \n",
    "            # Update \\Theta\n",
    "            theta[d] = np.random.dirichlet(alpha + nd[d])\n",
    "            \n",
    "        #print('Theta updated.')\n",
    "        \n",
    "        for k in range(n_topics):\n",
    "            \n",
    "            # Update \\phi\n",
    "            phi[k] = np.random.dirichlet(beta + mk[k])\n",
    "            \n",
    "        #print('phi updated.')\n",
    "        \n",
    "        for d in range(n_docs):\n",
    "            for j in range(len(docs[d])):\n",
    "                \n",
    "                p_dw = np.exp(np.log(theta[d]) + np.log([row[docs[d][j]] for row in phi]))\n",
    "                p_dw /= sum(p_dw)\n",
    "                \n",
    "                Z_dj[d][j] = np.random.multinomial(1, p_dw).argmax()\n",
    "                \n",
    "        if (it+1) % 10 == 0:\n",
    "            print('Iteration ' + str(it+1))\n",
    "                \n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"Elapsed time: \", elapsed_time)\n",
    "    \n",
    "    return mk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrices initialized.\n",
      "Iteration 10\n",
      "Iteration 20\n",
      "Iteration 30\n",
      "Iteration 40\n",
      "Iteration 50\n",
      "Iteration 60\n",
      "Iteration 70\n",
      "Iteration 80\n",
      "Iteration 90\n",
      "Iteration 100\n",
      "Elapsed time:  438.7033472061157\n"
     ]
    }
   ],
   "source": [
    "mk = gibbs(docs_mapped, n_docs, K, vocab_size, it, alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "tops = list()\n",
    "for k in range(K):\n",
    "    order = np.array(mk[k]).argsort()[::-1][:10]\n",
    "    dt = list()\n",
    "    for x in order:\n",
    "        for k,v in vocab.items():\n",
    "            if x==v:\n",
    "                dt.append(k)\n",
    "    tops.append(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 ['war', 'military', 'soviet', 'president', 'army', 'red', 'vietnam', 'u.s', 'american', 'force']\n",
      "Topic 1 ['series', 'esk', 'transformation', 'century', 'musil', 'discworld', 'magic', 'eve', 'dean', 'various']\n",
      "Topic 2 ['book', 'read', 'recommend', 'story', 'game', 'love', 'star', 'highly', 'also', 'really']\n",
      "Topic 3 ['book', 'one', 'good', 'use', 'read', 'many', 'find', 'also', 'make', 'well']\n",
      "Topic 4 ['....', 'john', 'card', 'real', 'rich', 'financial', 'dirk', 'company', 'estate', 'poor']\n",
      "Topic 5 ['news', 'human', 'people', 'drug', 'number', 'state', 'cancer', 'society', 'politic', 'report']\n",
      "Topic 6 ['plant', 'page', 'guide', 'north', 'title', 'common', 'picture', 'javascript', 'design', 'color']\n",
      "Topic 7 ['human', 'horse', 'machine', 'behavior', 'whelan', 'brain', 'model', 'von', 'intelligence', 'computer']\n",
      "Topic 8 ['theory', 'evidence', 'science', 'study', 'scientific', 'claim', 'system', 'elbow', 'order', 'approach']\n",
      "Topic 9 ['cat', 'translation', 'original', 'funny', 'ever', 'nietzsche', 'totally', 'dialect', 'iran', 'academic']\n",
      "Topic 10 ['book', 'one', 'story', 'read', 'novel', 'character', 'good', 'make', 'reader', 'love']\n",
      "Topic 11 ['highet', 'oil', 'vampire', 'henry', 'water', 'harper', 'house', 'energy', 'lily', 'ghost']\n",
      "Topic 12 ['alex', 'helga', 'spenser', 'syllable', 'man', 'pirsig', 'line', 'appeal', 'government', 'queene']\n",
      "Topic 13 ['british', 'anne', 'mention', 'cultural', 'game', 'thompson', 'hint', 'hal', 'creepy', 'lodge']\n",
      "Topic 14 ['woman', 'man', 'smith', 'sex', 'body', 'nature', 'female', 'another', 'pleasure', 'orgasm']\n",
      "Topic 15 ['issue', 'french', 'society', 'west', 'school', 'daughter', 'parent', 'order', 'reveal', 'religious']\n",
      "Topic 16 ['love', 'year', 'soul', 'old', 'friend', 'food', 'spiritual', 'life', 'chinese', 'wonderful']\n",
      "Topic 17 ['music', 'fish', 'pattern', 'slave', 'land', 'boat', 'nightjohn', 'pearl', 'bach', 'lung']\n",
      "Topic 18 ['recipe', 'diet', 'school', 'eat', 'food', 'asher', 'cook', 'freddie', 'cookbook', 'meal']\n",
      "Topic 19 ['jason', 'erica', 'people', 'james', 'reader', 'novel', 'valyan', 'business', 'kissinger', 'black']\n",
      "Topic 20 ['bob', 'leaphorn', 'name', 'chee', 'greek', 'william', 'doug', 'hillerman', 'navajo', 'mitchell']\n",
      "Topic 21 ['book', 'read', 'like', 'get', 'one', 'good', 'well', 'great', 'find', 'write']\n",
      "Topic 22 ['history', 'god', 'human', 'social', 'account', 'world', 'pray', 'detail', 'race', 'interested']\n",
      "Topic 23 ['sam', 'norris', 'organization', 'biography', 'guralnick', 'cooke', 'provide', 'event', 'chuck', 'world']\n",
      "Topic 24 ['america', 'show', 'american', 'mcginnis', 'mcdonald', 'medium', 'customer', 'new', 'call', 'united']\n",
      "Topic 25 ['book', 'work', 'great', 'edition', 'dog', 'another', 'use', 'one', 'well', 'write']\n",
      "Topic 26 ['book', 'would', 'one', 'author', 'time', 'read', 'make', 'even', 'find', 'also']\n",
      "Topic 27 ['child', 'garden', 'herb', 'hug', 'toad', 'mental', 'dillard', 'steinbeck', 'truly', 'feeling']\n",
      "Topic 28 ['life', 'young', 'story', 'man', 'child', 'family', 'people', 'world', 'father', 'time']\n",
      "Topic 29 ['student', 'book', 'work', 'teacher', 'great', 'use', 'child', 'year', 'understand', 'good']\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(tops):\n",
    "    print('Topic ' + str(i), topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
