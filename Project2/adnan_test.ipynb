{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assignment 2\n",
    "## NLP\n",
    "## Test adnan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import math\n",
    "\n",
    "import time\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Import stopwords\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords list\n",
    "\n",
    "stop = list(string.punctuation)\n",
    "for x in stopwords.words('english'):\n",
    "    stop.append(x)\n",
    "\n",
    "'''\n",
    "Add a couple of more stopwords which were observed manually\n",
    "'''\n",
    "for x in ['-PRON-','\\n','...','..',\"'d'\",\"n't\"]:\n",
    "    stop.append(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "\n",
    "corpus = 'books.txt'\n",
    "encoding = 'ISO-8859-1'\n",
    "\n",
    "alpha = 0.1 # Hyperparameter\n",
    "beta = 0.1 # Hyperparameter\n",
    "K = 20 # Nr of topics\n",
    "M = 100000 # Corpus size\n",
    "it = 50 # Nr of iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(corpus:str, encoding:str, nr_tokens:int, stop:list) -> list():\n",
    "    \n",
    "    '''\n",
    "    Only consider 100 000 tokens.\n",
    "    Removes strings from stopword-list from corpus.\n",
    "    Removes string with fewer characters than 3.\n",
    "    Keeps lemmatized words.\n",
    "    '''\n",
    "    \n",
    "    token_count = 0\n",
    "    docs = list()\n",
    "    docs_count = 0\n",
    "    freqs = Counter()  \n",
    "    \n",
    "    with open(corpus, encoding = encoding) as f:\n",
    "        \n",
    "        while token_count < nr_tokens:\n",
    "            \n",
    "            doc = f.readline()\n",
    "            docs_count += 1\n",
    "            temp_doc = list()\n",
    "            \n",
    "            # Lemmatizion of tokens\n",
    "            result = nlp(doc.lower())\n",
    "            \n",
    "            for token in result:\n",
    "                if len(token.lemma_) > 2:\n",
    "                    if token.lemma_ not in stop:\n",
    "                        token_count += 1\n",
    "                        freqs[token.lemma_] += 1\n",
    "                        temp_doc.append(token.lemma_)\n",
    "                        \n",
    "            docs.append(temp_doc)\n",
    "\n",
    "    print('Nr of tokens:' + str(token_count))\n",
    "    print('Nr of docs:' + str(docs_count))\n",
    "    \n",
    "    return docs\n",
    "\n",
    "def map_to_int(docs:list()) -> (list(), int, Counter()):\n",
    "    \n",
    "    '''\n",
    "    Create bag of words from cleaned and smaller corpus.\n",
    "    Associate each word in bag with an unique integer,\n",
    "    ranging from 0 (most common word) to length of bag of words.\n",
    "    Map each token in docs to the respective int. Return this list of list of ints.\n",
    "    '''\n",
    "    \n",
    "    freqs = Counter()\n",
    "    for doc in docs:\n",
    "        for token in doc:\n",
    "            freqs[token] += 1\n",
    "    most_common = freqs.most_common()\n",
    "    \n",
    "    token_to_int = []\n",
    "    for i in range(len(most_common)):\n",
    "        token_to_int.append(most_common[i][0])\n",
    "\n",
    "    # Get pairs of elements    \n",
    "    mapping = zip(token_to_int, range(0,len(token_to_int)))\n",
    "    \n",
    "    # Make pairs into a dictionary\n",
    "    vocab = dict(mapping)\n",
    "        \n",
    "    # Match token to int\n",
    "    docs_int = list()\n",
    "    \n",
    "    for doc in docs:\n",
    "        docs_int.append(list([vocab.get(x) for x in doc]))\n",
    "        \n",
    "    return docs_int, len(vocab), most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr of tokens:100010\n",
      "Nr of docs:1246\n"
     ]
    }
   ],
   "source": [
    "docs = clean_corpus(corpus=corpus, encoding=encoding, nr_tokens=M, stop=stop)\n",
    "docs_mapped, vocab_size, freqs = map_to_int(docs)\n",
    "n_docs = len(docs_mapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lda(docs:list(), n_docs:int, n_topics:int, vocab_size:int) -> (np.matrix, np.matrix, np.matrix):\n",
    "    \n",
    "    # \\Theta\n",
    "    theta = np.zeros((n_docs,n_topics))\n",
    "    for d in range(n_docs):\n",
    "        theta[d] = np.random.dirichlet(alpha * np.ones(n_topics))\n",
    "\n",
    "    # \\phi\n",
    "    phi = np.zeros((n_topics, vocab_size))\n",
    "    for z in range(n_topics):\n",
    "        phi[z] = np.random.dirichlet(beta*np.ones(vocab_size))\n",
    "    \n",
    "    # Word topic assignment\n",
    "    Z_dj = list()\n",
    "    for d in range(n_docs):\n",
    "        Z_dj.append([0]*len(docs_mapped[d]))\n",
    "    \n",
    "    # Init wt by randomly assigning topics to each word in corpus\n",
    "    for d in range(n_docs):\n",
    "        for j in range(len(docs[d])):\n",
    "            Z_dj[d][j] = np.random.randint(n_topics) # Generate random topic\n",
    "        \n",
    "    return theta, phi, Z_dj\n",
    "\n",
    "def update(docs, Z_dj, n_docs, n_topics, vocab_size, theta, phi):\n",
    "    \n",
    "    # Counter for word j in document d\n",
    "    nd = np.zeros((n_docs, n_topics))\n",
    "    mk = np.zeros((n_topics, vocab_size))\n",
    "    \n",
    "    for d in range(n_docs):\n",
    "        for j in range(len(docs[d])):\n",
    "            nd[d][Z_dj[d][j]] += 1\n",
    "            mk[Z_dj[d][j]][docs[d][j]] += 1\n",
    "            \n",
    "    return nd, mk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, phi, Z_dj = init_lda(docs_mapped, n_docs, K, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs(docs:list(), n_docs:int, n_topics:int, vocab_size:int, max_iterations:int, alpha:int, beta:int):\n",
    "\n",
    "    start_time = time.time()\n",
    "    theta, phi, Z_dj = init_lda(docs, n_docs=n_docs, n_topics=n_topics, vocab_size=vocab_size)\n",
    "    print('Matrices initialized.')\n",
    "\n",
    "    for it in range(max_iterations):\n",
    "        #print('Iteration ' + str(it))\n",
    "        \n",
    "        nd, mk = update(docs, Z_dj, n_docs, n_topics, vocab_size, theta, phi)\n",
    "        \n",
    "        for d in range(n_docs):\n",
    "            \n",
    "            # Update \\Theta\n",
    "            theta[d] = np.random.dirichlet(alpha + nd[d])\n",
    "            \n",
    "        #print('Theta updated.')\n",
    "        \n",
    "        for k in range(n_topics):\n",
    "            \n",
    "            # Update \\phi\n",
    "            phi[k] = np.random.dirichlet(beta + mk[k])\n",
    "            \n",
    "        #print('phi updated.')\n",
    "        \n",
    "        for d in range(n_docs):\n",
    "            for j in range(len(docs[d])):\n",
    "                \n",
    "                p_dw = np.exp(np.log(theta[d]) + np.log([row[docs[d][j]] for row in phi]))\n",
    "                p_dw /= sum(p_dw)\n",
    "                \n",
    "                Z_dj[d][j] = np.random.multinomial(1, p_dw).argmax()\n",
    "                \n",
    "        if (it+1) % 10 == 0:\n",
    "            print('Iteration ' + str(it+1))\n",
    "                \n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"Elapsed time: \", elapsed_time)\n",
    "    \n",
    "    return Z_dj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrices initialized.\n",
      "Iteration 10\n",
      "Iteration 20\n",
      "Iteration 30\n",
      "Iteration 40\n",
      "Iteration 50\n",
      "Elapsed time:  192.09104204177856\n"
     ]
    }
   ],
   "source": [
    "z_end = gibbs(docs_mapped, n_docs, K, vocab_size, it, alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
