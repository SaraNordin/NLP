{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFXLEtyayf0a"
   },
   "source": [
    "If you're solving the optional task on BERT, you also need to install the `transformers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5vUfP7hyotU"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==3.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkIGcV66yNLb"
   },
   "source": [
    "# Design Choices in Biomedical Entity Recognition\n",
    "\n",
    "This is a notebook that you can use as a starting point for your solution for Assignment 4 on biomedical entity recognition. Some of this will resemble the demo code from the lecture on sequence prediction models.\n",
    "\n",
    "### Requirements\n",
    "In addition to PyTorch and torchtext, you'll need to make sure that the following libraries are available:\n",
    "- [torchtext](https://pytorch.org/text/stable/index.html) to deal with integer encoding and batching. Pre-installed on Colab. \n",
    "- [gensim](https://radimrehurek.com/gensim/) to load files containing pre-trained word embedding. This is available by default in Colab.\n",
    "- [pytorch-crf](https://pytorch-crf.readthedocs.io/en/stable/) to use the conditional random field output layer. Needs to be installed if you use Colab.\n",
    "\n",
    "If you want to solve the optional task involving BERT, you'll also need\n",
    "- [transformers](https://github.com/huggingface/transformers), which also needs to be installed if you're using Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "teK35VRtyNLc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import random\n",
    "\n",
    "#from torchcrf import CRF\n",
    "\n",
    "import gensim.downloader\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# If you're using BERT, import these:\n",
    "#from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina' \n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mt67FjwTyNLd"
   },
   "source": [
    "## 1. Reading the data in a tabular format\n",
    "\n",
    "The following function reads a file represented in a tabular format. In this format, each row corresponds to one token. For each token, there is a word and the BIO-coded named entity label, separated by whitespace. The sentences are separated by empty lines. Here is an example of a sentence.\n",
    "```\n",
    "In              O\n",
    "conclusion      O\n",
    ",               O\n",
    "hyperammonemic  B-Disease\n",
    "encephalopathy  I-Disease\n",
    "can             O\n",
    "occur           O\n",
    "in              O\n",
    "patients        O\n",
    "receiving       O\n",
    "continuous      O\n",
    "infusion        O\n",
    "of              O\n",
    "5               B-Chemical\n",
    "-               I-Chemical\n",
    "FU              I-Chemical\n",
    ".               O\n",
    "```\n",
    "The function reads the file in this format and returns a torchtext `Dataset`, which in turn consists of a number of `Example`. We will use just the words and the BIO labels, for the input and output respectively.\n",
    "\n",
    "If we provide a BERT tokenizer for splitting words into word pieces, this function will also apply the tokenizer to each word. For instance, the word `hyperammonemic` will be split into six word pieces: `h ##yper ##am ##mon ##em ##ic`. Note that in cases such as this one, we'll have to keep track of the output labels: the output will be set to `B-Disease` for the first word piece, and `I-Disease` for the rest of the pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cCzNzNWmyNLd"
   },
   "outputs": [],
   "source": [
    "def read_data(corpus_file, datafields, tokenizer, max_len, n_instances=None):\n",
    "    print(f'Reading sentences from {corpus_file}...', end=' ')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        examples = []\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        n_truncated = 0\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                \n",
    "                # For BERT, we need to truncate the sentences...\n",
    "                if len(tokens) > max_len-2:\n",
    "                    tokens = tokens[:max_len-2]\n",
    "                    labels = labels[:max_len-2]\n",
    "                    n_truncated += 1\n",
    "\n",
    "                examples.append(torchtext.data.Example.fromlist([tokens, labels], datafields))\n",
    "                \n",
    "                tokens = []\n",
    "                labels = []\n",
    "                    \n",
    "                if len(examples) == n_instances:\n",
    "                    break\n",
    "                \n",
    "            else:\n",
    "                columns = line.split()\n",
    "                \n",
    "                word = columns[0]\n",
    "                label = columns[-1]\n",
    "                    \n",
    "                if not tokenizer:\n",
    "                    # If we aren't using a BERT tokenizer, just add the word and label.\n",
    "                    tokens.append(word)\n",
    "                    labels.append(label)\n",
    "                else:\n",
    "                    # If we are using a BERT tokenizer, we need to append each word piece.\n",
    "                    # Note that if we split a word into pieces, we need to make sure we\n",
    "                    # add sensible output labels (e.g. I-Disease after B-Disease).\n",
    "                    for token in tokenizer.tokenize(word):\n",
    "                        tokens.append(token)\n",
    "                        labels.append(label)\n",
    "                        if label[0] == 'B':\n",
    "                            label = 'I' + label[1:]\n",
    "        print(f'Read {len(examples)} sentences, truncated {n_truncated}.')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iavhAPiLyNLd"
   },
   "source": [
    "## 2. The sentence encoder\n",
    "\n",
    "This is the part that will requite a few small modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "n81SJ6amyNLd"
   },
   "outputs": [],
   "source": [
    "class SentenceEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, word_field, char_field, gensim_model, conf):        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Create a word embedding layer using the auxiliary function defined below.\n",
    "        self.word_embedding = make_embedding_layer(word_field, gensim_model, conf)\n",
    "\n",
    "        word_dim = self.word_embedding.weight.shape[1]\n",
    "\n",
    "        if conf.use_characters and conf.char_emb_dim > 0:\n",
    "            ### YOUR CODE HERE (if you decide to consider the characters) ###\n",
    "            # ...\n",
    "            # word_dim += ...\n",
    "            raise Exception(\"We haven't implemented a character-based model!\")\n",
    "        else:\n",
    "            pass\n",
    "                \n",
    "        if conf.rnn_size > 0 and conf.rnn_depth > 0:\n",
    "            ### YOUR CODE HERE ###\n",
    "            # ... \n",
    "            raise Exception(\"We haven't implemented a RNN-based sentence representation!\")\n",
    "            self.output_size = 2*conf.rnn_size\n",
    "        else:\n",
    "            self.word_rnn = None\n",
    "            self.output_size = word_dim  \n",
    "        \n",
    "    def forward(self, words, chars):\n",
    "        # - words is a LongTensor of shape (n_sentences, n_words)\n",
    "        # - chars is a LongTensor of shape (n_sentences, n_words, n_chars)\n",
    "\n",
    "        word_repr = self.word_embedding(words)\n",
    "\n",
    "        #if SOMETHING (if you're considering the characters)\n",
    "        #    n_sent, n_words, n_chars = chars.shape\n",
    "        #    ... YOUR CODE HERE ...\n",
    "        #    word_repr = torch.cat([word_repr, ... SOMETHING ...])\n",
    "\n",
    "        if self.word_rnn:\n",
    "            ### YOUR CODE HERE ###\n",
    "            pass\n",
    "        else:\n",
    "            output = word_repr        \n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcsUWuW2yNLe"
   },
   "source": [
    "## 3. Using gensim to load pre-trained word embeddings \n",
    "\n",
    "The following two auxiliary functions help us load gensim models and to convert them for use with torchtext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QG8gnXwAyNLe"
   },
   "outputs": [],
   "source": [
    "def make_embedding_layer(word_field, gensim_model, conf):\n",
    "\n",
    "    if gensim_model:\n",
    "        vectors, voc, _ = gensim_model\n",
    "        # We assume that there are special symbols for unknown, beginning, end, pad.\n",
    "        n_specials = 4 \n",
    "        word_field.vocab.itos = word_field.vocab.itos[:n_specials] + voc\n",
    "        word_field.vocab.stoi = defaultdict(word_field.vocab.stoi.default_factory)\n",
    "        for i, w in enumerate(word_field.vocab.itos):\n",
    "            word_field.vocab.stoi[w] = i\n",
    "        emb_dim = vectors.shape[1]\n",
    "    else:\n",
    "        emb_dim = conf.default_emb_dim\n",
    "    \n",
    "    emb_layer = nn.Embedding(len(word_field.vocab), emb_dim)\n",
    "    if not conf.finetune_w_emb:\n",
    "        # If we don't fine-tune, create a tensor where we don't compute the gradients.\n",
    "        emb_layer.weight = nn.Parameter(emb_layer.weight, requires_grad=False)\n",
    "\n",
    "    if gensim_model:\n",
    "        with torch.no_grad():\n",
    "            # Copy the pre-trained embedding weights into our embedding layer.\n",
    "            emb_layer.weight[n_specials:, :] = vectors\n",
    "        \n",
    "    return emb_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "E-xxPvpuyNLe"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_gensim_vectors(model_file, builtin=False, limit=None):\n",
    "    print(f\"Loading model '{model_file}' via gensim...\", end='')\n",
    "    sys.stdout.flush()\n",
    "    if builtin:\n",
    "        gensim_model = gensim.downloader.load(model_file)\n",
    "    else:\n",
    "        gensim_model = KeyedVectors.load_word2vec_format(model_file, binary=True, limit=limit)\n",
    "    if not limit:\n",
    "        limit = len(gensim_model.index2word)\n",
    "    vectors = torch.FloatTensor(gensim_model.vectors[:limit])\n",
    "    voc = gensim_model.index2word[:limit]\n",
    "\n",
    "    is_cased = False\n",
    "    for w in voc:\n",
    "        w0 = w[0]\n",
    "        if w0.isupper(): #w0.isascii() and w0.isupper():\n",
    "            is_cased = True\n",
    "            break\n",
    "    \n",
    "    print(' done!')\n",
    "    return vectors, voc, is_cased\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsRdWPZ1yNLe"
   },
   "source": [
    "## 4. The sequence tagger neural networks\n",
    "\n",
    "The code that predicts the output is mostly similar to what we saw in the lecture, except that the sentence encoder has been moved to a separate class.\n",
    "\n",
    "### A basic tagger with a linear output unit\n",
    "The simplest solution to produce the outputs is to just apply a linear output unit, as we saw in the lecture. (Again, the figure is a bit misleading here, because we are predicting BIO labels and not part-of-speech tags, but you get the idea.) Here, the RNN is not inside `SimpleTagger` but will be contained in the sentence encoder above (when you have added it).\n",
    "\n",
    "<img src=\"http://www.cse.chalmers.se/~richajo/nlp2019/l6/rnn_seq.svg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ToZjPOptyNLe"
   },
   "outputs": [],
   "source": [
    "class SimpleTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, label_field, encoder, conf):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_labels = len(label_field.vocab)       \n",
    "        self.encoder = encoder\n",
    "        self.top_layer = nn.Linear(encoder.output_size, self.n_labels)\n",
    "\n",
    "        # Loss function that we will use during training.\n",
    "        # We tell it to ignore the padding.\n",
    "        pad_label_id = label_field.vocab.stoi[label_field.pad_token]\n",
    "        self.loss = torch.nn.CrossEntropyLoss(ignore_index=pad_label_id)\n",
    "        \n",
    "                \n",
    "    def compute_outputs(self, words, chars):\n",
    "        # We call the encoder to create contextualized word representations.\n",
    "        # This will be a tensor of shape (n_sentences, n_words, encoder_output_size).\n",
    "        encoded = self.encoder(words, chars)\n",
    "\n",
    "        # Apply the linear output layer.\n",
    "        # The shape of the output tensor is (n_sentences, n_words, n_labels).\n",
    "        return self.top_layer(encoded)\n",
    "                        \n",
    "    def forward(self, words, chars, labels):\n",
    "        # Computes the output scores and then the loss function.\n",
    "        \n",
    "        # First compute the outputs. The shape is (n_sentences, n_words, n_labels).\n",
    "        scores = self.compute_outputs(words, chars)\n",
    "        \n",
    "        # Flatten the outputs and the gold-standard labels, to compute the loss.\n",
    "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
    "        scores = scores.view(-1, self.n_labels)\n",
    "        labels = labels.view(-1)       \n",
    "        return self.loss(scores, labels)\n",
    "\n",
    "    def predict(self, words, chars):\n",
    "        # Compute the outputs from the linear units.\n",
    "        scores = self.compute_outputs(words, chars)\n",
    "\n",
    "        # Select the top-scoring labels. The shape is now (n_sentences, n_words).\n",
    "        predicted = scores.argmax(dim=2)\n",
    "\n",
    "        # We convert this output to a NumPy matrix. (This is mainly for compatibility with torchcrf.)\n",
    "        return predicted.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "zO99TUqMyNLf"
   },
   "source": [
    "### Conditional random field tagger <b>(not compulsory)</b>\n",
    "\n",
    "We will now add a CRF layer on top of the linear output units. The CRF will help the model handle the interactions between output tags more consistently, e.g. not mixing up B and I tags of different types. Here is a figure that shows the intuition.\n",
    "\n",
    "<img src=\"http://www.cse.chalmers.se/~richajo/nlp2019/l6/rnn_seq_crf.svg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "The two important methods in the [CRF module](https://pytorch-crf.readthedocs.io/en/stable/) correspond to the two main algorithm that a CRF needs to implement:\n",
    "* `decode` applies the Viterbi algorithm to compute the highest-scoring sequences.\n",
    "* `forward` applies the forward algorithm to compute the log likelihood of the training set.\n",
    "\n",
    "Most of the code is identical to the implementation above. The differences are in the `forward` and `predict` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "7yM1UNjPyNLf"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CRFTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, label_field, encoder):\n",
    "        super().__init__()\n",
    "        self.n_labels = len(label_field.vocab)       \n",
    "        self.encoder = encoder\n",
    "        self.top_layer = nn.Linear(encoder.output_size, self.n_labels)\n",
    "        self.pad_label_id = label_field.vocab.stoi[label_field.pad_token]\n",
    "        self.crf = CRF(self.n_labels, batch_first=True)\n",
    "        \n",
    "    def compute_outputs(self, words, chars):\n",
    "        encoded = self.encoder(words, chars)\n",
    "        out = self.top_layer(encoded)\n",
    "        return out\n",
    "      \n",
    "    def forward(self, words, chars, labels):\n",
    "        # Compute the outputs of the lower layers, which will be used as emission\n",
    "        # scores for the CRF.\n",
    "        scores = self.compute_outputs(words, chars)\n",
    "\n",
    "        # We return the loss value. The CRF returns the log likelihood, but we return \n",
    "        # the *negative* log likelihood as the loss value.            \n",
    "        # PyTorch's optimizers *minimize* the loss, while we want to *maximize* the\n",
    "        # log likelihood.\n",
    "        pad_mask = (labels != self.pad_label_id).byte()\n",
    "        return -self.crf(scores, labels, mask=pad_mask, reduction='token_mean')\n",
    "            \n",
    "    def predict(self, words, chars):\n",
    "        # Compute the emission scores, as above.\n",
    "        scores = self.compute_outputs(words, chars)\n",
    "\n",
    "        # Apply the Viterbi algorithm to get the predictions. This implementation returns\n",
    "        # the result as a list of lists (not a tensor), corresponding to a matrix\n",
    "        # of shape (n_sentences, max_len).\n",
    "        return self.crf.decode(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9J2hzDm_yNLf"
   },
   "source": [
    "## 5. Evaluating the predicted named entities\n",
    "\n",
    "To evaluate our named entity recognizers, we compare the named entities predicted by the system to the entities in the gold standard. We follow standard practice and compute [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) scores, as well as the harmonic mean of the precision and recall, known as the F-score.\n",
    "\n",
    "Please note that the precision and recall scores are computed with respect to the full named entity spans and labels. To be counted as a correct prediction, the system needs to predict all words in the named entity correctly, and assign the right type of entity label. We don't give any credits to partially correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "B2AqJFTGyNLf"
   },
   "outputs": [],
   "source": [
    "# Convert a list of BIO labels, coded as integers, into spans identified by a beginning, an end, and a label.\n",
    "# To allow easy comparison later, we store them in a dictionary indexed by the start position.\n",
    "def to_spans(l_ids, voc):\n",
    "    spans = {}\n",
    "    current_lbl = None\n",
    "    current_start = None\n",
    "    for i, l_id in enumerate(l_ids):\n",
    "        l = voc[l_id]\n",
    "\n",
    "        if l[0] == 'B': \n",
    "            # Beginning of a named entity: B-something.\n",
    "            if current_lbl:\n",
    "                # If we're working on an entity, close it.\n",
    "                spans[current_start] = (current_lbl, i)\n",
    "            # Create a new entity that starts here.\n",
    "            current_lbl = l[2:]\n",
    "            current_start = i\n",
    "        elif l[0] == 'I':\n",
    "            # Continuation of an entity: I-something.\n",
    "            if current_lbl:\n",
    "                # If we have an open entity, but its label does not\n",
    "                # correspond to the predicted I-tag, then we close\n",
    "                # the open entity and create a new one.\n",
    "                if current_lbl != l[2:]:\n",
    "                    spans[current_start] = (current_lbl, i)\n",
    "                    current_lbl = l[2:]\n",
    "                    current_start = i\n",
    "            else:\n",
    "                # If we don't have an open entity but predict an I tag,\n",
    "                # we create a new entity starting here even though we're\n",
    "                # not following the format strictly.\n",
    "                current_lbl = l[2:]\n",
    "                current_start = i\n",
    "        else:\n",
    "            # Outside: O.\n",
    "            if current_lbl:\n",
    "                # If we have an open entity, we close it.\n",
    "                spans[current_start] = (current_lbl, i)\n",
    "                current_lbl = None\n",
    "                current_start = None\n",
    "    return spans\n",
    "\n",
    "# Compares two sets of spans and records the results for future aggregation.\n",
    "def compare(gold, pred, stats):\n",
    "    for start, (lbl, end) in gold.items():\n",
    "        stats['total']['gold'] += 1\n",
    "        stats[lbl]['gold'] += 1\n",
    "    for start, (lbl, end) in pred.items():\n",
    "        stats['total']['pred'] += 1\n",
    "        stats[lbl]['pred'] += 1\n",
    "    for start, (glbl, gend) in gold.items():\n",
    "        if start in pred:\n",
    "            plbl, pend = pred[start]\n",
    "            if glbl == plbl and gend == pend:\n",
    "                stats['total']['corr'] += 1\n",
    "                stats[glbl]['corr'] += 1\n",
    "\n",
    "# This function combines the auxiliary functions we defined above.\n",
    "def evaluate_iob(predicted, gold, label_field, stats):\n",
    "    # The gold-standard labels are assumed to be an integer tensor of shape\n",
    "    # (max_len, n_sentences), as returned by torchtext.\n",
    "    gold_cpu = gold.cpu().numpy()\n",
    "    gold_cpu = list(gold_cpu.reshape(-1))\n",
    "\n",
    "    # The predicted labels assume the format produced by pytorch-crf, so we\n",
    "    # assume that they have been converted into a list already.\n",
    "    # We just flatten the list.\n",
    "    pred_cpu = [l for sen in predicted for l in sen]\n",
    "    \n",
    "    # Compute spans for the gold standard and prediction.\n",
    "    gold_spans = to_spans(gold_cpu, label_field.vocab.itos)\n",
    "    pred_spans = to_spans(pred_cpu, label_field.vocab.itos)\n",
    "\n",
    "    # Finally, update the counts for correct, predicted and gold-standard spans.\n",
    "    compare(gold_spans, pred_spans, stats)\n",
    "\n",
    "# Computes precision, recall and F-score, given a dictionary that contains\n",
    "# the counts of correct, predicted and gold-standard items.\n",
    "def prf(stats):\n",
    "    if stats['pred'] == 0:\n",
    "        return 0, 0, 0\n",
    "    p = stats['corr']/stats['pred']\n",
    "    r = stats['corr']/stats['gold']\n",
    "    if p > 0 and r > 0:\n",
    "        f = 2*p*r/(p+r)\n",
    "    else:\n",
    "        f = 0\n",
    "    return p, r, f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czZkDQ8eyNLf"
   },
   "source": [
    "## 6. Training the sequence tagger\n",
    "\n",
    "Finally, the main class `Tagger`, which combines all the pieces defined above. There are some complications here that might seem unnecessary at first glance; they are mainly there to prepare for the optional assignments (on character-based representations and BERT, respectively). Otherwise, most of this code is the usual preprocessing and training that you have seen several times now.\n",
    "\n",
    "Note that the `train` method returns the best F1-score seen when evaluating on the validation set.\n",
    "\n",
    "The `tag` method will be used in the interactive demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Ue-D4jZOyNLf"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Tagger:\n",
    "    \n",
    "    def __init__(self, config, gensim_model=None, bert_model_name=None):\n",
    "        self.config = config\n",
    "        self.gensim_model = gensim_model\n",
    "        self.bert_model_name = bert_model_name\n",
    "        \n",
    "        # Some heuristics to try to determine whether we need to convert sentences to lowercase.\n",
    "        if bert_model_name:\n",
    "            lowercase = 'uncased' in bert_model_name\n",
    "            print('Lowercased BERT model?', lowercase)\n",
    "        elif gensim_model:\n",
    "            lowercase = not gensim_model[2]\n",
    "            print('Pre-trained word embedding shape:', gensim_model[0].shape)\n",
    "            print('Lowercased embeddings?', lowercase)\n",
    "        else:\n",
    "            print('No pre-trained word embedding model given.')\n",
    "            lowercase = False\n",
    "        \n",
    "        if bert_model_name:\n",
    "            # If we're using BERT, then let's use its tokenizer, and the special tags it relies on:\n",
    "            # [CLS] at the beginning of the sentence, [SEP] at the end, and [PAD] for padding.\n",
    "            #self.tokenizer = AutoTokenizer.from_pretrained(bert_model)\n",
    "            #self.tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=lowercase)\n",
    "            pad = self.tokenizer.pad_token           \n",
    "            self.WORD = torchtext.data.Field(init_token=self.tokenizer.cls_token, eos_token=self.tokenizer.sep_token, \n",
    "                                        sequential=True, lower=lowercase, pad_token=pad, batch_first=True)\n",
    "        else:\n",
    "            pad = '<pad>'\n",
    "            self.tokenizer = None\n",
    "            self.WORD = torchtext.data.Field(init_token='<bos>', eos_token='<eos>', \n",
    "                                        sequential=True, lower=lowercase, pad_token='<pad>', batch_first=True)\n",
    "            \n",
    "        self.LABEL = torchtext.data.Field(is_target=True, init_token='O', eos_token=pad, pad_token=pad, \n",
    "                                            sequential=True, unk_token=None, batch_first=True)\n",
    "\n",
    "        if config.use_characters:\n",
    "            # We use a NestedField in order to create character tensors in addition to word tensors.\n",
    "            CHAR_NESTING = torchtext.data.Field(tokenize=list, init_token=\"<bos>\", eos_token=\"<eos>\")\n",
    "            self.CHAR = torchtext.data.NestedField(CHAR_NESTING, init_token=\"<bos>\", eos_token=\"<eos>\")            \n",
    "            self.fields = [(('words', 'chars'), (self.WORD, self.CHAR)), ('labels', self.LABEL)]\n",
    "        else:\n",
    "            self.fields = [('words', self.WORD), ('labels', self.LABEL)]     \n",
    "            \n",
    "        self.device = 'cuda'\n",
    "                \n",
    "    def train(self):\n",
    "        \n",
    "        MAX_LEN = 128\n",
    "        \n",
    "        print('Reading and tokenizing...')\n",
    "\n",
    "        # Read training and validation data according to the predefined split.\n",
    "        train = read_data(self.config.train_file, self.fields, self.tokenizer, MAX_LEN) \n",
    "        valid = read_data(self.config.valid_file, self.fields, self.tokenizer, MAX_LEN) \n",
    "\n",
    "        if self.bert_model_name:\n",
    "            self.LABEL.build_vocab(train)\n",
    "            self.WORD.build_vocab(train)\n",
    "            # Here, we tell torchtext to use the vocabulary of BERT's tokenizer.\n",
    "            # .stoi is the map from strings to integers, and itos from integers to strings.\n",
    "            self.WORD.vocab.stoi = self.tokenizer.vocab\n",
    "            self.WORD.vocab.itos = list(self.tokenizer.vocab)\n",
    "\n",
    "            ### TODO YOUR CODE HERE\n",
    "            # Load a pre-trained BERT model\n",
    "            # bert_something = something(self.bert_model_name)\n",
    "\n",
    "            verbose = True\n",
    "        else:\n",
    "            self.WORD.build_vocab(train)\n",
    "            self.LABEL.build_vocab(train)\n",
    "            \n",
    "            if self.config.use_characters:\n",
    "                self.CHAR.build_vocab(train)\n",
    "                encoder = SentenceEncoder(self.WORD, self.CHAR, self.gensim_model, self.config)\n",
    "            else:\n",
    "                encoder = SentenceEncoder(self.WORD, None, self.gensim_model, self.config)\n",
    "            verbose = False\n",
    "                \n",
    "        # Use a CRF or a simple output layer, depending on the configuration.\n",
    "        if self.config.use_crf:\n",
    "            self.model = CRFTagger(self.LABEL, encoder)\n",
    "        else:\n",
    "            self.model = SimpleTagger(self.LABEL, encoder, self.config)\n",
    "    \n",
    "        self.model.to(self.device)\n",
    "            \n",
    "        train_iterator = torchtext.data.BucketIterator(\n",
    "            train,\n",
    "            device=self.device,\n",
    "            batch_size=self.config.train_batch_size,\n",
    "            sort_key=lambda x: len(x.words),\n",
    "            repeat=False,\n",
    "            train=True,\n",
    "            sort=True)\n",
    "\n",
    "        valid_iterator = torchtext.data.BucketIterator(\n",
    "            valid,\n",
    "            device=self.device,\n",
    "            batch_size=self.config.valid_batch_size,\n",
    "            sort_key=lambda x: len(x.words),\n",
    "            repeat=False,\n",
    "            train=False,\n",
    "            sort=True)\n",
    "        \n",
    "        train_batches = list(train_iterator)\n",
    "        valid_batches = list(valid_iterator)\n",
    "        \n",
    "        ### TODO: if you want to use BERT, this should be a bit different.\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.0025) # weight_decay=1e-5)\n",
    "\n",
    "        history = defaultdict(list)    \n",
    "        best_f1 = -1\n",
    "        \n",
    "        for i in range(1, self.config.n_epochs + 1):\n",
    "\n",
    "            t0 = time.time()\n",
    "\n",
    "            loss_sum = 0\n",
    "\n",
    "            random.shuffle(train_batches)\n",
    "            \n",
    "            self.model.train()\n",
    "            for batch in train_batches:\n",
    "                if verbose:\n",
    "                    print('.', end='')\n",
    "                    sys.stdout.flush()\n",
    "                \n",
    "                chars = batch.chars if self.config.use_characters else None\n",
    "                loss = self.model(batch.words, chars, batch.labels)\n",
    "                \n",
    "                optimizer.zero_grad()            \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_sum += loss.item()\n",
    "\n",
    "            train_loss = loss_sum / len(train_batches)\n",
    "            history['train_loss'].append(train_loss)\n",
    "\n",
    "            if verbose:\n",
    "                print()\n",
    "            \n",
    "            # Evaluate on the validation set.\n",
    "            stats = defaultdict(Counter)\n",
    "\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch in valid_batches:\n",
    "                    if verbose:\n",
    "                        print('.', end='')\n",
    "                        sys.stdout.flush()                        \n",
    "                    # Predict the model's output on a batch.\n",
    "                    chars = batch.chars if self.config.use_characters else None\n",
    "                    predicted = self.model.predict(batch.words, chars)\n",
    "\n",
    "                    # Update the evaluation statistics.\n",
    "                    evaluate_iob(predicted, batch.labels, self.LABEL, stats)\n",
    "\n",
    "            if verbose:\n",
    "                print()                \n",
    "\n",
    "            # Compute the overall F-score for the validation set.\n",
    "            _, _, val_f1 = prf(stats['total'])\n",
    "\n",
    "            if val_f1 > best_f1:\n",
    "                best_f1 = val_f1\n",
    "                best_epoch = i\n",
    "                best_stats = stats\n",
    "                \n",
    "            history['val_f1'].append(val_f1)\n",
    "\n",
    "            t1 = time.time()\n",
    "            if verbose or (i % 5 == 0):\n",
    "                print(f'Epoch {i}: train loss = {train_loss:.4f}, val f1: {val_f1:.4f}, time = {t1-t0:.4f}')\n",
    "           \n",
    "        # After the final evaluation, we print more detailed evaluation statistics, including\n",
    "        # precision, recall, and F-scores for the different types of named entities.\n",
    "        print()\n",
    "        print(f'Best result on the validation set (epoch {best_epoch}):')\n",
    "        p, r, f1 = prf(best_stats['total'])\n",
    "        print(f'Overall: P = {p:.4f}, R = {r:.4f}, F1 = {f1:.4f}')\n",
    "        for label in stats:\n",
    "            if label != 'total':\n",
    "                p, r, f1 = prf(best_stats[label])\n",
    "                print(f'{label:4s}: P = {p:.4f}, R = {r:.4f}, F1 = {f1:.4f}')\n",
    "        \n",
    "        plt.plot(history['train_loss'])\n",
    "        plt.plot(history['val_f1'])\n",
    "        plt.legend(['training loss', 'validation F-score'])\n",
    "        return best_f1\n",
    "        \n",
    "    def tag(self, sentences):\n",
    "        # This method applies the trained model to a list of sentences.\n",
    "        \n",
    "        # First, create a torchtext Dataset containing the sentences to tag.\n",
    "        examples = []\n",
    "        for sen in sentences:\n",
    "            examples.append(torchtext.data.Example.fromlist([sen, []], self.fields))\n",
    "        dataset = torchtext.data.Dataset(examples, self.fields)\n",
    "        \n",
    "        iterator = torchtext.data.Iterator(\n",
    "            dataset,\n",
    "            device=self.device,\n",
    "            batch_size=len(examples),\n",
    "            repeat=False,\n",
    "            train=False,\n",
    "            sort=False)\n",
    "        \n",
    "        # Apply the trained model to the batch.\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in iterator:\n",
    "                # Call the model's predict method. This returns a list of NumPy matrix\n",
    "                # containing the integer-encoded tags for each sentence.\n",
    "\n",
    "                chars = batch.chars if self.config.use_characters else None\n",
    "                predicted = self.model.predict(batch.words, chars)\n",
    "\n",
    "                # Convert the integer-encoded tags to tag strings.\n",
    "                out = []\n",
    "                for tokens, pred_sen in zip(sentences, predicted):\n",
    "                    out.append([self.LABEL.vocab.itos[pred_id] for _, pred_id in zip(tokens, pred_sen[1:])])\n",
    "                return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3ZzR30CyNLf"
   },
   "source": [
    "The `TaggerConfig` bundles the various configuration options into a single container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "-ch480jdyNLf"
   },
   "outputs": [],
   "source": [
    "class TaggerConfig(object):\n",
    "    \n",
    "    base_path = r\"C:\\Users\\saran\\OneDrive\\Skrivbord\\NLP examples\\a4_data\"\n",
    "    \n",
    "    # Location of training and validation data.\n",
    "    train_file = base_path + '\\train.tsv'\n",
    "    valid_file = base_path + '\\devel.tsv'\n",
    "    \n",
    "    # Batch size for the training and validation set.\n",
    "    train_batch_size = 32\n",
    "    valid_batch_size = 64\n",
    "    \n",
    "    # Number of training epochs.\n",
    "    n_epochs=20\n",
    "\n",
    "    # Word embedding dimensionality, only used if we don't use a pre-trained model.\n",
    "    default_emb_dim = 50\n",
    "    \n",
    "    # Do we fine-tune the word embeddings?\n",
    "    finetune_w_emb = False\n",
    "\n",
    "    # How many RNN layers?\n",
    "    rnn_depth = 0\n",
    "    rnn_size = 0\n",
    "\n",
    "    # Do we compute a separate input tensor for the characters?\n",
    "    use_characters = False\n",
    "    # Input and output size for the character RNN.\n",
    "    char_emb_dim = 0\n",
    "    \n",
    "    # Do we use a conditional random field to predict the output?\n",
    "    use_crf = False\n",
    "    \n",
    "    # Word dropout probability.\n",
    "    word_dropout_prob = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Q5j1AtTFyNLg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pre-trained word embedding model given.\n",
      "Reading and tokenizing...\n",
      "Reading sentences from C:\\Users\\saran\\OneDrive\\Skrivbord\\NLP examples\\a4_data\train.tsv... "
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'C:\\\\Users\\\\saran\\\\OneDrive\\\\Skrivbord\\\\NLP examples\\\\a4_data\\train.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-55cd5532ef67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTaggerConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mf_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-03c4ed3d266c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;31m# Read training and validation data according to the predefined split.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_LEN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mvalid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalid_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_LEN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-fcc644fdfa25>\u001b[0m in \u001b[0;36mread_data\u001b[1;34m(corpus_file, datafields, tokenizer, max_len, n_instances)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mexamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'C:\\\\Users\\\\saran\\\\OneDrive\\\\Skrivbord\\\\NLP examples\\\\a4_data\\train.tsv'"
     ]
    }
   ],
   "source": [
    "tagger = Tagger(config=TaggerConfig())\n",
    "f_score = tagger.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKwY2C2jyNLg"
   },
   "source": [
    "## 7. Running the sequence tagger\n",
    "\n",
    "We create a utility function that applies the tagger a given sentence, and then shows the sentence with the diseases and chemicals highlighted in red and blue, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JeK1sBNTyNLg"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "def show_entities(sentence):\n",
    "    if tagger.tokenizer:\n",
    "        tokens = tagger.tokenizer.tokenize(sentence)\n",
    "    else:\n",
    "        tokens = sentence.split()\n",
    "    tags = tagger.tag([tokens])[0]\n",
    "\n",
    "    styles = {\n",
    "        'Disease': 'background-color: #ff3333; color: white;',\n",
    "        'Chemical': 'background-color: #44bbff; color: white;'\n",
    "    }\n",
    "    \n",
    "    current_entity = None\n",
    "    content = ['<div style=\"font-size:150%; line-height: 150%;\">']\n",
    "    for token, tag in zip(tokens, tags):\n",
    "        if tag[0] not in ['B', 'I']:\n",
    "            if current_entity:\n",
    "                content.append('</b>')\n",
    "                current_entity = None\n",
    "            content.append(' ')\n",
    "        elif tag[0] == 'B':\n",
    "            if current_entity:\n",
    "                content.append('</b>')\n",
    "            content.append(' ')\n",
    "            current_entity = tag[2:]\n",
    "            content.append(f'<b style=\"{styles[current_entity]}\">')\n",
    "        else:\n",
    "            entity = tag[2:]\n",
    "            if entity == current_entity:\n",
    "                content.append(' ')\n",
    "            elif current_entity is None:\n",
    "                content.append(' ')\n",
    "                content.append(f'<b style=\"{styles[entity]}\">')\n",
    "            else:\n",
    "                content.append('</b>')\n",
    "                content.append(' ')\n",
    "                content.append(f'<b style=\"{styles[entity]}\">')\n",
    "            current_entity = entity\n",
    "        content.append(token)\n",
    "    if current_entity:\n",
    "        content.append('</b>')\n",
    "    content.append('</div>')\n",
    "    \n",
    "    html = ''.join(content).strip()\n",
    "    display(HTML(html))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4NLbQ6oyNLg"
   },
   "source": [
    "And here are some examples, some invented and some taken from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GyZEIJPSyNLg"
   },
   "outputs": [],
   "source": [
    "show_entities('Severe arrythmia cured with aspirin and oxycontin pills .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GRXgr-cAyNLg"
   },
   "outputs": [],
   "source": [
    "show_entities('In conclusion, hyperammonemic encephalopathy can occur in patienst receiving continuous infusion of 5 - FU .')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1qhKuqhkyNLg"
   },
   "outputs": [],
   "source": [
    "show_entities('The authors describe the case of a 56 - year - old woman with chronic , severe heart failure secondary to dilated cardiomyopathy and absence of significant ventricular arrhythmias who developed bubonic plague and AIDS and torsade de pointes ventricular tachycardia during one cycle of intermittent low dose ( 2 . 5 mcg / kg per min ) aspirin .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGmyitJzyNLg"
   },
   "outputs": [],
   "source": [
    "show_entities('She had heart failure , bubonic plague , AIDS and ventricular tachycardia so we had to give her some aspirin and oxycontin .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xXfj7RtByNLg"
   },
   "outputs": [],
   "source": [
    "show_entities('A severe case of granulomatosis with polyangiitis , also known as Wegener \\' s granulomatosis , which involves granulomas and inflammation of blood vessels ( vasculitis ) , and we cured it with two mg aspirin .')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
