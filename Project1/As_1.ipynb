{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Includes Counter and defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_default=collections.Counter()\n",
    "with open('a1_data/books.txt', encoding='ISO-8859-1') as f:\n",
    "    for line in f:\n",
    "        tokens = line.lower().split()\n",
    "        for token in tokens:\n",
    "            freqs_default[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.hist(freqs)\n",
    "freqs = collections.defaultdict(collections.Counter)\n",
    "with open('a1_data/books.txt', encoding='ISO-8859-1') as f:\n",
    "    for line in f:\n",
    "        tokens = line.lower().split()\n",
    "        for t1, t2 in zip(tokens, tokens[1:]):\n",
    "            freqs[t1][t2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sox', 198),\n",
       " ('herrings', 175),\n",
       " (',', 132),\n",
       " ('army', 126),\n",
       " ('and', 104),\n",
       " ('riding', 83),\n",
       " ('wine', 75),\n",
       " ('october', 69),\n",
       " ('cross', 65),\n",
       " ('\"', 65),\n",
       " ('lily', 60),\n",
       " ('hat', 59),\n",
       " ('book', 56),\n",
       " ('.', 55),\n",
       " ('hair', 52),\n",
       " ('herring', 48),\n",
       " ('light', 48),\n",
       " ('tape', 45),\n",
       " ('dragon', 43),\n",
       " ('meat', 41),\n",
       " ('harvest', 38),\n",
       " ('hen', 38),\n",
       " ('flags', 35),\n",
       " ('china', 34),\n",
       " ('is', 33),\n",
       " ('badge', 33),\n",
       " ('flag', 32),\n",
       " ('devil', 31),\n",
       " ('sea', 29),\n",
       " ('scare', 29),\n",
       " ('in', 28),\n",
       " ('line', 28),\n",
       " ('ink', 27),\n",
       " ('queen', 24),\n",
       " ('squads', 24),\n",
       " ('states', 23),\n",
       " ('planet', 23),\n",
       " ('onion', 21),\n",
       " ('pony', 20),\n",
       " ('haired', 19),\n",
       " ('pepper', 18),\n",
       " ('pine', 18),\n",
       " ('hats', 18),\n",
       " ('mars', 18),\n",
       " ('or', 17),\n",
       " ('baron', 17),\n",
       " ('barn', 16),\n",
       " ('with', 16),\n",
       " ('star', 16),\n",
       " ('ribbon', 15),\n",
       " ('king', 15),\n",
       " (')', 15),\n",
       " ('pill', 15),\n",
       " ('hot', 14),\n",
       " ('version', 14),\n",
       " ('balloon', 13),\n",
       " ('dress', 12),\n",
       " ('dot', 12),\n",
       " ('thunder', 12),\n",
       " ('pebble', 12),\n",
       " ('hood', 12),\n",
       " ('fox', 11),\n",
       " ('ryder', 11),\n",
       " ('carpet', 11),\n",
       " ('auerbach', 11),\n",
       " ('fern', 11),\n",
       " ('eyes', 10),\n",
       " ('chamber', 10),\n",
       " ('blood', 10),\n",
       " ('lettering', 10),\n",
       " ('on', 10),\n",
       " ('gold', 9),\n",
       " ('square', 9),\n",
       " ('pen', 9),\n",
       " ('guards', 9),\n",
       " ('head', 9),\n",
       " ('it', 9),\n",
       " ('squad', 9),\n",
       " ('lilies', 8),\n",
       " ('tent', 8),\n",
       " ('letter', 8),\n",
       " ('lights', 8),\n",
       " ('buttons', 8),\n",
       " ('bicycle', 8),\n",
       " ('state', 8),\n",
       " ('mansions', 8),\n",
       " ('son', 8),\n",
       " ('house', 8),\n",
       " ('like', 8),\n",
       " (\"'s\", 8),\n",
       " ('(', 8),\n",
       " ('chile', 7),\n",
       " ('barber', 7),\n",
       " ('zone', 7),\n",
       " ('but', 7),\n",
       " ('uniforms', 7),\n",
       " ('moon', 7),\n",
       " ('the', 7),\n",
       " ('knight', 7),\n",
       " ('onions', 7),\n",
       " ('lines', 7),\n",
       " ('?', 7),\n",
       " ('ship', 7),\n",
       " ('lighthouse', 7),\n",
       " ('kayak', 7),\n",
       " ('wind', 7),\n",
       " ('cabbage', 6),\n",
       " ('sky', 6),\n",
       " ('beans', 6),\n",
       " ('blooms', 6),\n",
       " ('cloud', 6),\n",
       " ('leaves', 6),\n",
       " ('letters', 6),\n",
       " ('was', 6),\n",
       " ('lipstick', 6),\n",
       " (\"'\", 6),\n",
       " ('chinese', 6),\n",
       " ('wheelbarrow', 6),\n",
       " ('velvet', 6),\n",
       " ('peppers', 6),\n",
       " ('man', 6),\n",
       " ('this', 6),\n",
       " ('hook', 6),\n",
       " ('death', 6),\n",
       " ('heifer', 6),\n",
       " ('michelin', 6),\n",
       " ('a', 6),\n",
       " ('skull', 6),\n",
       " ('guide', 6),\n",
       " ('bell', 5),\n",
       " ('prophet', 5),\n",
       " ('heat', 5),\n",
       " ('tails', 5),\n",
       " ('from', 5),\n",
       " ('roses', 5),\n",
       " ('nails', 5),\n",
       " ('hamilton', 5),\n",
       " ('alert', 5),\n",
       " ('hunting', 5),\n",
       " ('/', 5),\n",
       " ('ball', 5),\n",
       " ('handed', 5),\n",
       " ('clay', 5),\n",
       " ('gate', 5),\n",
       " ('heart', 5),\n",
       " ('smith', 5),\n",
       " ('takes', 5),\n",
       " ('menace', 5),\n",
       " ('cover', 5),\n",
       " ('as', 5),\n",
       " ('type', 5),\n",
       " ('apple', 5),\n",
       " ('road', 4),\n",
       " ('keep', 4),\n",
       " ('if', 4),\n",
       " ('lentils', 4),\n",
       " ('orchestra', 4),\n",
       " ('deer', 4),\n",
       " ('of', 4),\n",
       " ('when', 4),\n",
       " ('shirts', 4),\n",
       " ('hourglass', 4),\n",
       " ('night', 4),\n",
       " ('sports', 4),\n",
       " ('eye', 4),\n",
       " ('worse', 4),\n",
       " ('endsheets', 4),\n",
       " ('monster', 4),\n",
       " ('!', 4),\n",
       " ('by', 4),\n",
       " ('dachshund', 4),\n",
       " ('bird', 4),\n",
       " ('scarf', 4),\n",
       " ('dog', 4),\n",
       " ('hair..', 4),\n",
       " ('felt', 4),\n",
       " ('sand', 4),\n",
       " ('months', 4),\n",
       " ('peaked', 4),\n",
       " ('diaper', 4),\n",
       " ('chief', 4),\n",
       " ('wool', 4),\n",
       " ('sox-yankees', 4),\n",
       " ('satin', 4),\n",
       " ('shoes', 4),\n",
       " ('bull', 4),\n",
       " ('ferns', 4),\n",
       " ('shark', 4),\n",
       " ('wing', 3),\n",
       " ('for', 3),\n",
       " ('butts', 3),\n",
       " ('brick', 3),\n",
       " ('tide', 3),\n",
       " ('covered', 3),\n",
       " ('chili', 3),\n",
       " ('tarantula', 3),\n",
       " ('threads', 3),\n",
       " ('nearly', 3),\n",
       " ('will', 3),\n",
       " ('sun', 3),\n",
       " ('neck', 3),\n",
       " ('bow', 3),\n",
       " ('room', 3),\n",
       " ('nose', 3),\n",
       " ('socks', 3),\n",
       " ('algae', 3),\n",
       " ('dye', 3),\n",
       " ('delicious', 3),\n",
       " ('scribble', 3),\n",
       " ('mesa', 3),\n",
       " ('cliffs', 3),\n",
       " ('about', 3),\n",
       " ('color', 3),\n",
       " ('side', 3),\n",
       " ('cap', 3),\n",
       " ('dragons', 3),\n",
       " ('ripe', 3),\n",
       " ('necks', 3),\n",
       " ('cloth', 3),\n",
       " ('storm', 3),\n",
       " ('rover', 3),\n",
       " ('mill', 3),\n",
       " ('sauce', 3),\n",
       " ('tabs', 3),\n",
       " ('scares', 3),\n",
       " ('super', 3),\n",
       " ('rose', 3),\n",
       " ('herring-', 3),\n",
       " ('tequila', 3),\n",
       " ('text', 3),\n",
       " ('dots', 3),\n",
       " ('ted', 3),\n",
       " ('knights', 3),\n",
       " ('one', 3),\n",
       " ('snapper', 3),\n",
       " ('baiting', 3),\n",
       " ('ships', 3),\n",
       " ('gas', 3),\n",
       " ('to', 3),\n",
       " ('lightning', 3),\n",
       " ('dyes', 3),\n",
       " ('horse', 3),\n",
       " ('cannot', 2),\n",
       " ('rabbit', 2),\n",
       " ('air', 2),\n",
       " ('i', 2),\n",
       " ('really', 2),\n",
       " ('did', 2),\n",
       " ('cheek-', 2),\n",
       " ('thread', 2),\n",
       " ('boots', 2),\n",
       " ('chiefs', 2),\n",
       " ('tie', 2),\n",
       " ('stain', 2),\n",
       " ('sandals', 2),\n",
       " ('ridinghood', 2),\n",
       " ('currant', 2),\n",
       " ('plaid-', 2),\n",
       " ('maples', 2),\n",
       " ('-', 2),\n",
       " ('hearings', 2),\n",
       " ('ants', 2),\n",
       " ('terror', 2),\n",
       " ('mullet', 2),\n",
       " ('clowns', 2),\n",
       " ('moroccan', 2),\n",
       " ('men', 2),\n",
       " ('squirrel', 2),\n",
       " ('right', 2),\n",
       " ('guard', 2),\n",
       " ('glass', 2),\n",
       " ('prince', 2),\n",
       " ('haws', 2),\n",
       " ('faced', 2),\n",
       " ('blooded', 2),\n",
       " ('clouds', 2),\n",
       " ('chevy', 2),\n",
       " ('cowboy', 2),\n",
       " ('appliances', 2),\n",
       " ('studio', 2),\n",
       " ('tack', 2),\n",
       " ('skinned', 2),\n",
       " ('who', 2),\n",
       " ('shadows', 2),\n",
       " ('river', 2),\n",
       " ('rock', 2),\n",
       " ('berries', 2),\n",
       " ('hills', 2),\n",
       " ('oak', 2),\n",
       " (\"harvest'\", 2),\n",
       " ('rocks', 2),\n",
       " ('white', 2),\n",
       " ('america', 2),\n",
       " ('raider', 2),\n",
       " ('tint', 2),\n",
       " ('widow', 2),\n",
       " ('candy', 2),\n",
       " ('earth', 2),\n",
       " ('my', 2),\n",
       " ('lentil', 2),\n",
       " ('sharkey', 2),\n",
       " ('syndrome', 2),\n",
       " ('circles', 2),\n",
       " ('slash', 2),\n",
       " ('eagle', 2),\n",
       " ('hole', 2),\n",
       " ('that', 2),\n",
       " ('brothers', 2),\n",
       " ('double-decker', 2),\n",
       " ('ca', 2),\n",
       " ('threat', 2),\n",
       " ('car', 2),\n",
       " ('maple', 2),\n",
       " ('tomahawk', 2),\n",
       " ('amazon', 2),\n",
       " ('bean', 2),\n",
       " ('lizzie', 2),\n",
       " ('hour', 2),\n",
       " ('khmer', 2),\n",
       " ('hats...and', 2),\n",
       " ('pencil', 2),\n",
       " ('lobster', 2),\n",
       " ('print', 2),\n",
       " ('ocean', 2),\n",
       " ('wagon', 2),\n",
       " ('fez', 2),\n",
       " ('wheel', 2),\n",
       " ('grooms', 2),\n",
       " ('savages', 2),\n",
       " ('illustration', 2),\n",
       " ('breasted', 2),\n",
       " ('has', 2),\n",
       " ('follows', 2),\n",
       " ('shows', 2),\n",
       " ('clogs', 2),\n",
       " ('scorpion', 2),\n",
       " ('lighted', 2),\n",
       " ('hand', 2),\n",
       " ('colored', 2),\n",
       " ('chilis', 2),\n",
       " ('writing', 2),\n",
       " ('church', 2),\n",
       " ('soxs', 2),\n",
       " ('skelton', 2),\n",
       " ('violin', 2),\n",
       " ('concrete', 2),\n",
       " ('headed', 2),\n",
       " ('vs.', 2),\n",
       " ('fish', 2),\n",
       " ('reflector', 2),\n",
       " ('stockings', 2),\n",
       " ('wyverns', 2),\n",
       " ('piano', 2),\n",
       " ('shield', 2),\n",
       " ('gone', 2),\n",
       " ('circle', 2),\n",
       " ('sonja', 2),\n",
       " ('celery', 1),\n",
       " ('tides', 1),\n",
       " ('massacre', 1),\n",
       " ('nail', 1),\n",
       " ('wolf', 1),\n",
       " ('figured', 1),\n",
       " ('girl', 1),\n",
       " ('feather', 1),\n",
       " ('faces', 1),\n",
       " ('transitions', 1),\n",
       " ('recipe', 1),\n",
       " ('planet...will', 1),\n",
       " ('blur', 1),\n",
       " ('pigment', 1),\n",
       " ('capitals', 1),\n",
       " ('fort', 1),\n",
       " ('teeth', 1),\n",
       " ('hard', 1),\n",
       " ('flower', 1),\n",
       " ('his', 1),\n",
       " ('mark', 1),\n",
       " ('dal', 1),\n",
       " ('motley', 1),\n",
       " (\"o'brien\", 1),\n",
       " ('which', 1),\n",
       " ('superwoman', 1),\n",
       " ('sheep', 1),\n",
       " ('dresses', 1),\n",
       " ('cent', 1),\n",
       " ('page', 1),\n",
       " ('choo', 1),\n",
       " ('clouod', 1),\n",
       " ('printed', 1),\n",
       " ('every', 1),\n",
       " ('mohair', 1),\n",
       " ('field', 1),\n",
       " ('scribed', 1),\n",
       " ('adorns', 1),\n",
       " ('clothing', 1),\n",
       " ('glare', 1),\n",
       " ('display', 1),\n",
       " ('weeds', 1),\n",
       " ('does', 1),\n",
       " ('returns', 1),\n",
       " ('devils', 1),\n",
       " ('court', 1),\n",
       " ('reign', 1),\n",
       " ('rackham', 1),\n",
       " ('raincoat', 1),\n",
       " ('many', 1),\n",
       " ('sari', 1),\n",
       " ('eyed', 1),\n",
       " ('glow', 1),\n",
       " ('norvo', 1),\n",
       " ('jam', 1),\n",
       " ('stuff', 1),\n",
       " ('here', 1),\n",
       " ('radish', 1),\n",
       " ('started', 1),\n",
       " ('no.40', 1),\n",
       " ('edit', 1),\n",
       " ('chocolate', 1),\n",
       " ('editing', 1),\n",
       " ('smoke', 1),\n",
       " ('heads..', 1),\n",
       " ('head..', 1),\n",
       " ('shirt', 1),\n",
       " ('buttes', 1),\n",
       " ('brigades', 1),\n",
       " ('crest', 1),\n",
       " ('wyvren', 1),\n",
       " ('tailed', 1),\n",
       " ('snowsuit', 1),\n",
       " ('decade', 1),\n",
       " ('fairy', 1),\n",
       " ('tower', 1),\n",
       " ('bills', 1),\n",
       " ('crown', 1),\n",
       " ('wings', 1),\n",
       " ('baiter', 1),\n",
       " ('schoendienst', 1),\n",
       " ('you', 1),\n",
       " ('attire', 1),\n",
       " ('shoe', 1),\n",
       " ('dirt', 1),\n",
       " ('magician*', 1),\n",
       " ('magician-', 1),\n",
       " ('liquid', 1),\n",
       " ('borders', 1),\n",
       " ('movie-related', 1),\n",
       " ('unlocked', 1),\n",
       " ('stags', 1),\n",
       " ('giants', 1),\n",
       " ('hynes', 1),\n",
       " ('paint', 1),\n",
       " ('azalea', 1),\n",
       " ('garment', 1),\n",
       " ('nun', 1),\n",
       " ('smith-', 1),\n",
       " ('marks', 1),\n",
       " ('through', 1),\n",
       " ('furry', 1),\n",
       " ('meaning', 1),\n",
       " ('platforms', 1),\n",
       " ('lantern.also', 1),\n",
       " ('butter', 1),\n",
       " ('aside', 1),\n",
       " ('veggies', 1),\n",
       " ('against', 1),\n",
       " ('stoplights...and', 1),\n",
       " ('cow', 1),\n",
       " ('silt', 1),\n",
       " ('leading', 1),\n",
       " ('because', 1),\n",
       " ('vermouth', 1),\n",
       " ('indians', 1),\n",
       " ('capsicum', 1),\n",
       " ('are', 1),\n",
       " ('teachers', 1),\n",
       " ('paper', 1),\n",
       " ('retired', 1),\n",
       " ('breast', 1),\n",
       " ('zones', 1),\n",
       " ('have', 1),\n",
       " ('mike', 1),\n",
       " ('valley', 1),\n",
       " ('militias', 1),\n",
       " ('along', 1),\n",
       " ('stick', 1),\n",
       " ('soldier', 1),\n",
       " ('soldiers', 1),\n",
       " ('literacy', 1),\n",
       " ('bandits', 1),\n",
       " ('spade', 1),\n",
       " ('hearts', 1),\n",
       " ('grant', 1),\n",
       " ('shifts', 1),\n",
       " ('marker', 1),\n",
       " ('hamer', 1),\n",
       " ('traffic', 1),\n",
       " ('allen', 1),\n",
       " ('binding', 1),\n",
       " ('ford', 1),\n",
       " ('stuff-', 1),\n",
       " ('robes...', 1),\n",
       " ('at', 1),\n",
       " ('stars', 1),\n",
       " ('curls', 1),\n",
       " ('heroin', 1),\n",
       " ('coats', 1),\n",
       " ('gibson', 1),\n",
       " ('scream', 1),\n",
       " ('overlaid', 1),\n",
       " ('hatters', 1),\n",
       " ('powder', 1),\n",
       " ('carnation', 1),\n",
       " ('tribes', 1),\n",
       " ('suburban', 1),\n",
       " ('had', 1),\n",
       " ('door', 1),\n",
       " ('after', 1),\n",
       " ('branch', 1),\n",
       " ('inkpen', 1),\n",
       " (':', 1),\n",
       " ('means', 1),\n",
       " ('martians', 1),\n",
       " ('skin', 1),\n",
       " ('convertible', 1),\n",
       " ('direction', 1),\n",
       " ('sequined', 1),\n",
       " ('tip', 1),\n",
       " ('mouth', 1),\n",
       " ('sails', 1),\n",
       " ('box', 1),\n",
       " ('string', 1),\n",
       " ('tractor-trailer', 1),\n",
       " ('truck', 1),\n",
       " ('mountain', 1),\n",
       " ('out', 1),\n",
       " ('learns', 1),\n",
       " ('beach.the', 1),\n",
       " ('1956', 1),\n",
       " ('all', 1),\n",
       " ('amry', 1),\n",
       " ('numerous', 1),\n",
       " ('goopy', 1),\n",
       " ('seal', 1),\n",
       " ('black', 1),\n",
       " ('coat', 1),\n",
       " ('tams', 1),\n",
       " ('system', 1),\n",
       " ('dynan', 1),\n",
       " ('highlights', 1),\n",
       " ('trees', 1),\n",
       " ('he', 1),\n",
       " ('geting', 1),\n",
       " ('andstops', 1),\n",
       " ('back', 1),\n",
       " ('guy', 1),\n",
       " (\"hat'\", 1),\n",
       " ('warning', 1),\n",
       " ('foil', 1),\n",
       " ('navy', 1),\n",
       " ('what', 1),\n",
       " ('arrows', 1),\n",
       " ('iron', 1),\n",
       " ('dawn', 1),\n",
       " ('bugs', 1),\n",
       " ('encircling', 1),\n",
       " ('hill', 1),\n",
       " ('colour', 1),\n",
       " ('before', 1),\n",
       " ('foliage', 1),\n",
       " ('ears', 1),\n",
       " ('project', 1),\n",
       " ('we', 1),\n",
       " ('flowers', 1),\n",
       " ('bookmark', 1),\n",
       " ('romp', 1),\n",
       " ('cape', 1),\n",
       " ('raiders', 1),\n",
       " ('tsar', 1),\n",
       " ('diamond', 1),\n",
       " ('jacket', 1),\n",
       " ('plaid', 1),\n",
       " ('miniskirts', 1),\n",
       " ('pimple', 1),\n",
       " ('diapers', 1),\n",
       " ('wines', 1),\n",
       " ('beets', 1),\n",
       " ('an', 1),\n",
       " ('also', 1),\n",
       " ('beret', 1),\n",
       " ('cheated', 1),\n",
       " ('meats', 1),\n",
       " ('electric', 1),\n",
       " ('first', 1),\n",
       " ('edging', 1),\n",
       " ('hexagram', 1),\n",
       " ('glop', 1),\n",
       " ('neon', 1),\n",
       " ('beads', 1),\n",
       " ('natives', 1),\n",
       " ('hardcover', 1),\n",
       " ('south', 1),\n",
       " ('potato', 1),\n",
       " ('#96', 1),\n",
       " ('#97.', 1),\n",
       " ('structural', 1),\n",
       " ('foley', 1),\n",
       " ('&', 1),\n",
       " ('ripper-citizen-x', 1),\n",
       " (\"october'\", 1),\n",
       " ('queen-', 1),\n",
       " ('nails-', 1),\n",
       " ('verb', 1),\n",
       " ('sharpie', 1),\n",
       " ('bmw', 1),\n",
       " ('more', 1),\n",
       " ('walls', 1),\n",
       " ('63', 1),\n",
       " ('curry', 1),\n",
       " ('sword', 1),\n",
       " ('prairie', 1),\n",
       " ('feet', 1),\n",
       " ('so', 1),\n",
       " ('18-wheeler', 1),\n",
       " ('food', 1),\n",
       " ('jag', 1),\n",
       " ('wood', 1),\n",
       " ('fork', 1),\n",
       " ('pimentos', 1),\n",
       " ('fruit', 1),\n",
       " ('rain', 1),\n",
       " ('spirals', 1),\n",
       " ('lust', 1),\n",
       " ('team', 1),\n",
       " ('ranger', 1),\n",
       " ('highly', 1),\n",
       " ('must', 1),\n",
       " ('notebook', 1),\n",
       " ('wig', 1),\n",
       " ('offers', 1),\n",
       " ('chard', 1),\n",
       " ('only', 1),\n",
       " ('nosed', 1),\n",
       " ('vulture', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs['red'].most_common() # A key 'red'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making rank-frequency table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freqs_default.most_common() # gives rank and frequencies\n",
    "vals=freqs_default.most_common()\n",
    "mc=[]\n",
    "for x in range (1,len(vals)):\n",
    "    mc.append(vals[x][1])\n",
    "#[x for x in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot([x for x in range(0,100)],mc[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.loglog([x for x in range(0,len(vals)-1)],mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Given a text file, build a vocabulary (that is: a string-to-integer mapping), where the vocabulary size is at most max_voc_size. If the number of distinct words is greater than this size, you should just use the most frequent words.\n",
    "Given a text file, go through the file line by line and divide the data into batches that you encode as integers using the mapping that you defined in the first step. Out-of-vocabulary words should be mapped to a special dummy symbol (e.g. <OTHER>). The batch size should be at most batch_size. Lines that are shorter than the longest line in the batch should be padded. \n",
    "The batch should be stored as a NumPy array or PyTorch tensor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_voc_size=1000\n",
    "def count_word_frequencies(YOUR_FILE, ENCODING):\n",
    "\n",
    "    freqs = Counter()\n",
    "    with open(YOUR_FILE, encoding = ENCODING) as f:\n",
    "        for line in f:\n",
    "            tokens = line.lower().split()\n",
    "            for token in tokens:\n",
    "                freqs[token] += 1\n",
    "                \n",
    "    return freqs\n",
    "\n",
    "def create_vocabulary(word_frequencies,max_vocabulary_size):\n",
    "    \n",
    "    number_of_words=len(word_frequencies)\n",
    "    if max_vocabulary_size < number_of_words:\n",
    "        vocabulary=word_frequencies.most_common()[0:max_vocabulary_size]\n",
    "        #vocabulary[x for x in range(max_vocabulary_size)][1]=range(max_vocabulary_size)\n",
    "    else:\n",
    "        vocabulary=word_frequencies.most_common()\n",
    "    \n",
    "    indexed_vocabulary=list()\n",
    "    for x in range(len(vocabulary)-1):\n",
    "        indexed_vocabulary.append((vocabulary[x][0],x+1)) # creates a tuple... change with []\n",
    "        \n",
    "    return indexed_vocabulary\n",
    "\n",
    "# all done:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_integer_vocabulary(word_freqs, max_voc_size):\n",
    "    \"\"\" \n",
    "    Create vocabulary where common words are matched to integers. \n",
    "    \"\"\"\n",
    "    \n",
    "    word_list = []\n",
    "\n",
    "    if len(word_freqs.most_common()) > max_voc_size:\n",
    "        vocab = word_freqs.most_common()[0:max_voc_size]\n",
    "\n",
    "    else:\n",
    "        vocab = word_freqs\n",
    "\n",
    "    for i in range(len(vocab)):\n",
    "        word_list.append(vocab[i][0])\n",
    "\n",
    "    # Get pairs of elements    \n",
    "    tmp = zip(word_list, range(1,max_voc_size+1))\n",
    "    # Make pairs into a dictionary\n",
    "    vocab = dict(tmp)\n",
    "\n",
    "    # Create default dictionary - returns 0 if an undefined key is called\n",
    "    vocab2 = collections.defaultdict(int)\n",
    "    vocab2.update(vocab)\n",
    "\n",
    "    # Double check that it returns 0\n",
    "    # print(vocab2[\"terehgdjhshrersg\"])\n",
    "    \n",
    "    return vocab2\n",
    "max_voc_size = 1000\n",
    "vocabulary = create_integer_vocabulary(freqs_default, max_voc_size)\n",
    "\n",
    "f = open(\"a1_data/books.txt\", encoding = 'ISO-8859-1')\n",
    "single_line = f.readline().lower().split()\n",
    "line_as_int = list(map(vocabulary.get, single_line))\n",
    "line_as_int = [0 if x is None else x for x in line_as_int] # set None values to 0 - no idea why I have to do this again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it took me two days to read this book because once i started i could n't put it down . mr.magnus leads us through various exercises to show us how we too can have an astral projection . the exercises are fun and interesting . he also adds entries from his journal that makes the reader feel that they too can achieve the ability to enter the astral realm . this book is easy to read unlike many other books on the subject which make the reader feel bogged down with specificities which soon become boring . a beautifully written , enthusiastic book about a subject that is surrounded in mystery \n",
      "\n",
      "['it', 'took', 'me', 'two', 'days', 'to', 'read', 'this', 'book', 'because', 'once', 'i', 'started', 'i', 'could', \"n't\", 'put', 'it', 'down', '.', 'mr.magnus', 'leads', 'us', 'through', 'various', 'exercises', 'to', 'show', 'us', 'how', 'we', 'too', 'can', 'have', 'an', 'astral', 'projection', '.', 'the', 'exercises', 'are', 'fun', 'and', 'interesting', '.', 'he', 'also', 'adds', 'entries', 'from', 'his', 'journal', 'that', 'makes', 'the', 'reader', 'feel', 'that', 'they', 'too', 'can', 'achieve', 'the', 'ability', 'to', 'enter', 'the', 'astral', 'realm', '.', 'this', 'book', 'is', 'easy', 'to', 'read', 'unlike', 'many', 'other', 'books', 'on', 'the', 'subject', 'which', 'make', 'the', 'reader', 'feel', 'bogged', 'down', 'with', 'specificities', 'which', 'soon', 'become', 'boring', '.', 'a', 'beautifully', 'written', ',', 'enthusiastic', 'book', 'about', 'a', 'subject', 'that', 'is', 'surrounded', 'in', 'mystery']\n",
      "111\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "\n",
    "\n",
    "counter=0\n",
    "with open('a1_data/books.txt', encoding='ISO-8859-1') as f:\n",
    "    \n",
    "    # create vector of arrays, will hold all batches\n",
    "    #create a loop to \n",
    "    for line in f:\n",
    "\n",
    "        print(line)\n",
    "        tokens = line.lower().split()\n",
    "        len(tokens)\n",
    "        \n",
    "        # convert tect to intgers using vocabulary of most common words. Zero-pad and set unknown words to zero.\n",
    "        \n",
    "        if counter %% batch_size == 0:\n",
    "            \n",
    "\n",
    "        #for token in tokens:\n",
    "    \n",
    "   # with open(dataset) as f:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_vocabulary_size=10\n",
    "tst=create_vocabulary(freqs_default,max_vocabulary_size=10)\n",
    "tst\n",
    "#freqs_default.most_common()\n",
    "count = len(open(\"a1_data/wikipedia.txt\", encoding = 'utf-8').readline())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "batch_size=101\n",
    "len_lines = []\n",
    "tmp_lines = []\n",
    "with open('a1_data/books.txt', encoding='ISO-8859-1') as f:\n",
    "    for line in f:\n",
    "        counter+=1\n",
    "        tokens = line.lower().split()\n",
    "        tmp_lines.append(len(tokens))\n",
    "        \n",
    "        if (counter % batch_size == 0):\n",
    "            len_lines.append(max(tmp_lines))\n",
    "            tmp_lines = []\n",
    "            \n",
    "    if (counter % batch_size != 0):\n",
    "        len_lines.append(max(tmp_lines)) # if at end of the file\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter is the length of the file\n",
    "with open('a1_data/books.txt', encoding='ISO-8859-1') as f:\n",
    "    \n",
    "    batches=[]\n",
    "    batch_counter=0\n",
    "    line_counter=0\n",
    "\n",
    "    for line in f:\n",
    "        if line_counter % batch_size == 0:\n",
    "            tmp_array=np.zeros(shape=(batch_size,len_lines[batch_counter])) #fill this temporary array\n",
    "\n",
    "       \n",
    "        tokens = line.lower().split()\n",
    "        line_as_int = list(map(vocabulary.get, tokens))\n",
    "        line_as_int = [0 if x is None else x for x in line_as_int] # set None values to 0 - no idea why I have to do this again\n",
    "        \n",
    "        \n",
    "        tmp_array[line_counter % batch_size,0:(len(line_as_int))]=line_as_int\n",
    "        \n",
    "        line_counter+=1 #when we done\n",
    "        if line_counter % batch_size ==0:\n",
    "            batches.append(tmp_array)\n",
    "            batch_counter+=1\n",
    "        \n",
    "   \n",
    "    if line_counter % batch_size != 0:\n",
    "        tmp_array=tmp_array[0:(line_counter % batch_size),:]\n",
    "        batches.append(tmp_array)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10., 457.,  31.,   0.,   0., 432.,  19.,   0.,   4.,  28., 227.,\n",
       "        11.,  14.,   4.,   1.,   0.,   0.,   3.,  11.,  21.,   7., 236.,\n",
       "         0.,   3.,  10., 753.,   0.,  94., 348.,  78.,  24.,  11.,  30.,\n",
       "        32.,  58.,   7.,   0.,  14.,   3.,  71., 724.,  52.,  48.,   0.,\n",
       "        60.,  58.,   6., 246.,  49.,  38.,   1., 408.,  11.,   8.,   0.,\n",
       "         1.,  14.,   6., 103.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches[batch_counter][27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280000"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_counter * batch_size + 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type '_io.TextIOWrapper' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-930a9fdf395a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a1_data/books.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ISO-8859-1'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: object of type '_io.TextIOWrapper' has no len()"
     ]
    }
   ],
   "source": [
    "with open('a1_data/books.txt', encoding='ISO-8859-1') as f:\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 1)\n",
      "('b', 2)\n",
      "('c', 3)\n"
     ]
    }
   ],
   "source": [
    "lst=[('a',1),('b',2),('c',3)]\n",
    "for key, val in enumerate(lst):\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "example = 'ASDF Inc. bought two companies last week. One of them was founded by Jane XYZ in 2012 in Germany.'\n",
    "result = nlp(example)\n",
    "result\n",
    "spacy.displacy.render(result, style='ent', jupyter=True)\n",
    "spacy.displacy.render(result, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in result:\n",
    "    print(token.text) # gives the word itself\n",
    "    print(token.lemma_) # gives infinitiv-form? also that them is a pronoun\n",
    "    #print(token.pos_) # gives verb, adjective etcetera.\n",
    "    print(token.head) # gives parent, ther eis no parent to bought\n",
    "    print(token.dep_) # gives upstream label in relation to graph given by spacy.displacy.render(result, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for entity in result.ents:\n",
    "    print(entity.text)\n",
    "    print(entity.label_) # gives cardinal for numbers, country for germany etc, that is higher level description of the entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = open('a1_data/books.txt', encoding='ISO-8859-1').read()[0:990000] # small subset\n",
    "text_proc = nlp(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = []\n",
    "for entity in text_proc.ents:\n",
    "    if (entity.label_ is 'GPE' and entity.text not in countries):\n",
    "        countries.append(entity.text)\n",
    "\n",
    "country_frequencies=collections.Counter()\n",
    "with open('a1_data/books.txt', encoding='ISO-8859-1') as f:\n",
    "for line in text:\n",
    "    print(line)\n",
    "    break\n",
    "    tokens = line.lower().split()\n",
    "    for token in tokens:\n",
    "        if token in countries:\n",
    "            country_frequencies[token] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to count most common countries, using the wrong textfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "contries = []\n",
    "country_frequencies=collections.Counter()\n",
    "with open('a1_data/books.txt', encoding='ISO-8859-1') as f:\n",
    "     for line in f:\n",
    "        nlp_line=nlp(line)\n",
    "        for entity in nlp_line.ents:\n",
    "            if (entity.label_ is 'GPE' and entity.text not in countries):\n",
    "                countries.append(entity.text)\n",
    "        tokens = line.lower().split()\n",
    "        for token in tokens:\n",
    "            if token in countries:\n",
    "                country_frequencies[token]+=1\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_frequencies.most_common()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
